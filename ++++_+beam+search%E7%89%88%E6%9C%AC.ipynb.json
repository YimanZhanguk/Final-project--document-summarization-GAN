{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import codecs\n",
    "from sklearn.utils import shuffle\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import re\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.ops import nn_ops\n",
    "from tensorflow.python.ops import tensor_array_ops\n",
    "from tensorflow.python.ops import embedding_ops\n",
    "from tensorflow.python.ops import variable_scope as vs\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.util import nest\n",
    "from tensorflow.python.ops import variable_scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PAD_ID = 0\n",
    "GO_ID = 1\n",
    "EOS_ID = 2\n",
    "UNK_ID = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    prob = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sequence_loss_by_example(logits, targets, weights,\n",
    "                             average_across_timesteps=True,\n",
    "                             softmax_loss_function=None, name=None):\n",
    "    # print('sequence_loss_by_example')\n",
    "    if len(targets) != len(logits) or len(weights) != len(logits):\n",
    "        raise ValueError(\"Lengths of logits, weights, and targets must be the same \"\n",
    "                     \"%d, %d, %d.\" % (len(logits), len(weights), len(targets)))\n",
    "    with ops.name_scope(name, \"sequence_loss_by_example\",\n",
    "                      logits + targets + weights):\n",
    "        log_perp_list = []\n",
    "        for logit, target, weight in zip(logits, targets, weights):\n",
    "            if softmax_loss_function is None:\n",
    "                target = array_ops.reshape(target, [-1])\n",
    "                crossent = nn_ops.sparse_softmax_cross_entropy_with_logits(logit, target)\n",
    "            else:\n",
    "                crossent = softmax_loss_function(logit, target)\n",
    "            log_perp_list.append(crossent * weight)\n",
    "        log_perps = math_ops.add_n(log_perp_list)\n",
    "        if average_across_timesteps:\n",
    "            total_size = math_ops.add_n(weights)\n",
    "            total_size += 1e-12  # Just to avoid division by 0 for all-0 weights.\n",
    "            log_perps /= total_size\n",
    "    return log_perps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sequence_loss(logits, targets, weights,\n",
    "                  average_across_timesteps=True, average_across_batch=True,\n",
    "                  softmax_loss_function=None, name=None):\n",
    "    #print(\"sequence_loss\")\n",
    "    with ops.name_scope(name, \"sequence_loss\", logits + targets + weights):\n",
    "        cost = math_ops.reduce_sum(sequence_loss_by_example(\n",
    "                                                        logits, targets, weights,\n",
    "                                                        average_across_timesteps=average_across_timesteps,\n",
    "                                                        softmax_loss_function=softmax_loss_function))\n",
    "        if average_across_batch:\n",
    "            batch_size = array_ops.shape(targets[0])[0]\n",
    "            return cost / math_ops.cast(batch_size, cost.dtype)\n",
    "        else:\n",
    "            return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_BIAS_VARIABLE_NAME = \"bias\"\n",
    "_WEIGHTS_VARIABLE_NAME = \"kernel\"\n",
    "def linear(args,\n",
    "            output_size,\n",
    "            bias,\n",
    "            bias_initializer=None,\n",
    "            kernel_initializer=None):\n",
    "    if args is None or (nest.is_sequence(args) and not args):\n",
    "        raise ValueError(\"`args` must be specified\")\n",
    "    if not nest.is_sequence(args):\n",
    "        args = [args]\n",
    "\n",
    "    # Calculate the total size of arguments on dimension 1.\n",
    "    total_arg_size = 0\n",
    "    shapes = [a.get_shape() for a in args]\n",
    "    for shape in shapes:\n",
    "        if shape.ndims != 2:\n",
    "            raise ValueError(\"linear is expecting 2D arguments: %s\" % shapes)\n",
    "        if shape[1].value is None:\n",
    "            raise ValueError(\"linear expects shape[1] to be provided for shape %s, \"\n",
    "                       \"but saw %s\" % (shape, shape[1]))\n",
    "        else:\n",
    "            total_arg_size += shape[1].value\n",
    "\n",
    "    dtype = [a.dtype for a in args][0]\n",
    "    \n",
    "    # Now the computation.\n",
    "    scope = vs.get_variable_scope()\n",
    "    with vs.variable_scope(scope) as outer_scope:\n",
    "        weights = vs.get_variable(\n",
    "                                    _WEIGHTS_VARIABLE_NAME, [total_arg_size, output_size],\n",
    "                                    dtype=dtype,\n",
    "                                    initializer=kernel_initializer)\n",
    "        if len(args) == 1:\n",
    "            res = math_ops.matmul(args[0], weights)\n",
    "        else:\n",
    "            res = math_ops.matmul(array_ops.concat(args, 1), weights)\n",
    "        if not bias:\n",
    "            return res\n",
    "        with vs.variable_scope(outer_scope) as inner_scope:\n",
    "            inner_scope.set_partitioner(None)\n",
    "            if bias_initializer is None:\n",
    "                bias_initializer = init_ops.constant_initializer(0.0, dtype=dtype)\n",
    "            biases = vs.get_variable(\n",
    "                                      _BIAS_VARIABLE_NAME, [output_size],\n",
    "                                      dtype=dtype,\n",
    "                                        initializer=bias_initializer)\n",
    "        return nn_ops.bias_add(res, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def attention_decoder(decoder_inputs,\n",
    "                      initial_state,\n",
    "                      attention_states,\n",
    "                      cell,\n",
    "                      output_size=None,\n",
    "                      num_heads=1,\n",
    "                      loop_function=None,\n",
    "                      dtype=None,\n",
    "                      scope=None,\n",
    "                      initial_state_attention=False):\n",
    "\n",
    "    #print('attention_decoder')\n",
    "    if not decoder_inputs:\n",
    "        raise ValueError(\"Must provide at least 1 input to attention decoder.\")\n",
    "    if num_heads < 1:\n",
    "        raise ValueError(\"With less than 1 heads, use a non-attention decoder.\")\n",
    "    if attention_states.get_shape()[2].value is None:\n",
    "        raise ValueError(\"Shape[2] of attention_states must be known: %s\"\n",
    "                     % attention_states.get_shape())\n",
    "    if output_size is None:\n",
    "        output_size = cell.output_size\n",
    "\n",
    "    with variable_scope.variable_scope(scope or \"attention_decoder\", dtype=dtype) as scope:\n",
    "        dtype = scope.dtype\n",
    "\n",
    "        batch_size = array_ops.shape(decoder_inputs[0])[0]  # Needed for reshaping.\n",
    "        attn_length = attention_states.get_shape()[1].value\n",
    "        if attn_length is None:\n",
    "            attn_length = shape(attention_states)[1]\n",
    "        attn_size = attention_states.get_shape()[2].value\n",
    "\n",
    "        # To calculate W1 * h_t we use a 1-by-1 convolution, need to reshape before.\n",
    "        hidden = array_ops.reshape(attention_states, [-1, attn_length, 1, attn_size])\n",
    "        hidden_features = []\n",
    "        v = []\n",
    "        attention_vec_size = attn_size  # Size of query vectors for attention.\n",
    "        for a in xrange(num_heads):\n",
    "            k = variable_scope.get_variable(\"AttnW_%d\" % a,\n",
    "                                      [1, 1, attn_size, attention_vec_size])\n",
    "            hidden_features.append(nn_ops.conv2d(hidden, k, [1, 1, 1, 1], \"SAME\"))\n",
    "            v.append(variable_scope.get_variable(\"AttnV_%d\" % a, [attention_vec_size]))\n",
    "\n",
    "        state = initial_state\n",
    "\n",
    "        def attention(query):\n",
    "            \"\"\"Put attention masks on hidden using hidden_features and query.\"\"\"\n",
    "            #print(\"attention_decoder.attention\")\n",
    "            ds = []  # Results of attention reads will be stored here.\n",
    "            if nest.is_sequence(query):  # If the query is a tuple, flatten it.\n",
    "                query_list = nest.flatten(query)\n",
    "                for q in query_list:  # Check that ndims == 2 if specified.\n",
    "                    ndims = q.get_shape().ndims\n",
    "                    if ndims:\n",
    "                        assert ndims == 2\n",
    "                query = array_ops.concat(query_list,1)\n",
    "            for a in xrange(num_heads):\n",
    "                with variable_scope.variable_scope(\"Attention_%d\" % a):\n",
    "                    #print('调用linear')\n",
    "                    y = linear(query, attention_vec_size,True)\n",
    "                    y = array_ops.reshape(y, [-1, 1, 1, attention_vec_size])\n",
    "                    # Attention mask is a softmax of v^T * tanh(...).\n",
    "                    s = math_ops.reduce_sum(v[a] * math_ops.tanh(hidden_features[a] + y), [2, 3])\n",
    "                    a = nn_ops.softmax(s)\n",
    "                    # Now calculate the attention-weighted vector d.\n",
    "                    d = math_ops.reduce_sum(\n",
    "                    array_ops.reshape(a, [-1, attn_length, 1, 1]) * hidden,[1, 2])\n",
    "                    ds.append(array_ops.reshape(d, [-1, attn_size]))\n",
    "            return ds\n",
    "\n",
    "        outputs = []\n",
    "        prev = None\n",
    "        batch_attn_size = array_ops.stack([batch_size, attn_size])\n",
    "        attns = [array_ops.zeros(batch_attn_size, dtype=dtype)for _ in xrange(num_heads)]\n",
    "        for a in attns:  # Ensure the second shape of attention vectors is set.\n",
    "            a.set_shape([None, attn_size])\n",
    "        if initial_state_attention:\n",
    "            attns = attention(initial_state)\n",
    "        for i, inp in enumerate(decoder_inputs):\n",
    "            if i > 0:\n",
    "                variable_scope.get_variable_scope().reuse_variables()\n",
    "            # If loop_function is set, we use it instead of decoder_inputs.\n",
    "            if loop_function is not None and prev is not None:\n",
    "                with variable_scope.variable_scope(\"loop_function\", reuse=True):\n",
    "                    inp = loop_function(prev, i)\n",
    "            # Merge input and previous attentions into one vector of the right size.\n",
    "            input_size = inp.get_shape().with_rank(2)[1]\n",
    "            if input_size.value is None:\n",
    "                raise ValueError(\"Could not infer input size from input: %s\" % inp.name)\n",
    "                \n",
    "            #print('调用linear')\n",
    "            x = linear([inp] + attns, input_size,True)\n",
    "            cell_output, state = cell(x, state)\n",
    "            if i == 0 and initial_state_attention:\n",
    "                #print('找错1')\n",
    "                with variable_scope.variable_scope(variable_scope.get_variable_scope(),\n",
    "                                           reuse=True):\n",
    "                    #print(\"找错2\")\n",
    "                    attns = attention(state)\n",
    "            else:\n",
    "                #print(\"找错3\")\n",
    "                attns = attention(state)\n",
    "            \n",
    "            with variable_scope.variable_scope(\"AttnOutputProjection\"):\n",
    "                #print('output = linear([cell_output] + attns, output_size,True)')\n",
    "                output = linear([cell_output] + attns, output_size,True)\n",
    "            if loop_function is not None:\n",
    "                prev = output\n",
    "            #print('outputs.append(output)')\n",
    "            outputs.append(output)\n",
    "\n",
    "    return outputs, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _argmax_or_mcsearch(embedding, output_projection=None, update_embedding=True, mc_search=False):\n",
    "    def loop_function(prev, _):\n",
    "        if output_projection is not None:\n",
    "            prev = nn_ops.xw_plus_b(prev, output_projection[0], output_projection[1])\n",
    "\n",
    "\n",
    "        if isinstance(mc_search, bool):\n",
    "            prev_symbol = tf.reshape(tf.multinomial(prev, 1), [-1]) if mc_search else math_ops.argmax(prev, 1)\n",
    "        else:\n",
    "            prev_symbol = tf.cond(mc_search, lambda: tf.reshape(tf.multinomial(prev, 1), [-1]), lambda: tf.argmax(prev, 1))\n",
    "\n",
    "\n",
    "        emb_prev = embedding_ops.embedding_lookup(embedding, prev_symbol)\n",
    "        if not update_embedding:\n",
    "            emb_prev = array_ops.stop_gradient(emb_prev)\n",
    "        return emb_prev\n",
    "    return loop_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embedding_attention_decoder(decoder_inputs,\n",
    "                                initial_state,\n",
    "                                attention_states,\n",
    "                                cell,\n",
    "                                num_symbols,\n",
    "                                embedding_size,\n",
    "                                num_heads=1,\n",
    "                                output_size=None,\n",
    "                                output_projection=None,\n",
    "                                feed_previous=False,\n",
    "                                update_embedding_for_previous=True,\n",
    "                                dtype=None,\n",
    "                                scope=None,\n",
    "                                initial_state_attention=False,\n",
    "                                mc_search = False):\n",
    "    #print('embedding_attention_decoder')\n",
    "    if output_size is None:\n",
    "        output_size = cell.output_size\n",
    "    if output_projection is not None:\n",
    "        proj_biases = ops.convert_to_tensor(output_projection[1], dtype=dtype)\n",
    "        proj_biases.get_shape().assert_is_compatible_with([num_symbols])\n",
    "\n",
    "    with variable_scope.variable_scope(scope or \"embedding_attention_decoder\", dtype=dtype) as scope:\n",
    "        embedding = variable_scope.get_variable(\"embedding\",[num_symbols, embedding_size])\n",
    "\n",
    "        loop_function = None\n",
    "        if feed_previous == True:\n",
    "            loop_function = _argmax_or_mcsearch(embedding, output_projection, update_embedding_for_previous, mc_search)\n",
    "\n",
    "        emb_inp = [embedding_ops.embedding_lookup(embedding, i) for i in decoder_inputs]\n",
    "        return attention_decoder(\n",
    "                                emb_inp,\n",
    "                                initial_state,\n",
    "                                attention_states,\n",
    "                                cell,\n",
    "                                output_size=output_size,\n",
    "                                num_heads=num_heads,\n",
    "                                loop_function=loop_function,\n",
    "                                initial_state_attention=initial_state_attention,\n",
    "                                scope=scope)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# return output, state, encoder_state\n",
    "\n",
    "def embedding_attention_seq2seq(encoder_inputs,\n",
    "                                decoder_inputs,\n",
    "                                cell,\n",
    "                                num_encoder_symbols,\n",
    "                                num_decoder_symbols,\n",
    "                                embedding_size,\n",
    "                                num_heads=1,\n",
    "                                output_projection=None,\n",
    "                                feed_previous=False,\n",
    "                                dtype=None,\n",
    "                                scope=None,\n",
    "                                initial_state_attention=False,\n",
    "                                mc_search=False):\n",
    "\n",
    "    with variable_scope.variable_scope(scope or \"embedding_attention_seq2seq\", dtype=dtype) as scope:\n",
    "        dtype = scope.dtype\n",
    "        #print('embedding_attention_seq2seq')\n",
    "        \n",
    "        # Encoder.\n",
    "        #print('encoder_cell')\n",
    "        encoder_cell = tf.contrib.rnn.core_rnn_cell.EmbeddingWrapper(\n",
    "                cell, embedding_classes=num_encoder_symbols,\n",
    "                embedding_size=embedding_size)\n",
    "        #print('encoder_outputs, encoder_state = tf.contrib.rnn.static_rnn')\n",
    "        encoder_outputs, encoder_state = tf.contrib.rnn.static_rnn(\n",
    "                encoder_cell, encoder_inputs, dtype=dtype)\n",
    "        #print('encoder_outputs')\n",
    "        \n",
    "        # First calculate a concatenation of encoder outputs to put attention on.\n",
    "        #print('top-state')\n",
    "        #top_states = [array_ops.reshape([-1, 1, cell.output_size],e)for e in encoder_outputs]\n",
    "        top_states = tf.stack(encoder_outputs)\n",
    "        top_states = tf.transpose(top_states, [1,0,2])\n",
    "        #print('attention_state')\n",
    "        attention_states = array_ops.concat(top_states,1)\n",
    "\n",
    "        # Decoder.\n",
    "        #print('decoder')\n",
    "        output_size = None\n",
    "        if output_projection is None:\n",
    "            cell = rnn_cell.OutputProjectionWrapper(cell, num_decoder_symbols)\n",
    "            output_size = num_decoder_symbols\n",
    "\n",
    "        if isinstance(feed_previous, bool):\n",
    "            outputs, state = embedding_attention_decoder(\n",
    "                          decoder_inputs,\n",
    "                          encoder_state,\n",
    "                          attention_states,\n",
    "                          cell,\n",
    "                          num_decoder_symbols,\n",
    "                          embedding_size,\n",
    "                          num_heads=num_heads,\n",
    "                          output_size=output_size,\n",
    "                          output_projection=output_projection,\n",
    "                          feed_previous=feed_previous,\n",
    "                          initial_state_attention=initial_state_attention,\n",
    "                          mc_search=mc_search,\n",
    "                          scope=scope)\n",
    "            return outputs, state, encoder_state\n",
    "\n",
    "        # If feed_previous is a Tensor, we construct 2 graphs and use cond.\n",
    "        def decoder(feed_previous_bool):\n",
    "            reuse = None if feed_previous_bool else True\n",
    "            with variable_scope.variable_scope(variable_scope.get_variable_scope(), reuse=reuse) as scope:\n",
    "                outputs, state = embedding_attention_decoder(\n",
    "                            decoder_inputs,\n",
    "                            encoder_state,\n",
    "                            attention_states,\n",
    "                            cell,\n",
    "                            num_decoder_symbols,\n",
    "                            embedding_size,\n",
    "                            num_heads=num_heads,\n",
    "                            output_size=output_size,\n",
    "                            output_projection=output_projection,\n",
    "                            feed_previous=feed_previous_bool,\n",
    "                            update_embedding_for_previous=False,\n",
    "                            initial_state_attention=initial_state_attention,\n",
    "                            mc_search=mc_search,\n",
    "                            scope=scope)\n",
    "                state_list = [state]\n",
    "                if nest.is_sequence(state):\n",
    "                    state_list = nest.flatten(state)\n",
    "                return outputs + state_list\n",
    "\n",
    "        outputs_and_state = control_flow_ops.cond(feed_previous,\n",
    "                                              lambda: decoder(True),\n",
    "                                              lambda: decoder(False))\n",
    "        outputs_len = len(decoder_inputs)  # Outputs length same as decoder inputs.\n",
    "        state_list = outputs_and_state[outputs_len:]\n",
    "        state = state_list[0]\n",
    "        if nest.is_sequence(encoder_state):\n",
    "            state = nest.pack_sequence_as(structure=encoder_state,\n",
    "                                    flat_sequence=state_list)\n",
    "        return outputs_and_state[:outputs_len], state, encoder_state\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# return outputs, losses, encoder_states\n",
    "\n",
    "def model_with_buckets(encoder_inputs, decoder_inputs, targets, weights, buckets, vocab_size, batch_size, seq2seq,\n",
    "                       output_projection=None, softmax_loss_function=None, per_example_loss=False, name=None):\n",
    "    #print('model_with_buckets')\n",
    "    if len(encoder_inputs) < buckets[-1][0]:\n",
    "        raise ValueError(\"Length of encoder_inputs (%d) must be at least that of la\"\n",
    "                     \"st bucket (%d).\" % (len(encoder_inputs), buckets[-1][0]))\n",
    "    if len(targets) < buckets[-1][1]:\n",
    "        raise ValueError(\"Length of targets (%d) must be at least that of last\"\n",
    "                     \"bucket (%d).\" % (len(targets), buckets[-1][1]))\n",
    "    if len(weights) < buckets[-1][1]:\n",
    "        raise ValueError(\"Length of weights (%d) must be at least that of last\"\n",
    "                     \"bucket (%d).\" % (len(weights), buckets[-1][1]))\n",
    "\n",
    "    all_inputs = encoder_inputs + decoder_inputs + targets + weights\n",
    "    losses = []\n",
    "    outputs = []\n",
    "    encoder_states = []\n",
    "    with ops.name_scope(name, \"model_with_buckets\", all_inputs):\n",
    "        for j, bucket in enumerate(buckets):\n",
    "            with variable_scope.variable_scope(variable_scope.get_variable_scope(),\n",
    "                                         reuse=True if j > 0 else None):\n",
    "                bucket_outputs, decoder_states, encoder_state = seq2seq(encoder_inputs[:bucket[0]],\n",
    "                                    decoder_inputs[:bucket[1]])\n",
    "                outputs.append(bucket_outputs)\n",
    "                #print(\"bucket outputs: %s\" %bucket_outputs)\n",
    "                encoder_states.append(encoder_state)\n",
    "                if per_example_loss:\n",
    "                    losses.append(sequence_loss_by_example(\n",
    "                    outputs[-1], targets[:bucket[1]], weights[:bucket[1]],\n",
    "                    softmax_loss_function=softmax_loss_function))\n",
    "                else:\n",
    "                    # losses.append(sequence_loss_by_mle(outputs[-1], targets[:bucket[1]], vocab_size, bucket[1], batch_size, output_projection))\n",
    "                    losses.append(sequence_loss(outputs[-1], targets[:bucket[1]], weights[:bucket[1]], softmax_loss_function=softmax_loss_function))\n",
    "\n",
    "    return outputs, losses, encoder_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Seq2SeqModel(object):\n",
    "    def __init__(self, config,name_scope, forward_only,use_lstm=False,num_samples=512):\n",
    "\n",
    "        dtype=tf.float32\n",
    "        \n",
    "        source_vocab_size = config.vocab_size\n",
    "        target_vocab_size = config.vocab_size\n",
    "        size= config.emb_dim\n",
    "\n",
    "        self.buckets = config.buckets\n",
    "        self.batch_size = config.batch_size\n",
    "        self.learning_rate = tf.Variable(float(config.learning_rate), trainable=False)\n",
    "        self.learning_rate_decay_op = self.learning_rate.assign(\n",
    "            self.learning_rate * config.learning_rate_decay_factor)\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        max_gradient_norm = config.max_gradient_norm\n",
    "        self.forward_only = tf.placeholder(tf.bool, name=\"forward_only\")\n",
    "        #self.num_layers = config.num_layers\n",
    "        num_layers = config.num_layers\n",
    "        self.max_gradient_norm = config.max_gradient_norm\n",
    "        \n",
    "        # If we use sampled softmax, we need an output projection.\n",
    "        output_projection = None\n",
    "        softmax_loss_function = None\n",
    "        \n",
    "        # ADD\n",
    "        self.mc_search = tf.placeholder(tf.bool, name=\"mc_search\")\n",
    "        self.up_reward = tf.placeholder(tf.bool, name=\"up_reward\")\n",
    "        self.reward_bias = tf.get_variable(\"reward_bias\", [1], dtype=tf.float32)\n",
    "        \n",
    "        # Sampled softmax only makes sense if we sample less than vocabulary size.\n",
    "        if num_samples > 0 and num_samples < target_vocab_size:\n",
    "            w = tf.get_variable(\"proj_w\", [size, target_vocab_size])\n",
    "            w_t = tf.transpose(w)\n",
    "            b = tf.get_variable(\"proj_b\", [target_vocab_size])\n",
    "            output_projection = (w, b)\n",
    "\n",
    "            def sampled_loss(inputs, labels):\n",
    "                \n",
    "                labels = tf.reshape(labels, [-1, 1])\n",
    "                local_w_t = tf.cast(w_t, tf.float32)\n",
    "                local_b = tf.cast(b, tf.float32)\n",
    "                local_inputs = tf.cast(inputs, tf.float32)\n",
    "                \n",
    "                return tf.cast(\n",
    "                    tf.nn.sampled_softmax_loss(local_w_t, local_b, labels, local_inputs,\n",
    "                                               num_samples, target_vocab_size), dtype)\n",
    "            softmax_loss_function = sampled_loss\n",
    "\n",
    "        # Create the internal multi-layer cell for our RNN.\n",
    "        single_cell = tf.contrib.rnn.GRUCell(size)\n",
    "        print('single_cell')\n",
    "        if use_lstm:\n",
    "            single_cell = tf.contrib.rnn.LSTMCell(size)\n",
    "        cell = single_cell\n",
    "        cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=0.5)\n",
    "        if num_layers > 1:\n",
    "            cell = tf.contrib.rnn.MultiRNNCell([single_cell] * num_layers)\n",
    "        # tf.contrib.seq2seq\n",
    "        # The seq2seq function: we use embedding for the input and attention.\n",
    "        def seq2seq_f(encoder_inputs, decoder_inputs, do_decode):\n",
    "            return embedding_attention_seq2seq(\n",
    "                  encoder_inputs, decoder_inputs, cell,\n",
    "                  num_encoder_symbols=source_vocab_size,\n",
    "                  num_decoder_symbols=target_vocab_size,\n",
    "                  embedding_size=size,\n",
    "                  output_projection=output_projection,\n",
    "                  feed_previous=do_decode)\n",
    "\n",
    "        # Feeds for inputs.\n",
    "        self.encoder_inputs = []\n",
    "        self.decoder_inputs = []\n",
    "        self.target_weights = []\n",
    "        for i in xrange(self.buckets[-1][0]):  # Last bucket is the biggest one.\n",
    "            self.encoder_inputs.append(tf.placeholder(tf.int32, shape=[None],\n",
    "                                                name=\"encoder{0}\".format(i)))\n",
    "        for i in xrange(self.buckets[-1][1] + 1):\n",
    "            self.decoder_inputs.append(tf.placeholder(tf.int32, shape=[None],\n",
    "                                                name=\"decoder{0}\".format(i)))\n",
    "            self.target_weights.append(tf.placeholder(tf.float32, shape=[None],\n",
    "                                                name=\"weight{0}\".format(i)))\n",
    "        self.reward = [tf.placeholder(tf.float32, name=\"reward_%i\" % i) for i in range(len(self.buckets))] # ADD\n",
    "        \n",
    "        \n",
    "        # Our targets are decoder inputs shifted by one.\n",
    "        targets = [self.decoder_inputs[i + 1] for i in xrange(len(self.decoder_inputs) - 1)]\n",
    "\n",
    "        self.outputs, self.losses,self.encoder_state  = model_with_buckets(\n",
    "                  self.encoder_inputs, self.decoder_inputs, targets,\n",
    "                  self.target_weights, self.buckets, source_vocab_size, self.batch_size,\n",
    "                  lambda x, y: seq2seq_f(x, y, tf.where(self.forward_only, True, False)),\n",
    "                  output_projection=output_projection,\n",
    "                  softmax_loss_function=softmax_loss_function)\n",
    "        \n",
    "        for b in xrange(len(self.buckets)):\n",
    "            self.outputs[b] = [\n",
    "                tf.cond(\n",
    "                    self.forward_only,\n",
    "                    lambda: tf.matmul(output, output_projection[0]) + output_projection[1],\n",
    "                    lambda: output\n",
    "                )\n",
    "                for output in self.outputs[b]\n",
    "            ]\n",
    "        \n",
    "        if not forward_only:\n",
    "            with tf.name_scope(\"gradient_descent\"):\n",
    "                self.gradient_norms = []\n",
    "                self.updates = []\n",
    "                self.aj_losses = []\n",
    "                self.gen_params = [p for p in tf.trainable_variables() if name_scope in p.name]\n",
    "                opt = tf.train.AdamOptimizer()\n",
    "                for b in xrange(len(self.buckets)):\n",
    "                    R =  tf.subtract(self.reward[b], self.reward_bias)\n",
    "                    adjusted_loss = tf.cond(self.up_reward,\n",
    "                                              lambda:tf.multiply(self.losses[b], self.reward[b]),\n",
    "                                              lambda: self.losses[b])\n",
    "\n",
    "                    self.aj_losses.append(adjusted_loss)\n",
    "                    gradients = tf.gradients(adjusted_loss, self.gen_params)\n",
    "                    clipped_gradients, norm = tf.clip_by_global_norm(gradients, self.max_gradient_norm)\n",
    "                    self.gradient_norms.append(norm)\n",
    "                    self.updates.append(opt.apply_gradients(\n",
    "                        zip(clipped_gradients, self.gen_params), global_step=self.global_step))\n",
    "\n",
    "        self.gen_variables = [k for k in tf.global_variables() if name_scope in k.name]\n",
    "        self.saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "    def step(self, session, encoder_inputs, decoder_inputs, target_weights,\n",
    "           bucket_id, forward_only=True, reward=1, mc_search=False, up_reward=False, debug=True):\n",
    "        #print(\"model.step\")\n",
    "        # Check if the sizes match.\n",
    "        encoder_size, decoder_size = self.buckets[bucket_id]\n",
    "        if len(encoder_inputs) != encoder_size:\n",
    "            raise ValueError(\"Encoder length must be equal to the one in bucket,\"\n",
    "                       \" %d != %d.\" % (len(encoder_inputs), encoder_size))\n",
    "        if len(decoder_inputs) != decoder_size:\n",
    "            raise ValueError(\"Decoder length must be equal to the one in bucket,\"\n",
    "                       \" %d != %d.\" % (len(decoder_inputs), decoder_size))\n",
    "        if len(target_weights) != decoder_size:\n",
    "            raise ValueError(\"Weights length must be equal to the one in bucket,\"\n",
    "                       \" %d != %d.\" % (len(target_weights), decoder_size))\n",
    "        #print(\"input feed\")\n",
    "        # Input feed: encoder inputs, decoder inputs, target_weights, as provided.\n",
    "        input_feed = {self.forward_only.name: forward_only,\n",
    "                      self.up_reward.name:  up_reward,\n",
    "                      self.mc_search.name: mc_search\n",
    "                     }\n",
    "        for l in xrange(len(self.buckets)):\n",
    "            input_feed[self.reward[l].name] = reward\n",
    "        for l in xrange(encoder_size):\n",
    "            input_feed[self.encoder_inputs[l].name] = encoder_inputs[l]\n",
    "        for l in xrange(decoder_size):\n",
    "            input_feed[self.decoder_inputs[l].name] = decoder_inputs[l]\n",
    "            input_feed[self.target_weights[l].name] = target_weights[l]\n",
    "        #print(\"last_target\")\n",
    "        # Since our targets are decoder inputs shifted by one, we need one more.\n",
    "        last_target = self.decoder_inputs[decoder_size].name\n",
    "        input_feed[last_target] = np.zeros([self.batch_size], dtype=np.int32)\n",
    "\n",
    "        # Output feed: depends on whether we do a backward step or not.\n",
    "        if not forward_only:\n",
    "            #print(\"not forward_only\")\n",
    "            output_feed = [self.updates[bucket_id],  # Update Op that does SGD.\n",
    "                     self.gradient_norms[bucket_id],  # Gradient norm.\n",
    "                     self.losses[bucket_id]]  # Loss for this batch.\n",
    "        else:\n",
    "            #print(\"forward_only\")\n",
    "            output_feed = [self.losses[bucket_id]]  # Loss for this batch.\n",
    "            for l in xrange(decoder_size):  # Output logits.\n",
    "                output_feed.append(self.outputs[bucket_id][l])\n",
    "\n",
    "        outputs = session.run(output_feed, input_feed)\n",
    "        if not forward_only:\n",
    "              return outputs[1], outputs[2], None  # Gradient norm, loss, no outputs.\n",
    "        else:\n",
    "              return None, outputs[0], outputs[1:]  # No gradient norm, loss, outputs.\n",
    "\n",
    "    def get_batch(self, data, bucket_id):\n",
    "\n",
    "        encoder_size, decoder_size = self.buckets[bucket_id]\n",
    "        encoder_inputs, decoder_inputs = [], []\n",
    "\n",
    "        for _ in xrange(self.batch_size):\n",
    "            encoder_input, decoder_input = random.choice(data[bucket_id])\n",
    "\n",
    "            # Encoder inputs are padded and then reversed.\n",
    "            encoder_pad = [PAD_ID] * (encoder_size - len(encoder_input))\n",
    "            encoder_inputs.append(list(reversed(encoder_input + encoder_pad)))\n",
    "\n",
    "            # Decoder inputs get an extra \"GO\" symbol, and are padded then.\n",
    "            decoder_pad_size = decoder_size - len(decoder_input) - 1\n",
    "            decoder_inputs.append([GO_ID] + decoder_input +\n",
    "                            [PAD_ID] * decoder_pad_size)\n",
    "\n",
    "        # Now we create batch-major vectors from the data selected above.\n",
    "        batch_encoder_inputs, batch_decoder_inputs, batch_weights = [], [], []\n",
    "\n",
    "        # Batch encoder inputs are just re-indexed encoder_inputs.\n",
    "        for length_idx in xrange(encoder_size):\n",
    "            batch_encoder_inputs.append(\n",
    "                np.array([encoder_inputs[batch_idx][length_idx]\n",
    "                    for batch_idx in xrange(self.batch_size)], dtype=np.int32))\n",
    "\n",
    "        # Batch decoder inputs are re-indexed decoder_inputs, we create weights.\n",
    "        for length_idx in xrange(decoder_size):\n",
    "            batch_decoder_inputs.append(\n",
    "                np.array([decoder_inputs[batch_idx][length_idx]\n",
    "                    for batch_idx in xrange(self.batch_size)], dtype=np.int32))\n",
    "\n",
    "            # Create target_weights to be 0 for targets that are padding.\n",
    "            batch_weight = np.ones(self.batch_size, dtype=np.float32)\n",
    "            for batch_idx in xrange(self.batch_size):\n",
    "                # We set weight to 0 if the corresponding target is a PAD symbol.\n",
    "                # The corresponding target is decoder_input shifted by 1 forward.\n",
    "                if length_idx < decoder_size - 1:\n",
    "                    target = decoder_inputs[batch_idx][length_idx + 1]\n",
    "                if length_idx == decoder_size - 1 or target == PAD_ID:\n",
    "                    batch_weight[batch_idx] = 0.0\n",
    "            batch_weights.append(batch_weight)\n",
    "        return batch_encoder_inputs, batch_decoder_inputs, batch_weights, encoder_inputs,decoder_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(config):\n",
    "\n",
    "    train_data_set = [[] for _ in config.buckets]\n",
    "    dev_data_set = [[] for _ in config.buckets]\n",
    "\n",
    "    s2s_data = pd.read_csv('_news_data.csv')\n",
    "    train_data = s2s_data[0:900000]\n",
    "    dev_data = s2s_data[900000:1000000]\n",
    "    \n",
    "    train_source = train_data['news_content']\n",
    "    train_target = train_data['news_summary']\n",
    "    \n",
    "    dev_source = dev_data['news_content']\n",
    "    dev_target = dev_data['news_summary']\n",
    "    \n",
    "    vocabulary = json.load(open('newdataset_vocabulary.json','r'))\n",
    "    \n",
    "    for m in range(0,len(train_source)):\n",
    "        if m % 200000 == 0:\n",
    "            print(\"  reading train data line %d\" % m)\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "        source_sen = train_source[m]\n",
    "        target_sen = train_target[m]\n",
    "            \n",
    "        source_word = []\n",
    "        target_word = []\n",
    "        \n",
    "        try:\n",
    "            s_w = source_sen.split(' ')\n",
    "        except:\n",
    "            s_w = ['float']\n",
    "        for i in range(0,len(s_w)):\n",
    "            s_w[i] = s_w[i].replace('.','')\n",
    "            s_w[i] = s_w[i].replace(',','')\n",
    "            s_w[i] = s_w[i].replace('?','')\n",
    "            s_w[i] = s_w[i].replace('(','')\n",
    "            s_w[i] = s_w[i].replace(')','')\n",
    "            s_w[i] = s_w[i].replace('!','')\n",
    "            s_w[i] = s_w[i].lower()\n",
    "            source_word.append(s_w[i])\n",
    "            \n",
    "        s_w = target_sen.split(' ')\n",
    "        for i in range(0,len(s_w)):\n",
    "            s_w[i] = s_w[i].replace('.','')\n",
    "            s_w[i] = s_w[i].replace(',','')\n",
    "            s_w[i] = s_w[i].replace('?','')\n",
    "            s_w[i] = s_w[i].replace('(','')\n",
    "            s_w[i] = s_w[i].replace(')','')\n",
    "            s_w[i] = s_w[i].replace('!','')\n",
    "            s_w[i] = s_w[i].lower()\n",
    "            target_word.append(s_w[i])            \n",
    "            \n",
    "        source_ids = []\n",
    "        target_ids = []\n",
    "            \n",
    "        # convert to number\n",
    "        \n",
    "        for w in source_word:\n",
    "            try:\n",
    "                source_ids.append(vocabulary[w])\n",
    "            except:\n",
    "                source_ids.append(3)# 3 is UNK token\n",
    "                \n",
    "        # add start tag to target sentence\n",
    "        #target_ids.append(1) # 1 is _GO: start token\n",
    "  \n",
    "        for w in target_word:\n",
    "            try:\n",
    "                target_ids.append(vocabulary[w])\n",
    "            except:\n",
    "                target_ids.append(3)# 3 is UNK token\n",
    "\n",
    "        # add finish tag to target sentence. \n",
    "        target_ids.append(EOS_ID) # EOS_ID is end token\n",
    "        \n",
    "        \n",
    "        # bucket data\n",
    "        #config.buckets:  [bucket_id, (source_size, target_size)]\n",
    "        for bucket_id, (source_size, target_size) in enumerate(config.buckets): #[bucket_id, (source_size, target_size)]\n",
    "            if len(source_ids) < source_size and len(target_ids) < target_size:\n",
    "                train_data_set[bucket_id].append([source_ids, target_ids])\n",
    "                break\n",
    "                 \n",
    "    for m in range(900000,900000+len(dev_source)):\n",
    "        if m % 200000 == 0:\n",
    "            print(\"  reading dev data line %d\" % m)\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "        source_sen = dev_source[m]\n",
    "        target_sen = dev_target[m]\n",
    " \n",
    "        source_word = []\n",
    "        target_word = []\n",
    "        \n",
    "        try:\n",
    "            s_w = source_sen.split(' ')\n",
    "        except:\n",
    "            s_w = ['float']\n",
    "        for i in range(0,len(s_w)):\n",
    "            s_w[i] = s_w[i].replace('.','')\n",
    "            s_w[i] = s_w[i].replace(',','')\n",
    "            s_w[i] = s_w[i].replace('?','')\n",
    "            s_w[i] = s_w[i].replace('(','')\n",
    "            s_w[i] = s_w[i].replace(')','')\n",
    "            s_w[i] = s_w[i].replace('!','')\n",
    "            s_w[i] = s_w[i].lower()\n",
    "            source_word.append(s_w[i])\n",
    "            \n",
    "        s_w = target_sen.split(' ')\n",
    "        for i in range(0,len(s_w)):\n",
    "            s_w[i] = s_w[i].replace('.','')\n",
    "            s_w[i] = s_w[i].replace(',','')\n",
    "            s_w[i] = s_w[i].replace('?','')\n",
    "            s_w[i] = s_w[i].replace('(','')\n",
    "            s_w[i] = s_w[i].replace(')','')\n",
    "            s_w[i] = s_w[i].replace('!','')\n",
    "            s_w[i] = s_w[i].lower()\n",
    "            target_word.append(s_w[i])    \n",
    "            \n",
    "        source_ids = []\n",
    "        target_ids = []\n",
    "            \n",
    "        # convert to number\n",
    "        for w in source_word:\n",
    "            try:\n",
    "                source_ids.append(vocabulary[w])\n",
    "            except:\n",
    "                source_ids.append(3)\n",
    "                \n",
    "        #target_ids.append(1) # 1 is _GO: start token\n",
    "        for w in target_word:\n",
    "            try:\n",
    "                target_ids.append(vocabulary[w])\n",
    "            except:\n",
    "                target_ids.append(3)\n",
    "\n",
    "        # add finish tag to target sentence. \n",
    "        target_ids.append(EOS_ID) # End token\n",
    "                \n",
    "        # bucket data\n",
    "        #config.buckets:  [bucket_id, (source_size, target_size)]\n",
    "        for bucket_id, (source_size, target_size) in enumerate(config.buckets): #[bucket_id, (source_size, target_size)]\n",
    "            if len(source_ids) < source_size and len(target_ids) < target_size:\n",
    "                dev_data_set[bucket_id].append([source_ids, target_ids])\n",
    "                break\n",
    "                                \n",
    "    return vocabulary,train_data_set,dev_data_set\n",
    "    #,train_query,dev_query\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_data(gen_config):\n",
    "    vocab, train_set,dev_set = read_data(gen_config)\n",
    "    rev_vocab = {v: k for k, v in vocab.items()}\n",
    "    return vocab, rev_vocab, dev_set, train_set\n",
    "    #,train_query,dev_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_model(session, gen_config, forward_only,name_scope, initializer=None):\n",
    "    \"\"\"Create translation model and initialize or load parameters in session.\"\"\"\n",
    "    with tf.variable_scope(name_or_scope=name_scope, initializer=initializer):\n",
    "        model = Seq2SeqModel(gen_config, name_scope=name_scope, forward_only=forward_only)\n",
    "        \n",
    "        # checkpoint\n",
    "        gen_ckpt_dir = os.path.abspath(os.path.join(gen_config.train_dir, \"checkpoints\"))\n",
    "        # os.path.abspath: 返回path规范化的绝对路径。 \n",
    "        ckpt = tf.train.get_checkpoint_state(gen_ckpt_dir)\n",
    "        \n",
    "        if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\n",
    "            print(\"Reading Gen model parameters from %s\" % ckpt.model_checkpoint_path)\n",
    "            model.saver.restore(session, ckpt.model_checkpoint_path)\n",
    "            #return model\n",
    "        else:\n",
    "            print(\"Created Gen model with fresh parameters.\")\n",
    "            gen_global_variables = [gv for gv in tf.global_variables() if name_scope in gv.name]\n",
    "            session.run(tf.variables_initializer(gen_global_variables))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(gen_config):\n",
    "    \n",
    "    vocab, rev_vocab, dev_set, train_set = prepare_data(gen_config)\n",
    "    \n",
    "    for b_set in train_set:\n",
    "        print(\"bucket_set: \", len(b_set))\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "    #with tf.device(\"/gpu:1\"):\n",
    "        # Create model.\n",
    "        print(\"Creating %d layers of %d units.\" % (gen_config.num_layers, gen_config.emb_dim))\n",
    "        model = create_model(sess, gen_config, forward_only=False,name_scope=gen_config.name_model)\n",
    "        \n",
    "        #size of each bucket in the train_dataset.\n",
    "        train_bucket_sizes = [len(train_set[b]) for b in xrange(len(gen_config.buckets))]\n",
    "        # add them to get total train dataset size\n",
    "        train_total_size = float(sum(train_bucket_sizes))\n",
    "        # 0~1 --> id of bucket data.\n",
    "        train_buckets_scale = [sum(train_bucket_sizes[:i + 1]) / train_total_size\n",
    "                               for i in xrange(len(train_bucket_sizes))]\n",
    "\n",
    "        # This is the training loop.\n",
    "        step_time, loss = 0.0, 0.0\n",
    "        current_step = 0\n",
    "        previous_losses = []\n",
    "        gen_loss_summary = tf.Summary()\n",
    "        gen_writer = tf.summary.FileWriter(gen_config.tensorboard_dir, sess.graph)\n",
    "        print(\"training.......\")\n",
    "        while True:\n",
    "            # Choose a bucket.\n",
    "            # Choose a bucket according to data distribution. We pick a random number\n",
    "            # in [0, 1] and use the corresponding interval in train_buckets_scale.\n",
    "            random_number_01 = np.random.random_sample()\n",
    "            bucket_id = min([i for i in xrange(len(train_buckets_scale)) if train_buckets_scale[i] > random_number_01])\n",
    "\n",
    "            \n",
    "            # Get a batch and make a step.\n",
    "            start_time = time.time()\n",
    "            encoder_inputs, decoder_inputs, target_weights,_,_ = model.get_batch(train_set, bucket_id)\n",
    "            # model.get_batch 和 model.step 都是seq2seq里的函数。\n",
    "            _, step_loss, _ = model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id, forward_only=False)\n",
    "\n",
    "            step_time += (time.time() - start_time) / gen_config.steps_per_checkpoint\n",
    "            loss += step_loss / gen_config.steps_per_checkpoint\n",
    "            current_step += 1\n",
    "\n",
    "            \n",
    "            \n",
    "            if current_step % gen_config.steps_per_checkpoint == 0:\n",
    "                bucket_value = gen_loss_summary.value.add()\n",
    "                bucket_value.tag = gen_config.name_loss\n",
    "                bucket_value.simple_value = float(loss)\n",
    "                gen_writer.add_summary(gen_loss_summary, int(model.global_step.eval()))\n",
    "\n",
    "                # Print statistics for the previous epoch.\n",
    "                perplexity = math.exp(loss) if loss < 300 else float('inf')\n",
    "                print (\"global step %d learning rate %.4f step-time %.2f perplexity \"\n",
    "                       \"%.2f\" % (model.global_step.eval(), model.learning_rate.eval(),\n",
    "                                 step_time, perplexity))\n",
    "\n",
    "                # Decrease learning rate if no improvement was seen over last 3 times.\n",
    "                if len(previous_losses) > 2 and loss > max(previous_losses[-3:]):\n",
    "                    sess.run(model.learning_rate_decay_op)\n",
    "                previous_losses.append(loss)\n",
    "                \n",
    "                if current_step % (gen_config.steps_per_checkpoint * 3) == 0:\n",
    "                    print(\"current_step: %d, save model\" %(current_step))\n",
    "                    gen_ckpt_dir = os.path.abspath(os.path.join(gen_config.train_dir, \"checkpoints\"))\n",
    "                    if not os.path.exists(gen_ckpt_dir):\n",
    "                        #os.makedirs() 方法用于递归创建目录\n",
    "                        os.makedirs(gen_ckpt_dir)\n",
    "                    checkpoint_path = os.path.join(gen_ckpt_dir, \"chitchat.model\")\n",
    "                    model.saver.save(sess, checkpoint_path, global_step=model.global_step)\n",
    "\n",
    "                step_time, loss = 0.0, 0.0\n",
    "                sys.stdout.flush()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class gen_config(object):\n",
    "    beam_size = 10\n",
    "    learning_rate = 0.5\n",
    "    learning_rate_decay_factor = 0.99\n",
    "    max_gradient_norm = 2.0\n",
    "    batch_size = 64\n",
    "    emb_dim = 256\n",
    "    num_layers = 4\n",
    "    vocab_size = 250000\n",
    "    name_model = \"st_model\"\n",
    "    train_dir = \"./gen_data_p/\"\n",
    "    tensorboard_dir = \"./p_tensorboard/gen_log/\"\n",
    "    name_loss = \"gen_loss\"\n",
    "    teacher_loss = \"teacher_loss\"\n",
    "    reward_name = \"reward\"\n",
    "    max_train_data_size = 0\n",
    "    steps_per_checkpoint = 100\n",
    "    buckets = [(125,10),(125,12),(125,15),(125,30)]\n",
    "    buckets_concat = [(125,10),(125,12),(125,15),(125,30)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# train(gen_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## decode (generate data for discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class gen_config(object):\n",
    "    beam_size = 10\n",
    "    learning_rate = 0.5\n",
    "    learning_rate_decay_factor = 0.99\n",
    "    max_gradient_norm = 2.0\n",
    "    batch_size = 1\n",
    "    emb_dim = 256\n",
    "    num_layers = 4\n",
    "    vocab_size = 250000\n",
    "    name_model = \"st_model\"\n",
    "    train_dir = \"./gen_data_t/\"\n",
    "    tensorboard_dir = \"./t_tensorboard/gen_log/\"\n",
    "    name_loss = \"gen_loss\"\n",
    "    teacher_loss = \"teacher_loss\"\n",
    "    reward_name = \"reward\"\n",
    "    max_train_data_size = 0\n",
    "    steps_per_checkpoint = 100\n",
    "    buckets = [(120,10),(123,12),(125,15),(131,30)]\n",
    "    buckets_concat = [(120,10),(123,12),(125,15),(131,30)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode(decode_num_step):\n",
    "    \n",
    "    # Load vocabularies.\n",
    "    vocab, rev_vocab, dev_set, train_set = prepare_data(gen_config)\n",
    "        \n",
    "        \n",
    "    train_bucket_sizes = [len(train_set[b]) for b in xrange(len(gen_config.buckets))]\n",
    "    train_total_size = float(sum(train_bucket_sizes))\n",
    "    train_buckets_scale = [sum(train_bucket_sizes[:i + 1]) / train_total_size\n",
    "                           for i in xrange(len(train_bucket_sizes))]\n",
    "    \n",
    "    input_document = []\n",
    "    target_summary = []\n",
    "    generated_summary = []\n",
    "    with tf.Session() as sess:\n",
    "        # Create model and load parameters.\n",
    "        model = create_model(sess, gen_config, forward_only=True, name_scope=gen_config.name_model)\n",
    "\n",
    "        num_step = 0\n",
    "        while num_step < decode_num_step:\n",
    "            print(\"generating num_step: \", num_step)\n",
    "            random_number_01 = np.random.random_sample()\n",
    "            bucket_id = min([i for i in xrange(len(train_buckets_scale))\n",
    "                             if train_buckets_scale[i] > random_number_01])\n",
    "            # Get a 1-element batch to feed the sentence to the model.\n",
    "            encoder_inputs, decoder_inputs, target_weights, _encoder_inputs, _decoder_inputs = model.get_batch(\n",
    "                  train_set, bucket_id)#get_batch(train_set,bucket_id)\n",
    "            # Get output logits for the sentence.\n",
    "            _, _, output_logits = model.step(sess, encoder_inputs, decoder_inputs,target_weights, bucket_id, True)\n",
    "            # This is a greedy decoder - outputs are just argmaxes of output_logits.\n",
    "            outputs = [int(np.argmax(logit, axis=1)) for logit in output_logits]\n",
    "            #print('batch_encoder_input')\n",
    "            #print(\" \".join([str(rev_vocab[an]) for an in encoder_inputs]))\n",
    "            # If there is an EOS symbol in outputs, cut them at that point.\n",
    "            if EOS_ID in outputs:\n",
    "                outputs = outputs[:outputs.index(EOS_ID)]\n",
    "            # Print out French sentence corresponding to outputs.\n",
    "            summary_g = \" \".join([tf.compat.as_str(rev_vocab[output]) for output in outputs])\n",
    "            print(summary_g)\n",
    "            generated_summary.append(summary_g)\n",
    "            \n",
    "            for query, answer,outputs in zip(_encoder_inputs, _decoder_inputs,outputs):\n",
    "\n",
    "                answer_str = \" \".join([str(rev_vocab[an]) for an in answer])\n",
    "                answer_str = answer_str.replace('_GO ','')\n",
    "                answer_str = answer_str.replace(' _EOS','')\n",
    "                answer_str = answer_str.replace(' _PAD','')\n",
    "                print(answer_str)\n",
    "                target_summary.append(answer_str)\n",
    "                \n",
    "                query_str = \" \".join([str(rev_vocab[qu]) for qu in query])\n",
    "                query_str = query_str.replace(' _PAD','')\n",
    "                #print(query_str)\n",
    "                document_str = query_str.split(' ')\n",
    "                i = len(document_str)-1\n",
    "                d_str = ''\n",
    "                while i>0:\n",
    "                    d_str = d_str + document_str[i]\n",
    "                    d_str = d_str + ' '\n",
    "                    i = i-1\n",
    "                #print(d_str)\n",
    "                input_document.append(d_str)\n",
    "            num_step +=1\n",
    "    return input_document, target_summary, generated_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#list1,list2,list3 = decode(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generated_data_for_D(Flag,decode_num_step):\n",
    "    list1,list2,list3 = decode(decode_num_step)\n",
    "    with open('generated_sample.txt','w')as f:\n",
    "        for i in range(0,len(list3)):\n",
    "            f.write(list3[i])\n",
    "            f.write('\\n')\n",
    "    with open('real_sample.txt','w')as f:\n",
    "        for i in range(0,len(list3)):\n",
    "            f.write(list2[i])\n",
    "            f.write('\\n')\n",
    "    with open('document_sample.txt','w')as f:\n",
    "        for i in range(0,len(list1)):\n",
    "            f.write(list1[i])\n",
    "            f.write('\\n')\n",
    "    if Flag == True:\n",
    "        with open('generated_data_for_D.csv', 'w') as datacsv:\n",
    "            writer = csv.writer(datacsv,dialect=(\"excel\"))\n",
    "            writer.writerow(['generated_id','document_content','target_summary','generated_summary'])\n",
    "            for i in range(0,len(list1)):\n",
    "                writer.writerow([i,list1[i],list2[i],list3[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data = pd.read_csv('generated_data_for_D.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 写入real_sample.txt 和 generated_sample.txt 中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# experiment CNN 输入：\n",
    "# - document + summary 判断\n",
    "# - summary 判断。\n",
    "# 看哪个结果好。\n",
    "# 先试summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Flag = True # Flag = true: write a csv file Flag = False, don't write\n",
    "#Flag = False\n",
    "#decode_num_step = 5000\n",
    "#generated_data_for_D(Flag,decode_num_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Rouge score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#list1 = []\n",
    "#with open(\"generated_sample.txt\",\"r\") as f:\n",
    "#    for line in f:\n",
    "#        list1.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#list1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#list2 = []\n",
    "#with open(\"real_sample.txt\",\"r\") as f:\n",
    "#    for line in f:\n",
    "#        list2.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#list2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'list1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-6f7c27b00dc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mrouge_l_r_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mnum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mhypothesis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mreference\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'list1' is not defined"
     ]
    }
   ],
   "source": [
    "rouge_1_f_sum = 0\n",
    "rouge_1_p_sum = 0\n",
    "rouge_1_r_sum = 0\n",
    "rouge_2_f_sum = 0\n",
    "rouge_2_p_sum = 0\n",
    "rouge_2_r_sum = 0\n",
    "rouge_l_f_sum = 0\n",
    "rouge_l_p_sum = 0\n",
    "rouge_l_r_sum = 0\n",
    "num = 0\n",
    "for i in range(0,len(list1)):\n",
    "    hypothesis = list1[i]\n",
    "    reference = list2[i]\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(reference, hypothesis)\n",
    "    rouge_1_f = scores[0]['rouge-1']['f']\n",
    "    rouge_1_p = scores[0]['rouge-1']['p']\n",
    "    rouge_1_r = scores[0]['rouge-1']['r']\n",
    "    rouge_2_f = scores[0]['rouge-2']['f']\n",
    "    rouge_2_p = scores[0]['rouge-2']['p']\n",
    "    rouge_2_r = scores[0]['rouge-2']['r']\n",
    "    rouge_l_f = scores[0]['rouge-l']['f']\n",
    "    rouge_l_p = scores[0]['rouge-l']['p']\n",
    "    rouge_l_r = scores[0]['rouge-l']['r']\n",
    "    \n",
    "    rouge_1_f_sum = rouge_1_f_sum + rouge_1_f\n",
    "    rouge_1_p_sum = rouge_1_p_sum + rouge_1_p\n",
    "    rouge_1_r_sum = rouge_1_r_sum + rouge_1_r\n",
    "    rouge_2_f_sum = rouge_2_f_sum + rouge_2_f\n",
    "    rouge_2_p_sum = rouge_2_p_sum + rouge_2_p\n",
    "    rouge_2_r_sum = rouge_2_r_sum + rouge_2_r\n",
    "    rouge_l_f_sum = rouge_l_f_sum + rouge_l_f\n",
    "    rouge_l_p_sum = rouge_l_p_sum + rouge_l_p\n",
    "    rouge_l_r_sum = rouge_l_r_sum + rouge_l_r\n",
    "    \n",
    "    num = num + 1\n",
    "\n",
    "rouge_1_F = rouge_1_f_sum/num\n",
    "rouge_1_P = rouge_1_p_sum/num\n",
    "rouge_1_R = rouge_1_r_sum/num\n",
    "rouge_2_F = rouge_2_f_sum/num\n",
    "rouge_2_P = rouge_2_p_sum/num\n",
    "rouge_2_R = rouge_2_r_sum/num\n",
    "rouge_l_F = rouge_l_f_sum/num\n",
    "rouge_l_P = rouge_l_p_sum/num\n",
    "rouge_l_R = rouge_l_r_sum/num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge_1: f: \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'rouge_1_F' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-5afad1d561e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rouge_1: f: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrouge_1_F\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rouge_1: p: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrouge_1_P\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rouge_1: r: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'rouge_1_F' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"rouge_1: f: \")\n",
    "print(rouge_1_F)\n",
    "print(\"rouge_1: p: \")\n",
    "print(rouge_1_P)\n",
    "print(\"rouge_1: r: \")\n",
    "print(rouge_1_R)\n",
    "\n",
    "print(\"rouge_2: f: \")\n",
    "print(rouge_2_F)\n",
    "print(\"rouge_2: p: \")\n",
    "print(rouge_2_P)\n",
    "print(\"rouge_2: r: \")\n",
    "print(rouge_2_R)\n",
    "      \n",
    "print(\"rouge_L: f: \")\n",
    "print(rouge_l_F)\n",
    "print(\"rouge_L: p: \")\n",
    "print(rouge_l_P)\n",
    "print(\"rouge_L: r: \")\n",
    "print(rouge_l_R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Generate samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_predicted_sentence(sess, input_token_ids, vocab, model,\n",
    "                           beam_size, buckets, mc_search=True,debug=False):\n",
    " \n",
    "    def model_step(encoder_inputs, decoder_inputs, dptr, target_weights, bucket_id):\n",
    "        #model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id, True)\n",
    "        _, _, logits = model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id, True)\n",
    "        prob = softmax(logits[dptr][0])\n",
    "        # print(\"model_step @ %s\" % (datetime.now()))\n",
    "        return prob\n",
    "\n",
    "    def greedy_dec(output_logits):\n",
    "        selected_token_ids = [int(np.argmax(logit, axis=1)) for logit in output_logits]\n",
    "        return selected_token_ids\n",
    "\n",
    "    #input_token_ids = data_utils.sentence_to_token_ids(input_sentence, vocab)\n",
    "    # Which bucket does it belong to?\n",
    "    print(buckets)\n",
    "    print(len(input_token_ids))\n",
    "    bucket_id = min([b for b in range(len(buckets)) if buckets[b][0] > len(input_token_ids)])\n",
    "    outputs = []\n",
    "    feed_data = {bucket_id: [(input_token_ids, outputs)]}\n",
    "\n",
    "    # Get a 1-element batch to feed the sentence to the model.   None,bucket_id, True\n",
    "    encoder_inputs, decoder_inputs, target_weights, _, _ = model.get_batch(feed_data, bucket_id)\n",
    "    if debug: print(\"\\n[get_batch]\\n\", encoder_inputs, decoder_inputs, target_weights)\n",
    "\n",
    "    ### Original greedy decoding\n",
    "    if beam_size == 1 or (not mc_search):\n",
    "        _, _, output_logits = model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id, True)\n",
    "        return [{\"dec_inp\": greedy_dec(output_logits), 'prob': 1}]\n",
    "\n",
    "    # Get output logits for the sentence. # initialize beams as (log_prob, empty_string, eos)\n",
    "    beams, new_beams, results = [(1, {'eos': 0, 'dec_inp': decoder_inputs, 'prob': 1, 'prob_ts': 1, 'prob_t': 1})], [], []\n",
    "\n",
    "    for dptr in range(len(decoder_inputs)-1):\n",
    "        if dptr > 0:\n",
    "            target_weights[dptr] = [1.]\n",
    "            beams, new_beams = new_beams[:beam_size], []\n",
    "        if debug: print(\"=====[beams]=====\", beams)\n",
    "        heapq.heapify(beams)  # since we will srot and remove something to keep N elements\n",
    "        for prob, cand in beams:\n",
    "            if cand['eos']:\n",
    "                results += [(prob, cand)]\n",
    "                continue\n",
    "\n",
    "            all_prob_ts = model_step(encoder_inputs, cand['dec_inp'], dptr, target_weights, bucket_id)\n",
    "            all_prob_t  = [0]*len(all_prob_ts)\n",
    "            all_prob    = all_prob_ts\n",
    "\n",
    "            # suppress copy-cat (respond the same as input)\n",
    "            if dptr < len(input_token_ids):\n",
    "                all_prob[input_token_ids[dptr]] = all_prob[input_token_ids[dptr]] * 0.01\n",
    "\n",
    "            # beam search\n",
    "            for c in np.argsort(all_prob)[::-1][:beam_size]:\n",
    "                new_cand = {\n",
    "                    'eos'     : (c == EOS_ID),\n",
    "                    'dec_inp' : [(np.array([c]) if i == (dptr+1) else k) for i, k in enumerate(cand['dec_inp'])],\n",
    "                    'prob_ts' : cand['prob_ts'] * all_prob_ts[c],\n",
    "                    'prob_t'  : cand['prob_t'] * all_prob_t[c],\n",
    "                    'prob'    : cand['prob'] * all_prob[c],\n",
    "                }\n",
    "                new_cand = (new_cand['prob'], new_cand) # for heapq can only sort according to list[0]\n",
    "\n",
    "                if (len(new_beams) < beam_size):\n",
    "                    heapq.heappush(new_beams, new_cand)\n",
    "                elif (new_cand[0] > new_beams[0][0]):\n",
    "                    heapq.heapreplace(new_beams, new_cand)\n",
    "\n",
    "    results += new_beams  # flush last cands\n",
    "\n",
    "    # post-process results\n",
    "    res_cands = []\n",
    "    for prob, cand in sorted(results, reverse=True):\n",
    "        res_cands.append(cand)\n",
    "    return res_cands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_sample(sess ,gen_config, model, vocab, source_inputs, source_outputs, mc_search=True):\n",
    "    sample_inputs = []\n",
    "    sample_labels =[]\n",
    "    rep = []\n",
    "\n",
    "    for source_query, source_answer in zip(source_inputs, source_outputs):\n",
    "        sample_inputs.append(source_query+source_answer)# query + answer\n",
    "        sample_labels.append(1)\n",
    "        responses = get_predicted_sentence(sess, source_query, vocab,\n",
    "                                           model, gen_config.beam_size, gen_config.buckets, mc_search)# source_query\n",
    "\n",
    "        for resp in responses:\n",
    "            if gen_config.beam_size == 1 or (not mc_search):\n",
    "                dec_inp = [dec for dec in resp['dec_inp']]\n",
    "                rep.append(dec_inp)\n",
    "                dec_inp = dec_inp[:]\n",
    "            else:\n",
    "                dec_inp = [dec.tolist()[0] for dec in resp['dec_inp'][:]]\n",
    "                rep.append(dec_inp)\n",
    "                dec_inp = dec_inp[1:]\n",
    "            print(\"  (%s) -> %s\" % (resp['prob'], dec_inp))\n",
    "            sample_neg = source_query + dec_inp\n",
    "            sample_inputs.append(sample_neg)\n",
    "            sample_labels.append(0)\n",
    "\n",
    "    return sample_inputs, sample_labels, rep\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode_beam(decode_num_step):\n",
    "    \n",
    "    # Load vocabularies.\n",
    "    vocab, rev_vocab, dev_set, train_set = prepare_data(gen_config)\n",
    "        \n",
    "        \n",
    "    train_bucket_sizes = [len(train_set[b]) for b in xrange(len(gen_config.buckets))]\n",
    "    train_total_size = float(sum(train_bucket_sizes))\n",
    "    train_buckets_scale = [sum(train_bucket_sizes[:i + 1]) / train_total_size\n",
    "                           for i in xrange(len(train_bucket_sizes))]\n",
    "    \n",
    "    input_document = []\n",
    "    target_summary = []\n",
    "    generated_summary = []\n",
    "    with tf.Session() as sess:\n",
    "        # Create model and load parameters.\n",
    "        model = create_model(sess, gen_config, forward_only=True, name_scope=gen_config.name_model)\n",
    "        # model = create_model(sess, gen_config, forward_only=True, name_scope=gen_config.name_model)\n",
    "        num_step = 0\n",
    "        while num_step < decode_num_step:\n",
    "            print(\"generating num_step: \", num_step)\n",
    "            random_number_01 = np.random.random_sample()\n",
    "            bucket_id = min([i for i in xrange(len(train_buckets_scale))\n",
    "                             if train_buckets_scale[i] > random_number_01])\n",
    "            # Get a 1-element batch to feed the sentence to the model.\n",
    "            encoder_inputs, decoder_inputs, target_weights, _encoder_inputs, _decoder_inputs = model.get_batch(\n",
    "                  train_set, bucket_id)#get_batch(train_set,bucket_id)\n",
    "            \n",
    "            #print(_encoder_inputs)\n",
    "            print(_decoder_inputs)\n",
    "            sample_inputs, sample_labels, rep = gen_sample(sess ,gen_config, model, vocab, _encoder_inputs, _decoder_inputs, mc_search=True)\n",
    "            #print(sample_inputs)\n",
    "            #print(sample_labels)\n",
    "            print(rep)\n",
    "            print(\"*****************************************************\")\n",
    "            for i in range(0,len(rep)):\n",
    "                answer_str = \" \".join([str(rev_vocab[an]) for an in rep[i]])\n",
    "                answer_str = answer_str.replace('_GO ','')\n",
    "                answer_str = answer_str.replace(' _EOS','')\n",
    "                answer_str = answer_str.replace(' _PAD','')\n",
    "                print(answer_str)\n",
    "                \n",
    "            answer = \" \".join([str(rev_vocab[an]) for an in _decoder_inputs[0]])\n",
    "            answer = answer.replace('_GO ','')\n",
    "            answer = answer.replace(' _EOS','')\n",
    "            answer = answer.replace(' _PAD','')\n",
    "            print(answer)  \n",
    "            print(\"*****************************************************\")\n",
    "            num_step +=1\n",
    "    return sample_inputs, rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  reading train data line 0\n",
      "  reading train data line 200000\n",
      "  reading train data line 400000\n",
      "  reading train data line 600000\n",
      "  reading train data line 800000\n",
      "single_cell\n",
      "Reading Gen model parameters from /Users/zhangyiman/final project/ Seq2Seq_GAN_Try_Again/gen_data_p/checkpoints/chitchat.model-31500\n",
      "generating num_step:  0\n",
      "[[1, 2691, 2242, 17616, 102939, 5655, 252, 2024, 2209, 537, 93, 5, 4, 1503, 13269, 8, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "[(120, 10), (123, 12), (125, 15), (131, 30)]\n",
      "131\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "min() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-5597182ae155>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdecode_beam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-39-32b2f4138dab>\u001b[0m in \u001b[0;36mdecode_beam\u001b[0;34m(decode_num_step)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;31m#print(_encoder_inputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_decoder_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0msample_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mgen_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_encoder_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_decoder_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmc_search\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0;31m#print(sample_inputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;31m#print(sample_labels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-38-2ebbd3f753c7>\u001b[0m in \u001b[0;36mgen_sample\u001b[0;34m(sess, gen_config, model, vocab, source_inputs, source_outputs, mc_search)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0msample_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         responses = get_predicted_sentence(sess, source_query, vocab,\n\u001b[0;32m---> 10\u001b[0;31m                                            model, gen_config.beam_size, gen_config.buckets, mc_search)# source_query\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mresp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresponses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-c5d3e0823b19>\u001b[0m in \u001b[0;36mget_predicted_sentence\u001b[0;34m(sess, input_token_ids, vocab, model, beam_size, buckets, mc_search, debug)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuckets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_token_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mbucket_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuckets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mbuckets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_token_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mfeed_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mbucket_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_token_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: min() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "decode_beam(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary = json.load(open('newdataset_vocabulary.json','r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dis_dataloader():\n",
    "    def __init__(self, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        self.sentences = np.array([])\n",
    "        self.labels = np.array([])\n",
    "\n",
    "    def load_train_data(self,positive_file, negative_file):\n",
    "        # Load data\n",
    "        positive_examples = []\n",
    "        negative_examples = []\n",
    "        \n",
    "        positive_examples_word = []\n",
    "        positive_examples = list(codecs.open(positive_file, \"r\", \"utf-8\").readlines())\n",
    "        for s in positive_examples:\n",
    "            #p_w = re.split(r'(;|,|\\s)\\s*', s)\n",
    "            p_w = s.split()\n",
    "            positive_examples_word.append(p_w)\n",
    "                \n",
    "        negative_examples_word = []\n",
    "        negative_examples = list(codecs.open(negative_file, \"r\", \"utf-8\").readlines())\n",
    "        for s in negative_examples:\n",
    "            #n_w = re.split(r'(;|,|\\s)\\s*', s)\n",
    "            #n_w = n_w.remove(' ')\n",
    "            n_w = s.split()\n",
    "            negative_examples_word.append(n_w)        \n",
    "        \n",
    "        \n",
    "        # use part of data to test the model\n",
    "        #positive_examples_word = positive_examples_word[:5000]\n",
    "        #negative_examples_word = negative_examples_word[:5000]\n",
    "        \n",
    "        #Pads all sentences to the same length.#这里需要改为 全都变为一个长度 比如400之类的 然后把dis传入的参数\n",
    "        #里的 senquence length改为一样的。\n",
    "        padding_word=\"<PAD/>\"\n",
    "\n",
    "        positive_length = max(len(x) for x in positive_examples_word)\n",
    "        negative_length = max(len(x) for x in negative_examples_word)\n",
    "        if positive_length > negative_length:\n",
    "            sequence_length = positive_length\n",
    "        else:\n",
    "            sequence_length = negative_length\n",
    "            \n",
    "        padded_positive_examples = []\n",
    "        for i in range(len(positive_examples_word)):\n",
    "            sentence = positive_examples_word[i]\n",
    "            num_padding = sequence_length - len(sentence)\n",
    "            new_sentence = sentence + [padding_word] * num_padding\n",
    "            padded_positive_examples.append(new_sentence)\n",
    "        \n",
    "        padded_negative_examples = []\n",
    "        for i in range(len(negative_examples_word)):\n",
    "            sentence = negative_examples_word[i]\n",
    "            num_padding = sequence_length - len(sentence)\n",
    "            new_sentence = sentence + [padding_word] * num_padding\n",
    "            padded_negative_examples.append(new_sentence)\n",
    "        \n",
    "        self.sentences = padded_positive_examples + padded_negative_examples\n",
    "        \n",
    "        # Generate labels\n",
    "        positive_labels = [[0, 1] for _ in positive_examples_word]\n",
    "        negative_labels = [[1, 0] for _ in negative_examples_word]\n",
    "        #############################################################################\n",
    "        self.labels = np.concatenate([positive_labels, negative_labels], 0)\n",
    "\n",
    "        a = self.sentences\n",
    "        b = self.labels\n",
    "        \n",
    "        a,b = shuffle(a,b)\n",
    "        \n",
    "        self.sentences = a\n",
    "        self.lables = b\n",
    "        \n",
    "        #############################################################################\n",
    "        sentences = self.sentences\n",
    "        for sentence in sentences:\n",
    "            for i in range(0,len(sentence)):\n",
    "                sentence[i] = sentence[i].replace('.','')\n",
    "                sentence[i] = sentence[i].replace(',','')\n",
    "                sentence[i] = sentence[i].replace('?','')\n",
    "                sentence[i] = sentence[i].replace('(','')\n",
    "                sentence[i] = sentence[i].replace(')','')\n",
    "                sentence[i] = sentence[i].replace('!','')\n",
    "                sentence[i] = sentence[i].lower()\n",
    "                \n",
    "        for sentence in sentences:\n",
    "            for i in range(0,len(sentence)):\n",
    "                try: \n",
    "                    sentence[i] = vocabulary[sentence[i]]\n",
    "                except: \n",
    "                    sentence[i] = 3 # len(vocabulary) + 1\n",
    "        self.sentences = np.array(sentences)\n",
    "        \n",
    "        # Split batches\n",
    "        self.num_batch = int(len(self.labels) / self.batch_size)\n",
    "        self.sentences = self.sentences[:self.num_batch * self.batch_size]\n",
    "        self.labels = self.labels[:self.num_batch * self.batch_size]\n",
    "        self.sentences_batches = np.split(self.sentences, self.num_batch, 0)\n",
    "        self.labels_batches = np.split(self.labels, self.num_batch, 0)\n",
    "\n",
    "        self.pointer = 0\n",
    "\n",
    "\n",
    "    def next_batch(self):\n",
    "        ret = self.sentences_batches[self.pointer], self.labels_batches[self.pointer]\n",
    "        self.pointer = (self.pointer + 1) % self.num_batch\n",
    "        return ret\n",
    "\n",
    "    def reset_pointer(self):\n",
    "        self.pointer = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# An alternative to tf.nn.rnn_cell._linear function, which has been removed in Tensorfow 1.0.1\n",
    "# The highway layer is borrowed from https://github.com/mkroutikov/tf-lstm-char-cnn\n",
    "def dis_linear(input_, output_size, scope=None):\n",
    "    '''\n",
    "    Linear map: output[k] = sum_i(Matrix[k, i] * input_[i] ) + Bias[k]\n",
    "    Args:\n",
    "    input_: a tensor or a list of 2D, batch x n, Tensors.\n",
    "    output_size: int, second dimension of W[i].\n",
    "    scope: VariableScope for the created subgraph; defaults to \"Linear\".\n",
    "  Returns:\n",
    "    A 2D Tensor with shape [batch x output_size] equal to\n",
    "    sum_i(input_[i] * W[i]), where W[i]s are newly created matrices.\n",
    "  Raises:\n",
    "    ValueError: if some of the arguments has unspecified or wrong shape.\n",
    "  '''\n",
    "\n",
    "    shape = input_.get_shape().as_list()\n",
    "    if len(shape) != 2:\n",
    "        raise ValueError(\"Linear is expecting 2D arguments: %s\" % str(shape))\n",
    "    if not shape[1]:\n",
    "        raise ValueError(\"Linear expects shape[1] of arguments: %s\" % str(shape))\n",
    "    input_size = shape[1]\n",
    "\n",
    "    # Now the computation.\n",
    "    with tf.variable_scope(scope or \"SimpleLinear\"):\n",
    "        matrix = tf.get_variable(\"Matrix\", [output_size, input_size], dtype=input_.dtype)\n",
    "        bias_term = tf.get_variable(\"Bias\", [output_size], dtype=input_.dtype)\n",
    "\n",
    "    return tf.matmul(input_, tf.transpose(matrix)) + bias_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def highway(input_, size, num_layers=1, bias=-2.0, f=tf.nn.relu, scope='Highway'):\n",
    "    \"\"\"Highway Network (cf. http://arxiv.org/abs/1505.00387).\n",
    "    t = sigmoid(Wy + b)\n",
    "    z = t * g(Wy + b) + (1 - t) * y\n",
    "    where g is nonlinearity, t is transform gate, and (1 - t) is carry gate.\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.variable_scope(scope):\n",
    "        for idx in range(num_layers):\n",
    "            g = f(dis_linear(input_, size, scope='highway_lin_%d' % idx))\n",
    "\n",
    "            t = tf.sigmoid(dis_linear(input_, size, scope='highway_gate_%d' % idx) + bias)\n",
    "\n",
    "            output = t * g + (1. - t) * input_\n",
    "            input_ = output\n",
    "\n",
    "    return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Discriminator(object):\n",
    "    '''A CNN for classification: \n",
    "    Distinguish between the ground truth answer and the generated answer\n",
    "    CNN: embedding layer --> convolutional, max pooling-->softmax '''\n",
    "    \n",
    "    # parameters\n",
    "    def __init__(\n",
    "                self, \n",
    "                sequence_length, \n",
    "                num_classes, \n",
    "                vocab_size,\n",
    "                embedding_size, \n",
    "                filter_sizes, \n",
    "                num_filters, \n",
    "                l2_reg_lambda=0.0):\n",
    "        \n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "        \n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        l2_loss = tf.constant(0.0)\n",
    "        \n",
    "        with tf.variable_scope('discriminator'):\n",
    "            # Embedding layer\n",
    "            with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "                self.W = tf.Variable(\n",
    "                    tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\n",
    "                    name=\"W\")\n",
    "                self.embedded_chars = tf.nn.embedding_lookup(self.W, self.input_x)\n",
    "                self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "\n",
    "\n",
    "            # Create a convolution + maxpool layer for each filter size\n",
    "            pooled_outputs = []\n",
    "            for filter_size, num_filter in zip(filter_sizes, num_filters):\n",
    "                with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                    # Convolution Layer\n",
    "                    filter_shape = [filter_size, embedding_size, 1, num_filter]\n",
    "                    W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                    b = tf.Variable(tf.constant(0.1, shape=[num_filter]), name=\"b\")\n",
    "                    conv = tf.nn.conv2d(\n",
    "                        self.embedded_chars_expanded,\n",
    "                        W,\n",
    "                        strides=[1, 1, 1, 1],\n",
    "                        padding=\"VALID\",\n",
    "                        name=\"conv\")\n",
    "                    # Apply nonlinearity\n",
    "                    h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                    # Maxpooling over the outputs\n",
    "                    pooled = tf.nn.max_pool(\n",
    "                        h,\n",
    "                        ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                        strides=[1, 1, 1, 1],\n",
    "                        padding='VALID',\n",
    "                        name=\"pool\")\n",
    "                    pooled_outputs.append(pooled)\n",
    "            \n",
    "            # Combine all the pooled features\n",
    "            num_filters_total = sum(num_filters)\n",
    "            self.h_pool = tf.concat(pooled_outputs, 3)\n",
    "            self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "\n",
    "            # Add highway\n",
    "            with tf.name_scope(\"highway\"):\n",
    "                self.h_highway = highway(self.h_pool_flat, self.h_pool_flat.get_shape()[1], 1, 0)\n",
    "\n",
    "            # Add dropout\n",
    "            with tf.name_scope(\"dropout\"):\n",
    "                self.h_drop = tf.nn.dropout(self.h_highway, self.dropout_keep_prob)\n",
    "            \n",
    "            \n",
    "            # Final (unnormalized) scores and predictions\n",
    "            with tf.name_scope(\"output\"):\n",
    "                W = tf.Variable(tf.truncated_normal([num_filters_total, num_classes], stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "                l2_loss += tf.nn.l2_loss(W)\n",
    "                l2_loss += tf.nn.l2_loss(b)\n",
    "                self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "                self.ypred_for_auc = tf.nn.softmax(self.scores)\n",
    "                self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "                \n",
    "            # CalculateMean cross-entropy loss\n",
    "            with tf.name_scope(\"loss\"):\n",
    "                losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
    "                self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss                \n",
    "            print(self.loss)\n",
    "            \n",
    "        self.params = [param for param in tf.trainable_variables() if 'discriminator' in param.name]\n",
    "        d_optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "        grads_and_vars = d_optimizer.compute_gradients(self.loss, self.params, aggregation_method=2)\n",
    "        self.train_op = d_optimizer.apply_gradients(grads_and_vars)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN train (try)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###  Discriminator  Hyper-parameters\n",
    "#########################################################################################\n",
    "dis_embedding_dim = 64\n",
    "dis_filter_sizes = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20]\n",
    "dis_num_filters = [100, 200, 200, 200, 200, 100, 100, 100, 100, 100, 160, 160]\n",
    "dis_dropout_keep_prob = 0.75\n",
    "dis_l2_reg_lambda = 0.2\n",
    "dis_batch_size = 64\n",
    "\n",
    "vocab_size = 436922\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## pre train discriminator 在all train 里包含了。 所以这部分不用跑。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dis_train():\n",
    "    dis_data_loader = Dis_dataloader(BATCH_SIZE)\n",
    "    \n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    \n",
    "    sess = tf.Session(config=config)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    negative_file = 'generated_sample.txt'\n",
    "    positive_file = 'real_sample.txt'\n",
    "    \n",
    "    dis_data_loader.load_train_data(positive_file, negative_file)\n",
    "    \n",
    "    \n",
    "    # Train 10 epoch on the generated data and do this for 50 times\n",
    "    global_step = 0\n",
    "    for current_step in range(3):\n",
    "    \n",
    "        print(\"global_step: \")\n",
    "        print(global_step)\n",
    "        \n",
    "        for _ in range(3):\n",
    "            print(' epoch: ')\n",
    "            print(_)\n",
    "            dis_data_loader.reset_pointer()\n",
    "            for it in range(dis_data_loader.num_batch):\n",
    "                x_batch, y_batch = dis_data_loader.next_batch()\n",
    "                feed = {\n",
    "                    discriminator.input_x: x_batch,\n",
    "                    discriminator.input_y: y_batch,\n",
    "                    discriminator.dropout_keep_prob: dis_dropout_keep_prob\n",
    "                }\n",
    "                _ = sess.run(discriminator.train_op, feed)\n",
    "            global_step = global_step + 1\n",
    "        #print(\"current_step: %d, save model\" %(current_step))\n",
    "        #checkpoint_path = os.path.join('./model/path')\n",
    "        #discriminator.saver.save(sess, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## 尝试用disc model 得到reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare disc_data for discriminator and generator\n",
    "def disc_train_data(sess, gen_model, vocab, source_inputs, source_outputs,\n",
    "                    encoder_inputs, decoder_inputs, target_weights, bucket_id, mc_search=False):\n",
    "    print('disc_train_data method: ')\n",
    "    train_query, train_answer = [], []\n",
    "    query_len = gen_config.buckets[bucket_id][0]\n",
    "    #answer_len = gen_config.buckets[bucket_id][1]\n",
    "    answer_len = 30\n",
    "    #print('try1')\n",
    "    for query, answer in zip(source_inputs, source_outputs):\n",
    "        query = query[:query_len] + [int(PAD_ID)] * (query_len - len(query) if query_len > len(query) else 0)\n",
    "        train_query.append(query)\n",
    "        answer = answer[:-1] # del tag EOS\n",
    "        answer = answer[:answer_len] + [int(PAD_ID)] * (answer_len - len(answer) if answer_len > len(answer) else 0)\n",
    "        train_answer.append(answer)\n",
    "        train_labels = [1 for _ in source_inputs]\n",
    "    #print(train_answer)\n",
    "    \n",
    "    train_answer = []\n",
    "    def decoder(num_roll):\n",
    "        output_list = []\n",
    "        for _ in xrange(num_roll):\n",
    "            print('roll_num')\n",
    "            print(_)\n",
    "            #print(\"try2\")\n",
    "            _, _, output_logits = gen_model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id,\n",
    "                                                 forward_only=True, mc_search=mc_search)\n",
    "            #print(\"try_f\")\n",
    "            seq_tokens = []\n",
    "            resps = []\n",
    "            for seq in output_logits:\n",
    "                row_token = []\n",
    "                for t in seq:\n",
    "                    row_token.append(int(np.argmax(t, axis=0)))\n",
    "                seq_tokens.append(row_token)\n",
    "\n",
    "            seq_tokens_t = []\n",
    "            for col in range(len(seq_tokens[0])):\n",
    "                seq_tokens_t.append([seq_tokens[row][col] for row in range(len(seq_tokens))])\n",
    "\n",
    "            for seq in seq_tokens_t:\n",
    "                if EOS_ID in seq:\n",
    "                    resps.append(seq[:seq.index(EOS_ID)][:gen_config.buckets[bucket_id][1]])\n",
    "                else:\n",
    "                    resps.append(seq[:gen_config.buckets[bucket_id][1]])\n",
    "            #print(resps)\n",
    "            \n",
    "            for i, output in enumerate(resps):\n",
    "                output = output[:answer_len] + [PAD_ID] * (answer_len - len(output) if answer_len > len(output) else 0)\n",
    "                train_query.append(train_query[i])\n",
    "                train_answer.append(output)\n",
    "                train_labels.append(0)\n",
    "                #print(train_answer)\n",
    "        return train_query, train_answer, train_labels\n",
    "    \n",
    "    if mc_search:\n",
    "        #print(\"try3\")\n",
    "        #train_query, train_answer, train_labels = decoder(gen_config.beam_size)\n",
    "        #只decode一个结果，因为seq2seq模型decode出来结果一样。所以在reward函数里再做蒙特卡洛的生成text。\n",
    "        train_query, train_answer, train_labels = decoder(1)\n",
    "        #print(\"try4\")\n",
    "    else:\n",
    "        train_query, train_answer, train_labels = decoder(1)\n",
    "        #print(\"try5\")\n",
    "\n",
    "    return train_query, train_answer, train_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#answer_len = gen_config.buckets[bucket_id][1]\n",
    "#print(answer_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_reward(sess, input_x, rollout_num, bucket_id, discriminator,source_inputs,source_outputs):\n",
    "    rewards = []\n",
    "    rewards = np.array(rewards)\n",
    "    self_batch_size = len(input_x)\n",
    "    self_sequence_length = gen_config.buckets[bucket_id][1]\n",
    "    \n",
    "    self_x = tf.placeholder(tf.int32, shape=[self_batch_size, self_sequence_length])\n",
    "    self_x = input_x\n",
    "\n",
    "    \n",
    "        \n",
    "    #print('miao')\n",
    "    feed = {discriminator.input_x: self_x, discriminator.dropout_keep_prob: 1.0}\n",
    "    ypred_for_auc = sess.run(discriminator.ypred_for_auc, feed)\n",
    "    print('ypred_for_auc')\n",
    "    print(ypred_for_auc)\n",
    "    print('y_pred')\n",
    "    ypred = np.array([item[1] for item in ypred_for_auc])\n",
    "    print(ypred)\n",
    "    reward = ypred[0]\n",
    "    #for i in range(0,ypred):\n",
    "    #    rewards.append(ypred[i])\n",
    "    #reward = ypred[0]\n",
    "    \n",
    "    # the last token reward\n",
    "    #feed = {discriminator.input_x: input_x, discriminator.dropout_keep_prob: 1.0}\n",
    "    #ypred_for_auc = sess.run(discriminator.ypred_for_auc, feed)\n",
    "    #ypred = np.array([item[1] for item in ypred_for_auc])\n",
    "    #print(ypred_for_auc)\n",
    "    #print(ypred)\n",
    "    #if i == 0:\n",
    "    #    rewards.append(ypred)\n",
    "    #else:\n",
    "    #   rewards[19] += ypred\n",
    "    #    \n",
    "    \n",
    "    #print('miaomiao')\n",
    "    #rewards = np.transpose(np.array(rewards)) / (1.0 * rollout_num)  # batch_size x seq_length\n",
    "    print(reward)\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from tensorflow.python.ops import tensor_array_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#reward = get_reward(sess, train_answer, rollout_num, discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  Discriminator  Hyper-parameters\n",
    "#########################################################################################\n",
    "dis_embedding_dim = 64\n",
    "dis_filter_sizes = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20]\n",
    "dis_num_filters = [100, 200, 200, 200, 200, 100, 100, 100, 100, 100, 160, 160]\n",
    "dis_dropout_keep_prob = 0.75\n",
    "dis_l2_reg_lambda = 0.2\n",
    "dis_batch_size = 64\n",
    "\n",
    "vocab_size = 436922\n",
    "BATCH_SIZE = 64\n",
    "vocab_size = 250000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def all_train():\n",
    "    dis_data_loader = Dis_dataloader(BATCH_SIZE)\n",
    "\n",
    "    sess = tf.Session()\n",
    "    \n",
    "    gen_model = create_model(sess, gen_config, forward_only=False, name_scope=gen_config.name_model)\n",
    "    discriminator = Discriminator(sequence_length=30, num_classes=2, vocab_size=vocab_size, embedding_size=dis_embedding_dim, \n",
    "                                filter_sizes=dis_filter_sizes, num_filters=dis_num_filters, l2_reg_lambda=dis_l2_reg_lambda)\n",
    "\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    ######################################################################################################\n",
    "    print('Start pre-training...')\n",
    "    print('pre-training Discriminator')\n",
    "    \n",
    "    negative_file = 'generated_sample.txt'\n",
    "    positive_file = 'real_sample.txt'\n",
    "    dis_data_loader.load_train_data(positive_file, negative_file)\n",
    "    \n",
    "    \n",
    "    # Train 10 epoch on the generated data and do this for 50 times\n",
    "    global_step = 0\n",
    "    for current_step in range(10):\n",
    "    \n",
    "        print(\"global_step: \")\n",
    "        print(global_step)\n",
    "        \n",
    "        for _ in range(5):\n",
    "            print(' epoch: ')\n",
    "            print(_)\n",
    "            dis_data_loader.reset_pointer()\n",
    "            for it in range(dis_data_loader.num_batch):\n",
    "                x_batch, y_batch = dis_data_loader.next_batch()\n",
    "                feed = {\n",
    "                    discriminator.input_x: x_batch,\n",
    "                    discriminator.input_y: y_batch,\n",
    "                    discriminator.dropout_keep_prob: dis_dropout_keep_prob\n",
    "                }\n",
    "                _ = sess.run(discriminator.train_op, feed)\n",
    "            global_step = global_step + 1\n",
    "    ######################################################################################\n",
    "    vocab, rev_vocab, dev_set, train_set = prepare_data(gen_config)\n",
    "    \n",
    "    for set in train_set:\n",
    "        print(\"all train len: \", len(set))\n",
    "\n",
    "    train_bucket_sizes = [len(train_set[b]) for b in xrange(len(gen_config.buckets))]\n",
    "    train_total_size = float(sum(train_bucket_sizes))\n",
    "    train_buckets_scale = [sum(train_bucket_sizes[:i + 1]) / train_total_size\n",
    "                               for i in xrange(len(train_bucket_sizes))]\n",
    "        \n",
    "    ######################################################################################\n",
    "    current_step = 0\n",
    "    step_time, disc_loss, gen_loss, t_loss, batch_reward = 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "    gen_loss_summary = tf.Summary()\n",
    "    gen_writer = tf.summary.FileWriter(gen_config.tensorboard_dir, sess.graph)\n",
    "\n",
    "    while True:\n",
    "        current_step += 1\n",
    "        start_time = time.time()\n",
    "        random_number_01 = np.random.random_sample()\n",
    "        bucket_id = min([i for i in xrange(len(train_buckets_scale))\n",
    "                        if train_buckets_scale[i] > random_number_01])\n",
    "        \n",
    "        print(\"==================Update Generator: %d=========================\" % current_step)\n",
    "        # 1.Sample (X,Y) from real disc_data\n",
    "        update_gen_data = gen_model.get_batch(train_set, bucket_id)\n",
    "        encoder, decoder, weights, source_inputs, source_outputs = update_gen_data \n",
    "        # 2.Sample (X,Y) and (X, ^Y) through ^Y ~ G(*|X) with Monte Carlo search\n",
    "        train_query, train_answer, train_labels = disc_train_data(sess, gen_model, vocab, source_inputs, source_outputs,\n",
    "                                                                encoder, decoder, weights, bucket_id, mc_search=True)\n",
    "        print(\"=============================mc_search: True====================================\")   \n",
    "        # 3.Compute Reward r for (X, ^Y ) using D.---based on Monte Carlo search\n",
    "        #***********************************************************************#\n",
    "        rollout_num = gen_config.beam_size\n",
    "        print(\"train_answer\")\n",
    "        print(train_answer)\n",
    "        print(len(train_answer))\n",
    "        for i in range(0,len(train_answer)):\n",
    "            train_answer_w = train_answer[i]\n",
    "            train_g= \" \".join([tf.compat.as_str(rev_vocab[train_answer_w_w]) for train_answer_w_w in train_answer_w])\n",
    "            print(train_g)\n",
    "        reward = get_reward(sess, train_answer, rollout_num, bucket_id, discriminator,source_inputs,source_outputs)\n",
    "        print(reward)\n",
    "        batch_reward += reward / gen_config.steps_per_checkpoint\n",
    "        print(batch_reward)\n",
    "        # 为什么要除以steps_per_checkpoint??\n",
    "        print(\"step_reward: \", reward)\n",
    "            \n",
    "        #*************************************************************************#\n",
    "        # 4.Update G on (X, ^Y ) using reward r\n",
    "        gan_adjusted_loss, gen_step_loss, _ =gen_model.step(sess, encoder, decoder, weights, bucket_id, forward_only=True,\n",
    "                                           reward=reward, up_reward=True, debug=True)\n",
    "        gen_loss += gen_step_loss / gen_config.steps_per_checkpoint\n",
    "        #为什么要除以steps_per_checkpoint??\n",
    "\n",
    "        print(\"gen_step_loss: \", gen_step_loss)\n",
    "        print(\"gen_step_adjusted_loss: \", gan_adjusted_loss)\n",
    "\n",
    "        # 5.Teacher-Forcing: Update G on (X, Y )\n",
    "        t_adjusted_loss, t_step_loss, a = gen_model.step(sess, encoder, decoder, weights, bucket_id, forward_only=False)\n",
    "        t_loss += t_step_loss / gen_config.steps_per_checkpoint\n",
    "           \n",
    "        print(\"t_step_loss: \", t_step_loss)\n",
    "        print(\"t_adjusted_loss\", t_adjusted_loss)           \n",
    "\n",
    "        if current_step % gen_config.steps_per_checkpoint == 0:\n",
    "\n",
    "            step_time += (time.time() - start_time) / gen_config.steps_per_checkpoint\n",
    "\n",
    "            print(\"current_steps: %d, step time: %.4f, disc_loss: %.3f, gen_loss: %.3f, t_loss: %.3f, reward: %.3f\"\n",
    "                      %(current_step, step_time, disc_loss, gen_loss, t_loss, batch_reward))\n",
    "\n",
    "            gen_global_steps = sess.run(gen_model.global_step)\n",
    "            gen_loss_value = gen_loss_summary.value.add()\n",
    "            gen_loss_value.tag = gen_config.name_loss\n",
    "            gen_loss_value.simple_value = float(gen_loss)\n",
    "            t_loss_value = gen_loss_summary.value.add()\n",
    "            t_loss_value.tag = gen_config.teacher_loss\n",
    "            t_loss_value.simple_value = float(t_loss)\n",
    "            batch_reward_value = gen_loss_summary.value.add()\n",
    "            batch_reward_value.tag = gen_config.reward_name\n",
    "            batch_reward_value.simple_value = float(batch_reward)\n",
    "            gen_writer.add_summary(gen_loss_summary, int(gen_global_steps))\n",
    "\n",
    "            if current_step % (gen_config.steps_per_checkpoint * 2) == 0:\n",
    "                print(\"current_steps: %d, save gen model\" % current_step)\n",
    "                gen_ckpt_dir = os.path.abspath(os.path.join(gen_config.train_dir, \"checkpoints\"))\n",
    "                if not os.path.exists(gen_ckpt_dir):\n",
    "                        os.makedirs(gen_ckpt_dir)\n",
    "                gen_model_path = os.path.join(gen_ckpt_dir, \"gen.model\")\n",
    "                gen_model.saver.save(sess, gen_model_path, global_step=gen_model.global_step)\n",
    "\n",
    "            step_time, disc_loss, gen_loss, t_loss, batch_reward = 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "            sys.stdout.flush()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "single_cell\n",
      "Reading Gen model parameters from /Users/zhangyiman/final project/ Seq2Seq_GAN_Try_Again/gen_data_p/checkpoints/chitchat.model-31800\n",
      "Tensor(\"discriminator/loss/add:0\", shape=(), dtype=float32)\n",
      "Start pre-training...\n",
      "pre-training Discriminator\n",
      "global_step: \n",
      "0\n",
      " epoch: \n",
      "0\n",
      " epoch: \n",
      "1\n",
      " epoch: \n",
      "2\n",
      " epoch: \n",
      "3\n",
      " epoch: \n",
      "4\n",
      "global_step: \n",
      "5\n",
      " epoch: \n",
      "0\n",
      " epoch: \n",
      "1\n",
      " epoch: \n",
      "2\n",
      " epoch: \n",
      "3\n",
      " epoch: \n",
      "4\n",
      "global_step: \n",
      "10\n",
      " epoch: \n",
      "0\n",
      " epoch: \n",
      "1\n",
      " epoch: \n",
      "2\n",
      " epoch: \n",
      "3\n",
      " epoch: \n",
      "4\n",
      "global_step: \n",
      "15\n",
      " epoch: \n",
      "0\n",
      " epoch: \n",
      "1\n",
      " epoch: \n",
      "2\n",
      " epoch: \n",
      "3\n",
      " epoch: \n",
      "4\n",
      "global_step: \n",
      "20\n",
      " epoch: \n",
      "0\n",
      " epoch: \n",
      "1\n",
      " epoch: \n",
      "2\n",
      " epoch: \n",
      "3\n",
      " epoch: \n",
      "4\n",
      "global_step: \n",
      "25\n",
      " epoch: \n",
      "0\n",
      " epoch: \n",
      "1\n",
      " epoch: \n",
      "2\n",
      " epoch: \n",
      "3\n",
      " epoch: \n",
      "4\n",
      "global_step: \n",
      "30\n",
      " epoch: \n",
      "0\n",
      " epoch: \n",
      "1\n",
      " epoch: \n",
      "2\n",
      " epoch: \n",
      "3\n",
      " epoch: \n",
      "4\n",
      "global_step: \n",
      "35\n",
      " epoch: \n",
      "0\n",
      " epoch: \n",
      "1\n",
      " epoch: \n",
      "2\n",
      " epoch: \n",
      "3\n",
      " epoch: \n",
      "4\n",
      "global_step: \n",
      "40\n",
      " epoch: \n",
      "0\n",
      " epoch: \n",
      "1\n",
      " epoch: \n",
      "2\n",
      " epoch: \n",
      "3\n",
      " epoch: \n",
      "4\n",
      "global_step: \n",
      "45\n",
      " epoch: \n",
      "0\n",
      " epoch: \n",
      "1\n",
      " epoch: \n",
      "2\n",
      " epoch: \n",
      "3\n",
      " epoch: \n",
      "4\n",
      "  reading train data line 0\n",
      "  reading train data line 200000\n",
      "  reading train data line 400000\n",
      "  reading train data line 600000\n",
      "  reading train data line 800000\n",
      "all train len:  238720\n",
      "all train len:  168458\n",
      "all train len:  184783\n",
      "all train len:  120318\n",
      "==================Update Generator: 1=========================\n",
      "disc_train_data method: \n",
      "roll_num\n",
      "0\n",
      "=============================mc_search: True====================================\n",
      "train_answer\n",
      "[[224126, 209011, 110611, 110611, 148426, 148426, 148426, 148426, 148426, 148426, 148426, 148426, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [50011, 103713, 103713, 103713, 103713, 103713, 103713, 22782, 22782, 22782, 22782, 22782, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [149058, 149058, 218915, 218915, 218915, 218915, 14528, 14528, 14528, 149058, 149058, 149058, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [113097, 92720, 227130, 43883, 43883, 43883, 43883, 43883, 223286, 223286, 223286, 223286, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [77776, 46849, 46849, 46849, 46849, 46849, 177679, 177679, 177679, 177679, 177679, 177679, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [185495, 237035, 26726, 26726, 26726, 26726, 30800, 30800, 30800, 30800, 30800, 30800, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [161327, 161327, 161327, 47924, 47924, 47924, 47924, 172762, 172762, 172762, 172762, 172762, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [51498, 51498, 51498, 159737, 159737, 102190, 102190, 102190, 102190, 65188, 65188, 65188, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [84468, 41244, 50191, 39532, 39532, 39532, 39532, 39532, 39532, 134330, 134330, 73413, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [185710, 39532, 39532, 39532, 169404, 169404, 169404, 169404, 169404, 169404, 169404, 227301, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [177574, 97716, 97716, 44101, 44101, 44101, 205481, 205481, 44101, 44101, 212203, 19974, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [70548, 168515, 112598, 240049, 221649, 221649, 221649, 26381, 26381, 26381, 26381, 26381, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [201806, 214704, 214704, 162599, 238308, 238308, 116086, 116086, 116086, 116086, 116086, 116086, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [7412, 224953, 224953, 224953, 224953, 19974, 19974, 19974, 19974, 19974, 19974, 19974, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [169635, 169635, 248574, 248574, 114909, 114909, 114909, 114909, 114909, 114909, 114909, 114909, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [243863, 243863, 243863, 243863, 243863, 9343, 9343, 9343, 9343, 9343, 9343, 9343, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [36397, 36397, 36397, 36397, 36397, 36397, 36397, 36397, 36397, 36397, 36397, 36397, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [102854, 36393, 36393, 36393, 36393, 36393, 36393, 36393, 126546, 126546, 126546, 126546, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [61740, 150686, 150686, 190821, 190821, 223445, 158589, 158589, 158589, 158589, 158589, 158589, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [91205, 91205, 184298, 107310, 107310, 107310, 107310, 107310, 107310, 107310, 107310, 34010, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [28256, 151456, 151456, 151456, 151456, 151456, 204593, 138235, 138235, 138235, 138235, 138235, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [149058, 149058, 88074, 88074, 88074, 88074, 80755, 80755, 80755, 4225, 6551, 6551, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [244891, 224953, 224953, 224953, 224953, 224953, 224953, 224953, 224953, 224953, 224953, 224953, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [33738, 137295, 137295, 137295, 137295, 137295, 137295, 137295, 137295, 137295, 211774, 211774, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [19974, 19974, 19974, 19974, 19974, 19974, 19974, 19974, 19974, 19974, 19974, 19974, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [175725, 237378, 237378, 237378, 188787, 188787, 188787, 102385, 102385, 102385, 102385, 102385, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [90907, 90907, 90907, 13728, 13728, 13728, 13728, 203841, 203841, 203841, 203841, 203841, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [26306, 26306, 24894, 24894, 24894, 24894, 203014, 203014, 165184, 165184, 165184, 165184, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [12806, 12806, 12806, 12806, 12806, 12806, 12806, 12806, 12806, 12806, 12806, 142288, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [205481, 205481, 205481, 205481, 205481, 205481, 205481, 205481, 212203, 212203, 212203, 212203, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [187882, 187882, 127360, 127360, 242310, 72262, 72262, 72262, 72262, 72262, 72262, 177988, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [149933, 149933, 116086, 116086, 116086, 116086, 173251, 213393, 99682, 99682, 99682, 99682, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [245399, 223025, 226434, 226434, 212203, 212203, 212203, 212203, 212203, 212203, 212203, 172651, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [7992, 7992, 206018, 206018, 91716, 91716, 225613, 225613, 225613, 225613, 225613, 225613, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [230885, 72953, 72953, 72953, 50191, 50191, 50191, 50191, 50191, 11271, 11271, 11271, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [209452, 244273, 85814, 85814, 85814, 85814, 85814, 85814, 85814, 85814, 85814, 85814, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [193259, 193259, 193259, 193259, 193259, 193259, 193259, 193259, 193259, 193259, 193259, 112905, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [137105, 137105, 137105, 63455, 63455, 63455, 238505, 95168, 95168, 175725, 175725, 175725, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [105170, 70197, 211965, 172870, 172870, 211415, 211415, 241012, 241012, 241012, 241012, 241012, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [78043, 78043, 78043, 78043, 78043, 78043, 78043, 78043, 78043, 78043, 78043, 78043, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [110687, 32084, 32084, 32084, 32084, 32084, 76139, 44326, 221664, 60709, 60709, 60709, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [33241, 104186, 104186, 133170, 133170, 187384, 187384, 187384, 42814, 42814, 42814, 42814, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [151663, 151663, 126052, 126052, 126052, 126052, 126052, 113335, 113335, 15558, 15558, 15558, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2869, 2869, 2869, 9475, 9475, 9475, 9475, 9475, 9475, 9475, 9475, 8300, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [54563, 62175, 2922, 135948, 135948, 135948, 135948, 135948, 135948, 146602, 146602, 146602, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [149868, 149868, 149868, 149868, 149868, 149868, 149868, 124285, 124285, 124285, 124285, 36655, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [149834, 56880, 56880, 56880, 56880, 56880, 56880, 85460, 85460, 59641, 59641, 59641, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [53104, 53104, 118060, 182890, 182890, 182890, 182890, 182890, 182890, 182890, 182890, 64301, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [50191, 50191, 50191, 50191, 50191, 50191, 50191, 50191, 50191, 90698, 211774, 211774, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [38578, 57158, 57158, 57158, 57158, 57158, 57158, 151727, 151727, 151727, 151727, 151727, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [24937, 41244, 20538, 87489, 237067, 237067, 77072, 77072, 77072, 77072, 77072, 77072, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [134601, 134601, 134601, 205481, 149757, 149757, 149757, 19974, 19974, 19974, 19974, 19974, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [217069, 217069, 224953, 224953, 224953, 224953, 19974, 19974, 19974, 19974, 19974, 19974, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [20958, 20958, 172038, 5017, 5017, 5017, 5017, 5017, 5017, 5017, 5017, 5017, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [205481, 205481, 205481, 205481, 38383, 38383, 38383, 38383, 36675, 36675, 36675, 36675, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [44101, 44101, 222629, 222629, 222629, 222629, 209758, 209758, 209758, 78663, 143128, 143128, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [172651, 172651, 19974, 19974, 19974, 19974, 19974, 19974, 19974, 19974, 19974, 19974, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [22996, 172651, 172651, 172651, 172651, 172651, 172651, 172651, 172651, 172651, 172651, 172651, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [84594, 196219, 196219, 196219, 196219, 196219, 22637, 22637, 223025, 223025, 43863, 212203, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [30194, 134729, 137012, 137012, 137012, 137012, 137012, 137012, 137012, 137012, 137012, 137012, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1303, 1303, 63795, 163614, 163614, 163614, 217978, 217978, 217978, 217978, 217978, 217978, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [67768, 234175, 234175, 234175, 234175, 234175, 234175, 234175, 143872, 144413, 144413, 144413, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [48121, 68107, 68107, 68107, 68107, 68107, 143060, 146440, 146440, 146440, 146440, 146440, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [192948, 90334, 90334, 27168, 90334, 199072, 199072, 199072, 199072, 199072, 199072, 199072, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "64\n",
      "“tank” nextwav wychbold wychbold vrnkhdsznzh vrnkhdsznzh vrnkhdsznzh vrnkhdsznzh vrnkhdsznzh vrnkhdsznzh vrnkhdsznzh vrnkhdsznzh _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "4.08 comaba comaba comaba comaba comaba comaba needs\" needs\" needs\" needs\" needs\" _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "ytjixy+42 ytjixy+42 jeeps! jeeps! jeeps! jeeps! requisit requisit requisit ytjixy+42 ytjixy+42 ytjixy+42 _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "gulag chávez annelis oncogenex oncogenex oncogenex oncogenex oncogenex cortext® cortext® cortext® cortext® _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "doig history! history! history! history! history! dannenbr dannenbr dannenbr dannenbr dannenbr dannenbr _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "denia tokay 1.40 1.40 1.40 1.40 510k 510k 510k 510k 510k 510k _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "[black [black [black safa safa safa safa yiv yiv yiv yiv yiv _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "hina hina hina nyse:ego nyse:ego octal octal octal octal darjeel darjeel darjeel _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "#popeinphilli aidnew hospitals\" violence” violence” violence” violence” violence” violence” dream!: dream!: stoneridg _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "https:site violence” violence” violence” elyria; elyria; elyria; elyria; elyria; elyria; elyria; healthdaysom _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "bvckup £433.2 £433.2 6:05 6:05 6:05 ipsilater ipsilater 6:05 6:05 kkp samir _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "mp: 39772 jolt\" betting chaboy chaboy chaboy blt blt blt blt blt _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "pavol ewwwww! ewwwww! ============================ vilest vilest unlucky\" unlucky\" unlucky\" unlucky\" unlucky\" unlucky\" _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "jackpot lightless lightless lightless lightless samir samir samir samir samir samir samir _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "pemba pemba vcwbwu— vcwbwu— 1433 1433 1433 1433 1433 1433 1433 1433 _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "blanchet blanchet blanchet blanchet blanchet discussion: discussion: discussion: discussion: discussion: discussion: discussion: _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "cabinetri cabinetri cabinetri cabinetri cabinetri cabinetri cabinetri cabinetri cabinetri cabinetri cabinetri cabinetri _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "miracle: executives executives executives executives executives executives executives petition! petition! petition! petition! _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "adults; 9+khamuwfgl 9+khamuwfgl converted converted hollyweird visualazn visualazn visualazn visualazn visualazn visualazn _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "sejic sejic releasesquintessenti mandrel mandrel mandrel mandrel mandrel mandrel mandrel mandrel sensitis _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "nabi gostkowksi gostkowksi gostkowksi gostkowksi gostkowksi nhgri xeqexjbjkrxhjipnxnehjbxoocfpvzxzxwjyoqsrpxc xeqexjbjkrxhjipnxnehjbxoocfpvzxzxwjyoqsrpxc xeqexjbjkrxhjipnxnehjbxoocfpvzxzxwjyoqsrpxc xeqexjbjkrxhjipnxnehjbxoocfpvzxzxwjyoqsrpxc xeqexjbjkrxhjipnxnehjbxoocfpvzxzxwjyoqsrpxc _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "ytjixy+42 ytjixy+42 northway northway northway northway \"360 \"360 \"360 cuban adher adher _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "mcxindia lightless lightless lightless lightless lightless lightless lightless lightless lightless lightless lightless _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "inflect iddiwxas iddiwxas iddiwxas iddiwxas iddiwxas iddiwxas iddiwxas iddiwxas iddiwxas action': action': _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "samir samir samir samir samir samir samir samir samir samir samir samir _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "manship inswing inswing inswing alderney alderney alderney maclaren maclaren maclaren maclaren maclaren _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "18.5% 18.5% 18.5% edf edf edf edf like™\" like™\" like™\" like™\" like™\" _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "claremont claremont aortic aortic aortic aortic \"scam\" \"scam\" 4075 4075 4075 4075 _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "bouchard bouchard bouchard bouchard bouchard bouchard bouchard bouchard bouchard bouchard bouchard frustrating\" _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "ipsilater ipsilater ipsilater ipsilater ipsilater ipsilater ipsilater ipsilater kkp kkp kkp kkp _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "l̹uv l̹uv mentality\" mentality\" 73700 horse: horse: horse: horse: horse: horse: “arguabl _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "\"deloitt \"deloitt unlucky\" unlucky\" unlucky\" unlucky\" 13:16: fischer: 12.82% 12.82% 12.82% 12.82% _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "secretas 300138037 nyse:ee nyse:ee kkp kkp kkp kkp kkp kkp kkp erdington _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "cas cas mitsuru mitsuru #haryana #haryana baweja baweja baweja baweja baweja baweja _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "7800815878 isola isola isola hospitals\" hospitals\" hospitals\" hospitals\" hospitals\" dior dior dior _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "neulion® barahona testament: testament: testament: testament: testament: testament: testament: testament: testament: testament: _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "6302015 6302015 6302015 6302015 6302015 6302015 6302015 6302015 6302015 6302015 6302015 \"thunderball\" _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "yfxirmtut yfxirmtut yfxirmtut loca loca loca asst: “ok” “ok” manship manship manship _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "4.85% unikitti “deliveri lecocqassoci lecocqassoci 8]multiman 8]multiman ngac ngac ngac ngac ngac _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "\"col \"col \"col \"col \"col \"col \"col \"col \"col \"col \"col \"col _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "'i'v unev unev unev unev unev intervention\" inordin ncirc #travellerau #travellerau #travellerau _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "wyld crowdcub crowdcub unsg unsg naar naar naar lista lista lista lista _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "ghmgmza ghmgmza futures futures futures futures futures @deckchairblog @deckchairblog notwithstand notwithstand notwithstand _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "acknowledg acknowledg acknowledg khalifa khalifa khalifa khalifa khalifa khalifa khalifa khalifa 3% _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "hanya endorphin warrior obtmbly obtmbly obtmbly obtmbly obtmbly obtmbly pipefitt pipefitt pipefitt _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "wkzo wkzo wkzo wkzo wkzo wkzo wkzo banih banih banih banih comus _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "ehscsefxuikk huckleberri huckleberri huckleberri huckleberri huckleberri huckleberri pesticides pesticides narciso narciso narciso _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "\"toy \"toy \"spirit\" flexin flexin flexin flexin flexin flexin flexin flexin townsquar _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "hospitals\" hospitals\" hospitals\" hospitals\" hospitals\" hospitals\" hospitals\" hospitals\" hospitals\" \"2016 action': action': _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "killam \"sell \"sell \"sell \"sell \"sell \"sell ygkvjxnai+7 ygkvjxnai+7 ygkvjxnai+7 ygkvjxnai+7 ygkvjxnai+7 _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "tvline aidnew eutelsat bruh bacán bacán 10400 10400 10400 10400 10400 10400 _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "vermiculit vermiculit vermiculit ipsilater [drama [drama [drama samir samir samir samir samir _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "dnlm dnlm lightless lightless lightless lightless samir samir samir samir samir samir _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "corelog corelog amrutha clara clara clara clara clara clara clara clara clara _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "ipsilater ipsilater ipsilater ipsilater inf inf inf inf armament armament armament armament _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "6:05 6:05 nfdc nfdc nfdc nfdc cornflour cornflour cornflour wenner fugett fugett _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "erdington erdington samir samir samir samir samir samir samir samir samir samir _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "dud erdington erdington erdington erdington erdington erdington erdington erdington erdington erdington erdington _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "daymond aganaba aganaba aganaba aganaba aganaba tynesid tynesid 300138037 300138037 weebli kkp _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "boylan http:beta jackson; jackson; jackson; jackson; jackson; jackson; jackson; jackson; jackson; jackson; _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "loan loan ncsa width=100% width=100% width=100% varin varin varin varin varin varin _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "nasdaq:logm kollin kollin kollin kollin kollin kollin kollin dixter btk btk btk _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "campari lukasz lukasz lukasz lukasz lukasz videographer: tgrec tgrec tgrec tgrec tgrec _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "acidcow shopper: shopper: racing: shopper: edgytim edgytim edgytim edgytim edgytim edgytim edgytim _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "ypred_for_auc\n",
      "[[  9.81136382e-01   1.88636146e-02]\n",
      " [  9.97519016e-01   2.48103030e-03]\n",
      " [  9.99857187e-01   1.42739387e-04]\n",
      " [  9.99399066e-01   6.00945204e-04]\n",
      " [  9.99747455e-01   2.52556580e-04]\n",
      " [  9.94738758e-01   5.26129687e-03]\n",
      " [  9.92155135e-01   7.84484018e-03]\n",
      " [  9.99769032e-01   2.30924663e-04]\n",
      " [  9.99736369e-01   2.63596972e-04]\n",
      " [  9.99838948e-01   1.60973927e-04]\n",
      " [  9.99876022e-01   1.24015394e-04]\n",
      " [  9.99447763e-01   5.52189886e-04]\n",
      " [  9.99332845e-01   6.67081680e-04]\n",
      " [  9.99974012e-01   2.60231118e-05]\n",
      " [  9.99613464e-01   3.86601343e-04]\n",
      " [  9.96934891e-01   3.06508597e-03]\n",
      " [  9.98621225e-01   1.37878861e-03]\n",
      " [  9.94230866e-01   5.76916710e-03]\n",
      " [  9.99633670e-01   3.66340450e-04]\n",
      " [  9.99209285e-01   7.90711725e-04]\n",
      " [  9.99779522e-01   2.20516144e-04]\n",
      " [  9.98740375e-01   1.25963299e-03]\n",
      " [  9.94649827e-01   5.35019441e-03]\n",
      " [  9.99795020e-01   2.05039949e-04]\n",
      " [  9.99835253e-01   1.64709651e-04]\n",
      " [  9.99979377e-01   2.06308978e-05]\n",
      " [  9.99767721e-01   2.32258812e-04]\n",
      " [  9.98888195e-01   1.11187308e-03]\n",
      " [  9.99050200e-01   9.49789712e-04]\n",
      " [  9.65127766e-01   3.48722525e-02]\n",
      " [  9.99824464e-01   1.75601686e-04]\n",
      " [  9.99783099e-01   2.16923465e-04]\n",
      " [  9.99769032e-01   2.30980397e-04]\n",
      " [  9.99670506e-01   3.29483300e-04]\n",
      " [  9.99762595e-01   2.37416520e-04]\n",
      " [  9.99386549e-01   6.13444718e-04]\n",
      " [  9.96553540e-01   3.44650028e-03]\n",
      " [  9.94945228e-01   5.05476072e-03]\n",
      " [  9.99677777e-01   3.22200183e-04]\n",
      " [  9.98501539e-01   1.49849232e-03]\n",
      " [  9.99860168e-01   1.39853277e-04]\n",
      " [  9.99544442e-01   4.55479749e-04]\n",
      " [  9.99792159e-01   2.07815305e-04]\n",
      " [  9.99200404e-01   7.99591595e-04]\n",
      " [  9.99982476e-01   1.75506466e-05]\n",
      " [  9.99966025e-01   3.39166436e-05]\n",
      " [  9.99771774e-01   2.28216653e-04]\n",
      " [  9.99515772e-01   4.84226504e-04]\n",
      " [  9.99452293e-01   5.47737407e-04]\n",
      " [  9.98815656e-01   1.18437817e-03]\n",
      " [  9.99766529e-01   2.33438797e-04]\n",
      " [  9.99790251e-01   2.09778664e-04]\n",
      " [  9.99821484e-01   1.78520335e-04]\n",
      " [  9.99271691e-01   7.28329353e-04]\n",
      " [  9.99838352e-01   1.61660748e-04]\n",
      " [  9.94388878e-01   5.61115658e-03]\n",
      " [  9.99968052e-01   3.19254978e-05]\n",
      " [  9.99978662e-01   2.13530366e-05]\n",
      " [  9.99873996e-01   1.26037805e-04]\n",
      " [  9.99022841e-01   9.77142714e-04]\n",
      " [  9.88353431e-01   1.16466042e-02]\n",
      " [  9.83284533e-01   1.67154036e-02]\n",
      " [  9.98851418e-01   1.14860735e-03]\n",
      " [  9.99975324e-01   2.47303597e-05]]\n",
      "y_pred\n",
      "[  1.88636146e-02   2.48103030e-03   1.42739387e-04   6.00945204e-04\n",
      "   2.52556580e-04   5.26129687e-03   7.84484018e-03   2.30924663e-04\n",
      "   2.63596972e-04   1.60973927e-04   1.24015394e-04   5.52189886e-04\n",
      "   6.67081680e-04   2.60231118e-05   3.86601343e-04   3.06508597e-03\n",
      "   1.37878861e-03   5.76916710e-03   3.66340450e-04   7.90711725e-04\n",
      "   2.20516144e-04   1.25963299e-03   5.35019441e-03   2.05039949e-04\n",
      "   1.64709651e-04   2.06308978e-05   2.32258812e-04   1.11187308e-03\n",
      "   9.49789712e-04   3.48722525e-02   1.75601686e-04   2.16923465e-04\n",
      "   2.30980397e-04   3.29483300e-04   2.37416520e-04   6.13444718e-04\n",
      "   3.44650028e-03   5.05476072e-03   3.22200183e-04   1.49849232e-03\n",
      "   1.39853277e-04   4.55479749e-04   2.07815305e-04   7.99591595e-04\n",
      "   1.75506466e-05   3.39166436e-05   2.28216653e-04   4.84226504e-04\n",
      "   5.47737407e-04   1.18437817e-03   2.33438797e-04   2.09778664e-04\n",
      "   1.78520335e-04   7.28329353e-04   1.61660748e-04   5.61115658e-03\n",
      "   3.19254978e-05   2.13530366e-05   1.26037805e-04   9.77142714e-04\n",
      "   1.16466042e-02   1.67154036e-02   1.14860735e-03   2.47303597e-05]\n",
      "0.0188636\n",
      "0.0188636\n",
      "0.000188636146486\n",
      "step_reward:  0.0188636\n",
      "gen_step_loss:  10.3387\n",
      "gen_step_adjusted_loss:  None\n",
      "t_step_loss:  10.3623\n",
      "t_adjusted_loss 0.16205\n",
      "==================Update Generator: 2=========================\n",
      "disc_train_data method: \n",
      "roll_num\n",
      "0\n",
      "=============================mc_search: True====================================\n",
      "train_answer\n",
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8591, 11328, 11328, 11328, 11328, 107281, 107281, 107281, 107281, 149490, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [202207, 202207, 158991, 158991, 158991, 158991, 154332, 138968, 138968, 138968, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [148, 149490, 149490, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [122966, 122966, 122966, 122966, 14515, 14515, 17104, 17104, 17104, 17104, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [44326, 44326, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [180269, 180269, 141918, 141918, 141918, 141918, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [149490, 149490, 240049, 44326, 44326, 44326, 44326, 44326, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [154944, 96110, 96110, 96110, 96110, 122579, 122579, 122579, 122579, 59074, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [247274, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [44326, 44326, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [64212, 17578, 17578, 57978, 57978, 144055, 144055, 158037, 158037, 158037, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [67917, 71205, 71205, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [64915, 71810, 77043, 77043, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [240049, 240049, 240049, 240049, 240049, 240049, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [215782, 215782, 215782, 215782, 167339, 167339, 167339, 167339, 167339, 212181, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [5789, 216844, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [16125, 229075, 214007, 214007, 60160, 60160, 60160, 60160, 1809, 1809, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [163550, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [5789, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [39813, 39813, 39813, 39813, 83702, 83702, 83702, 83702, 83702, 83702, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [148, 142440, 142440, 142440, 142440, 142440, 142440, 142440, 142440, 142440, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [23897, 23897, 86000, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [56519, 185039, 185039, 185039, 185039, 77806, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [240049, 240049, 240049, 240049, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [35088, 35088, 125094, 125094, 107544, 92882, 92882, 32654, 32654, 32654, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [77197, 199382, 185039, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [163163, 240049, 240049, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [214047, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [49159, 187784, 187784, 187784, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [32389, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "64\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "advert scarf scarf scarf scarf monnat monnat monnat monnat “ohio _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "unemployment: unemployment: aleuy aleuy aleuy aleuy unhewn happybirthdaytothemostspecialwomanintheworld happybirthdaytothemostspecialwomanintheworld happybirthdaytothemostspecialwomanintheworld _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "here “ohio “ohio _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "pinkheartsandsparkledream pinkheartsandsparkledream pinkheartsandsparkledream pinkheartsandsparkledream menopaus menopaus coloss coloss coloss coloss _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "          _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "          _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "          _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "inordin inordin _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "14.57 14.57 ajztn ajztn ajztn ajztn _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "“ohio “ohio betting inordin inordin inordin inordin inordin _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "zpla jordain jordain jordain jordain nihilist nihilist nihilist nihilist praesidian _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "@varun_dvn _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "inordin inordin _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "rens ase ase berrio berrio mvsnzjvpnypv mvsnzjvpnypv 03:19 03:19 03:19 _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "ligat set\" set\" _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "toda amazonaw “convict “convict _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "betting betting betting betting betting betting _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "pumpmystomach pumpmystomach pumpmystomach pumpmystomach dmytro dmytro dmytro dmytro dmytro linktech _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "updated: id=\"attachment_120414\" _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "fledg lynor vision™ vision™ ntm ntm ntm ntm rain rain _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "mintkidsdentistri _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "updated: _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "search: search: search: search: hollowbodi hollowbodi hollowbodi hollowbodi hollowbodi hollowbodi _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "here emsizlblyl+ emsizlblyl+ emsizlblyl+ emsizlblyl+ emsizlblyl+ emsizlblyl+ emsizlblyl+ emsizlblyl+ emsizlblyl+ _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "mati mati \"swiss _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "brutalist bucuresti bucuresti bucuresti bucuresti domenici _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "betting betting betting betting _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "          _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "friendli friendli tinambac tinambac outrageous\" subscib subscib line! line! line! _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "foua mcharg bucuresti _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "releaseskrog betting betting _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "crestani _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "          _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "          _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "heckler deif deif deif       _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "\"sec\" _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "ypred_for_auc\n",
      "[[  9.88432586e-01   1.15674473e-02]\n",
      " [  9.99812782e-01   1.87240468e-04]\n",
      " [  9.99882698e-01   1.17334603e-04]\n",
      " [  9.92766380e-01   7.23362714e-03]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.79699135e-01   2.03008428e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.99911427e-01   8.85559784e-05]\n",
      " [  9.99911427e-01   8.85559784e-05]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.99911427e-01   8.85559784e-05]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.96052742e-01   3.94728919e-03]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.99391079e-01   6.08880015e-04]\n",
      " [  9.95571434e-01   4.42863675e-03]\n",
      " [  9.99878883e-01   1.21139463e-04]\n",
      " [  9.97354388e-01   2.64566112e-03]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.96052742e-01   3.94728919e-03]\n",
      " [  9.99979496e-01   2.05276283e-05]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.95869219e-01   4.13079700e-03]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.99231696e-01   7.68324069e-04]\n",
      " [  9.99626756e-01   3.73200077e-04]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.99689102e-01   3.10898788e-04]\n",
      " [  9.97703254e-01   2.29675137e-03]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.99738872e-01   2.61118112e-04]\n",
      " [  9.92102325e-01   7.89773744e-03]\n",
      " [  9.92258370e-01   7.74156814e-03]\n",
      " [  9.99868274e-01   1.31749446e-04]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.87335205e-01   1.26648005e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.97301519e-01   2.69854581e-03]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.95533466e-01   4.46657743e-03]\n",
      " [  9.99380946e-01   6.19056693e-04]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.99381065e-01   6.18884689e-04]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.99911427e-01   8.85559784e-05]\n",
      " [  9.94218230e-01   5.78174554e-03]\n",
      " [  9.91413713e-01   8.58631730e-03]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.99662399e-01   3.37631296e-04]\n",
      " [  9.83042240e-01   1.69577282e-02]\n",
      " [  9.99911427e-01   8.85559784e-05]\n",
      " [  9.99911427e-01   8.85559784e-05]\n",
      " [  9.99920249e-01   7.97930625e-05]\n",
      " [  9.95810866e-01   4.18913271e-03]]\n",
      "y_pred\n",
      "[  1.15674473e-02   1.87240468e-04   1.17334603e-04   7.23362714e-03\n",
      "   1.15674473e-02   2.03008428e-02   1.15674473e-02   8.85559784e-05\n",
      "   8.85559784e-05   1.15674473e-02   1.15674473e-02   8.85559784e-05\n",
      "   1.15674473e-02   3.94728919e-03   1.15674473e-02   1.15674473e-02\n",
      "   1.15674473e-02   6.08880015e-04   4.42863675e-03   1.21139463e-04\n",
      "   2.64566112e-03   1.15674473e-02   3.94728919e-03   2.05276283e-05\n",
      "   1.15674473e-02   4.13079700e-03   1.15674473e-02   1.15674473e-02\n",
      "   7.68324069e-04   3.73200077e-04   1.15674473e-02   3.10898788e-04\n",
      "   2.29675137e-03   1.15674473e-02   2.61118112e-04   7.89773744e-03\n",
      "   7.74156814e-03   1.31749446e-04   1.15674473e-02   1.26648005e-02\n",
      "   1.15674473e-02   1.15674473e-02   1.15674473e-02   2.69854581e-03\n",
      "   1.15674473e-02   1.15674473e-02   1.15674473e-02   4.46657743e-03\n",
      "   6.19056693e-04   1.15674473e-02   6.18884689e-04   1.15674473e-02\n",
      "   8.85559784e-05   5.78174554e-03   8.58631730e-03   1.15674473e-02\n",
      "   1.15674473e-02   1.15674473e-02   3.37631296e-04   1.69577282e-02\n",
      "   8.85559784e-05   8.85559784e-05   7.97930625e-05   4.18913271e-03]\n",
      "0.0115674\n",
      "0.0115674\n",
      "0.000304310619831\n",
      "step_reward:  0.0115674\n",
      "gen_step_loss:  10.4349\n",
      "gen_step_adjusted_loss:  None\n",
      "t_step_loss:  10.4635\n",
      "t_adjusted_loss 0.672964\n",
      "==================Update Generator: 3=========================\n",
      "disc_train_data method: \n",
      "roll_num\n",
      "0\n",
      "=============================mc_search: True====================================\n",
      "train_answer\n",
      "[[8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "64\n",
      "  _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "     _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "  _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "          _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "   _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "  _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "   _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "   _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "ypred_for_auc\n",
      "[[  9.98849034e-01   1.15090492e-03]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.99795377e-01   2.04646974e-04]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.98849034e-01   1.15090492e-03]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.99911427e-01   8.85559784e-05]\n",
      " [  9.99467790e-01   5.32214588e-04]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.98849034e-01   1.15090492e-03]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.97301519e-01   2.69854581e-03]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.99467790e-01   5.32214588e-04]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.99467790e-01   5.32214588e-04]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.97301519e-01   2.69854581e-03]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]]\n",
      "y_pred\n",
      "[  1.15090492e-03   1.15674473e-02   2.04646974e-04   1.15674473e-02\n",
      "   1.15090492e-03   1.15674473e-02   1.15674473e-02   1.15674473e-02\n",
      "   1.15674473e-02   1.15674473e-02   1.15674473e-02   1.15674473e-02\n",
      "   1.15674473e-02   1.15674473e-02   1.15674473e-02   1.15674473e-02\n",
      "   1.15674473e-02   1.15674473e-02   1.15674473e-02   1.15674473e-02\n",
      "   1.15674473e-02   1.15674473e-02   1.15674473e-02   1.15674473e-02\n",
      "   1.15674473e-02   1.15674473e-02   8.85559784e-05   5.32214588e-04\n",
      "   1.15674473e-02   1.15674473e-02   1.15090492e-03   1.15674473e-02\n",
      "   1.15674473e-02   1.15674473e-02   1.15674473e-02   1.15674473e-02\n",
      "   1.15674473e-02   1.15674473e-02   1.15674473e-02   1.15674473e-02\n",
      "   1.15674473e-02   2.69854581e-03   1.15674473e-02   1.15674473e-02\n",
      "   1.15674473e-02   1.15674473e-02   1.15674473e-02   1.15674473e-02\n",
      "   5.32214588e-04   1.15674473e-02   1.15674473e-02   5.32214588e-04\n",
      "   1.15674473e-02   1.15674473e-02   1.15674473e-02   1.15674473e-02\n",
      "   1.15674473e-02   1.15674473e-02   1.15674473e-02   1.15674473e-02\n",
      "   1.15674473e-02   2.69854581e-03   1.15674473e-02   1.15674473e-02]\n",
      "0.0011509\n",
      "0.0011509\n",
      "0.000315819669049\n",
      "step_reward:  0.0011509\n",
      "gen_step_loss:  10.4249\n",
      "gen_step_adjusted_loss:  None\n",
      "t_step_loss:  10.4242\n",
      "t_adjusted_loss 1.56754\n",
      "==================Update Generator: 4=========================\n",
      "disc_train_data method: \n",
      "roll_num\n",
      "0\n",
      "=============================mc_search: True====================================\n",
      "train_answer\n",
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "64\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "  _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "   _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "  _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "  _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "  _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "  _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "ypred_for_auc\n",
      "[[  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.98849034e-01   1.15090492e-03]\n",
      " [  9.97301519e-01   2.69854581e-03]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.97301519e-01   2.69854581e-03]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.97301519e-01   2.69854581e-03]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.97301519e-01   2.69854581e-03]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.97301519e-01   2.69854581e-03]\n",
      " [  9.97301519e-01   2.69854581e-03]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.97301519e-01   2.69854581e-03]\n",
      " [  9.97301519e-01   2.69854581e-03]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.99467790e-01   5.32214588e-04]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.97301519e-01   2.69854581e-03]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.97301519e-01   2.69854581e-03]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.98849034e-01   1.15090492e-03]\n",
      " [  9.98849034e-01   1.15090492e-03]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.98849034e-01   1.15090492e-03]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.98849034e-01   1.15090492e-03]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.97301519e-01   2.69854581e-03]\n",
      " [  9.88432586e-01   1.15674473e-02]]\n",
      "y_pred\n",
      "[ 0.01156745  0.01156745  0.01156745  0.01156745  0.01156745  0.01156745\n",
      "  0.01156745  0.0011509   0.00269855  0.01156745  0.01156745  0.01156745\n",
      "  0.01156745  0.01156745  0.01156745  0.00269855  0.01156745  0.01156745\n",
      "  0.01156745  0.00269855  0.01156745  0.01156745  0.01156745  0.00269855\n",
      "  0.01156745  0.00269855  0.00269855  0.01156745  0.01156745  0.01156745\n",
      "  0.01156745  0.01156745  0.01156745  0.01156745  0.00269855  0.00269855\n",
      "  0.01156745  0.00053221  0.01156745  0.01156745  0.00269855  0.01156745\n",
      "  0.01156745  0.01156745  0.01156745  0.01156745  0.00269855  0.01156745\n",
      "  0.01156745  0.01156745  0.01156745  0.01156745  0.01156745  0.0011509\n",
      "  0.0011509   0.01156745  0.0011509   0.01156745  0.0011509   0.01156745\n",
      "  0.01156745  0.01156745  0.00269855  0.01156745]\n",
      "0.0115674\n",
      "0.0115674\n",
      "0.000431494142395\n",
      "step_reward:  0.0115674\n",
      "gen_step_loss:  10.1818\n",
      "gen_step_adjusted_loss:  None\n",
      "t_step_loss:  10.2291\n",
      "t_adjusted_loss 2.147\n",
      "==================Update Generator: 5=========================\n",
      "disc_train_data method: \n",
      "roll_num\n",
      "0\n",
      "=============================mc_search: True====================================\n",
      "train_answer\n",
      "[[8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "64\n",
      "  _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "  _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "  _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "   _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "  _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "  _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "   _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "  _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "  _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "  _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "   _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "  _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "  _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "  _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "   _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "  _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "   _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "  _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "  _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "  _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "   _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "   _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "   _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "   _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "  _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "   _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "   _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "  _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "  _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "  _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "  _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "  _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "  _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "   _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "   _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "   _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "  _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "  _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "  _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "  _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "  _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "  _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "   _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "   _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "   _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "  _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "  _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "   _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "  _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "  _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "   _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "  _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "  _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "  _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "  _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "  _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "  _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "   _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "   _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "  _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "ypred_for_auc\n",
      "[[  9.98849034e-01   1.15090492e-03]\n",
      " [  9.98849034e-01   1.15090492e-03]\n",
      " [  9.98849034e-01   1.15090492e-03]\n",
      " [  9.99467790e-01   5.32214588e-04]\n",
      " [  9.98849034e-01   1.15090492e-03]\n",
      " [  9.98849034e-01   1.15090492e-03]\n",
      " [  9.99467790e-01   5.32214588e-04]\n",
      " [  9.98849034e-01   1.15090492e-03]\n",
      " [  9.98849034e-01   1.15090492e-03]\n",
      " [  9.98849034e-01   1.15090492e-03]\n",
      " [  9.99467790e-01   5.32214588e-04]\n",
      " [  9.98849034e-01   1.15090492e-03]\n",
      " [  9.98849034e-01   1.15090492e-03]\n",
      " [  9.98849034e-01   1.15090492e-03]\n",
      " [  9.99467790e-01   5.32214588e-04]\n",
      " [  9.98849034e-01   1.15090492e-03]\n",
      " [  9.99467790e-01   5.32214588e-04]\n",
      " [  9.98849034e-01   1.15090492e-03]\n",
      " [  9.98849034e-01   1.15090492e-03]\n",
      " [  9.98849034e-01   1.15090492e-03]\n",
      " [  9.99467790e-01   5.32214588e-04]\n",
      " [  9.97301519e-01   2.69854581e-03]\n",
      " [  9.99467790e-01   5.32214588e-04]\n",
      " [  9.99467790e-01   5.32214588e-04]\n",
      " [  9.99467790e-01   5.32214588e-04]\n",
      " [  9.98849034e-01   1.15090492e-03]\n",
      " [  9.99467790e-01   5.32214588e-04]\n",
      " [  9.99467790e-01   5.32214588e-04]\n",
      " [  9.98849034e-01   1.15090492e-03]\n",
      " [  9.98849034e-01   1.15090492e-03]\n",
      " [  9.98849034e-01   1.15090492e-03]\n",
      " [  9.98849034e-01   1.15090492e-03]\n",
      " [  9.98849034e-01   1.15090492e-03]\n",
      " [  9.98849034e-01   1.15090492e-03]\n",
      " [  9.99467790e-01   5.32214588e-04]\n",
      " [  9.99467790e-01   5.32214588e-04]\n",
      " [  9.99467790e-01   5.32214588e-04]\n",
      " [  9.98849034e-01   1.15090492e-03]\n",
      " [  9.98849034e-01   1.15090492e-03]\n",
      " [  9.98849034e-01   1.15090492e-03]\n",
      " [  9.98849034e-01   1.15090492e-03]\n",
      " [  9.98849034e-01   1.15090492e-03]\n",
      " [  9.98849034e-01   1.15090492e-03]\n",
      " [  9.99467790e-01   5.32214588e-04]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.99467790e-01   5.32214588e-04]\n",
      " [  9.99467790e-01   5.32214588e-04]\n",
      " [  9.98849034e-01   1.15090492e-03]\n",
      " [  9.98849034e-01   1.15090492e-03]\n",
      " [  9.99467790e-01   5.32214588e-04]\n",
      " [  9.98849034e-01   1.15090492e-03]\n",
      " [  9.98849034e-01   1.15090492e-03]\n",
      " [  9.99467790e-01   5.32214588e-04]\n",
      " [  9.98849034e-01   1.15090492e-03]\n",
      " [  9.98849034e-01   1.15090492e-03]\n",
      " [  9.98849034e-01   1.15090492e-03]\n",
      " [  9.98849034e-01   1.15090492e-03]\n",
      " [  9.98849034e-01   1.15090492e-03]\n",
      " [  9.98849034e-01   1.15090492e-03]\n",
      " [  9.99467790e-01   5.32214588e-04]\n",
      " [  9.99467790e-01   5.32214588e-04]\n",
      " [  9.88432586e-01   1.15674473e-02]\n",
      " [  9.97301519e-01   2.69854581e-03]\n",
      " [  9.98849034e-01   1.15090492e-03]]\n",
      "y_pred\n",
      "[ 0.0011509   0.0011509   0.0011509   0.00053221  0.0011509   0.0011509\n",
      "  0.00053221  0.0011509   0.0011509   0.0011509   0.00053221  0.0011509\n",
      "  0.0011509   0.0011509   0.00053221  0.0011509   0.00053221  0.0011509\n",
      "  0.0011509   0.0011509   0.00053221  0.00269855  0.00053221  0.00053221\n",
      "  0.00053221  0.0011509   0.00053221  0.00053221  0.0011509   0.0011509\n",
      "  0.0011509   0.0011509   0.0011509   0.0011509   0.00053221  0.00053221\n",
      "  0.00053221  0.0011509   0.0011509   0.0011509   0.0011509   0.0011509\n",
      "  0.0011509   0.00053221  0.01156745  0.00053221  0.00053221  0.0011509\n",
      "  0.0011509   0.00053221  0.0011509   0.0011509   0.00053221  0.0011509\n",
      "  0.0011509   0.0011509   0.0011509   0.0011509   0.0011509   0.00053221\n",
      "  0.00053221  0.01156745  0.00269855  0.0011509 ]\n",
      "0.0011509\n",
      "0.0011509\n",
      "0.000443003191613\n",
      "step_reward:  0.0011509\n",
      "gen_step_loss:  10.005\n",
      "gen_step_adjusted_loss:  None\n",
      "t_step_loss:  9.94965\n",
      "t_adjusted_loss 5.62611\n",
      "==================Update Generator: 6=========================\n",
      "disc_train_data method: \n",
      "roll_num\n",
      "0\n",
      "=============================mc_search: True====================================\n",
      "train_answer\n",
      "[[8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "64\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      " _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "ypred_for_auc\n",
      "[[ 0.99730152  0.00269855]\n",
      " [ 0.99730152  0.00269855]\n",
      " [ 0.99730152  0.00269855]\n",
      " [ 0.99730152  0.00269855]\n",
      " [ 0.99730152  0.00269855]\n",
      " [ 0.99730152  0.00269855]\n",
      " [ 0.99730152  0.00269855]\n",
      " [ 0.99730152  0.00269855]\n",
      " [ 0.99730152  0.00269855]\n",
      " [ 0.99730152  0.00269855]\n",
      " [ 0.99730152  0.00269855]\n",
      " [ 0.99730152  0.00269855]\n",
      " [ 0.99730152  0.00269855]\n",
      " [ 0.99730152  0.00269855]\n",
      " [ 0.99730152  0.00269855]\n",
      " [ 0.99730152  0.00269855]\n",
      " [ 0.99730152  0.00269855]\n",
      " [ 0.99730152  0.00269855]\n",
      " [ 0.99730152  0.00269855]\n",
      " [ 0.99730152  0.00269855]\n",
      " [ 0.99730152  0.00269855]\n",
      " [ 0.99730152  0.00269855]\n",
      " [ 0.99730152  0.00269855]\n",
      " [ 0.99730152  0.00269855]\n",
      " [ 0.99730152  0.00269855]\n",
      " [ 0.99730152  0.00269855]\n",
      " [ 0.99730152  0.00269855]\n",
      " [ 0.99730152  0.00269855]\n",
      " [ 0.99730152  0.00269855]\n",
      " [ 0.99730152  0.00269855]\n",
      " [ 0.99730152  0.00269855]\n",
      " [ 0.99730152  0.00269855]\n",
      " [ 0.99730152  0.00269855]\n",
      " [ 0.99730152  0.00269855]\n",
      " [ 0.99730152  0.00269855]\n",
      " [ 0.99730152  0.00269855]\n",
      " [ 0.99730152  0.00269855]\n",
      " [ 0.99730152  0.00269855]\n",
      " [ 0.99730152  0.00269855]\n",
      " [ 0.99730152  0.00269855]\n",
      " [ 0.99730152  0.00269855]\n",
      " [ 0.99730152  0.00269855]\n",
      " [ 0.99730152  0.00269855]\n",
      " [ 0.99730152  0.00269855]\n",
      " [ 0.99730152  0.00269855]\n",
      " [ 0.99730152  0.00269855]\n",
      " [ 0.99730152  0.00269855]\n",
      " [ 0.99730152  0.00269855]\n",
      " [ 0.99730152  0.00269855]\n",
      " [ 0.99730152  0.00269855]\n",
      " [ 0.99730152  0.00269855]\n",
      " [ 0.99730152  0.00269855]\n",
      " [ 0.99730152  0.00269855]\n",
      " [ 0.99730152  0.00269855]\n",
      " [ 0.99730152  0.00269855]\n",
      " [ 0.99730152  0.00269855]\n",
      " [ 0.99730152  0.00269855]\n",
      " [ 0.99730152  0.00269855]\n",
      " [ 0.99730152  0.00269855]\n",
      " [ 0.99730152  0.00269855]\n",
      " [ 0.99730152  0.00269855]\n",
      " [ 0.99730152  0.00269855]\n",
      " [ 0.99730152  0.00269855]\n",
      " [ 0.99730152  0.00269855]]\n",
      "y_pred\n",
      "[ 0.00269855  0.00269855  0.00269855  0.00269855  0.00269855  0.00269855\n",
      "  0.00269855  0.00269855  0.00269855  0.00269855  0.00269855  0.00269855\n",
      "  0.00269855  0.00269855  0.00269855  0.00269855  0.00269855  0.00269855\n",
      "  0.00269855  0.00269855  0.00269855  0.00269855  0.00269855  0.00269855\n",
      "  0.00269855  0.00269855  0.00269855  0.00269855  0.00269855  0.00269855\n",
      "  0.00269855  0.00269855  0.00269855  0.00269855  0.00269855  0.00269855\n",
      "  0.00269855  0.00269855  0.00269855  0.00269855  0.00269855  0.00269855\n",
      "  0.00269855  0.00269855  0.00269855  0.00269855  0.00269855  0.00269855\n",
      "  0.00269855  0.00269855  0.00269855  0.00269855  0.00269855  0.00269855\n",
      "  0.00269855  0.00269855  0.00269855  0.00269855  0.00269855  0.00269855\n",
      "  0.00269855  0.00269855  0.00269855  0.00269855]\n",
      "0.00269855\n",
      "0.00269855\n",
      "0.000469988649711\n",
      "step_reward:  0.00269855\n",
      "gen_step_loss:  9.54665\n",
      "gen_step_adjusted_loss:  None\n",
      "t_step_loss:  9.60648\n",
      "t_adjusted_loss 6.87781\n",
      "==================Update Generator: 7=========================\n",
      "disc_train_data method: \n",
      "roll_num\n",
      "0\n",
      "=============================mc_search: True====================================\n",
      "train_answer\n",
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "64\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "ypred_for_auc\n",
      "[[ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]]\n",
      "y_pred\n",
      "[ 0.01156745  0.01156745  0.01156745  0.01156745  0.01156745  0.01156745\n",
      "  0.01156745  0.01156745  0.01156745  0.01156745  0.01156745  0.01156745\n",
      "  0.01156745  0.01156745  0.01156745  0.01156745  0.01156745  0.01156745\n",
      "  0.01156745  0.01156745  0.01156745  0.01156745  0.01156745  0.01156745\n",
      "  0.01156745  0.01156745  0.01156745  0.01156745  0.01156745  0.01156745\n",
      "  0.01156745  0.01156745  0.01156745  0.01156745  0.01156745  0.01156745\n",
      "  0.01156745  0.01156745  0.01156745  0.01156745  0.01156745  0.01156745\n",
      "  0.01156745  0.01156745  0.01156745  0.01156745  0.01156745  0.01156745\n",
      "  0.01156745  0.01156745  0.01156745  0.01156745  0.01156745  0.01156745\n",
      "  0.01156745  0.01156745  0.01156745  0.01156745  0.01156745  0.01156745\n",
      "  0.01156745  0.01156745  0.01156745  0.01156745]\n",
      "0.0115674\n",
      "0.0115674\n",
      "0.000585663123056\n",
      "step_reward:  0.0115674\n",
      "gen_step_loss:  9.53863\n",
      "gen_step_adjusted_loss:  None\n",
      "t_step_loss:  9.52411\n",
      "t_adjusted_loss 5.42811\n",
      "==================Update Generator: 8=========================\n",
      "disc_train_data method: \n",
      "roll_num\n",
      "0\n",
      "=============================mc_search: True====================================\n",
      "train_answer\n",
      "[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
      "64\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "_PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD _PAD\n",
      "ypred_for_auc\n",
      "[[ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]\n",
      " [ 0.98843259  0.01156745]]\n",
      "y_pred\n",
      "[ 0.01156745  0.01156745  0.01156745  0.01156745  0.01156745  0.01156745\n",
      "  0.01156745  0.01156745  0.01156745  0.01156745  0.01156745  0.01156745\n",
      "  0.01156745  0.01156745  0.01156745  0.01156745  0.01156745  0.01156745\n",
      "  0.01156745  0.01156745  0.01156745  0.01156745  0.01156745  0.01156745\n",
      "  0.01156745  0.01156745  0.01156745  0.01156745  0.01156745  0.01156745\n",
      "  0.01156745  0.01156745  0.01156745  0.01156745  0.01156745  0.01156745\n",
      "  0.01156745  0.01156745  0.01156745  0.01156745  0.01156745  0.01156745\n",
      "  0.01156745  0.01156745  0.01156745  0.01156745  0.01156745  0.01156745\n",
      "  0.01156745  0.01156745  0.01156745  0.01156745  0.01156745  0.01156745\n",
      "  0.01156745  0.01156745  0.01156745  0.01156745  0.01156745  0.01156745\n",
      "  0.01156745  0.01156745  0.01156745  0.01156745]\n",
      "0.0115674\n",
      "0.0115674\n",
      "0.000701337596402\n",
      "step_reward:  0.0115674\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-e3463ab37f1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mall_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-42-88c43f7e3760>\u001b[0m in \u001b[0;36mall_train\u001b[0;34m()\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;31m# 4.Update G on (X, ^Y ) using reward r\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         gan_adjusted_loss, gen_step_loss, _ =gen_model.step(sess, encoder, decoder, weights, bucket_id, forward_only=True,\n\u001b[0;32m---> 95\u001b[0;31m                                            reward=reward, up_reward=True, debug=True)\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0mgen_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mgen_step_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mgen_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps_per_checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m#为什么要除以steps_per_checkpoint??\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-2e48a7cdc271>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, session, encoder_inputs, decoder_inputs, target_weights, bucket_id, forward_only, reward, mc_search, up_reward, debug)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0moutput_feed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbucket_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_feed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_feed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mforward_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m               \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# Gradient norm, loss, no outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "all_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generated_data_for_D_in_ALLtrain(decode_num_step):\n",
    "    list1,list2,list3 = decode_in_ALLtrain(decode_num_step)\n",
    "    with open('generated_sample.txt','w')as f:\n",
    "        for i in range(0,len(list3)):\n",
    "            f.write(list3[i])\n",
    "            f.write('\\n')\n",
    "    with open('real_sample.txt','w')as f:\n",
    "        for i in range(0,len(list3)):\n",
    "            f.write(list2[i])\n",
    "            f.write('\\n')\n",
    "    with open('document_sample.txt','w')as f:\n",
    "        for i in range(0,len(list1)):\n",
    "            f.write(list1[i])\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adversarial Learning for Neural Dialogue Generation\n",
    "def al_train():\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        vocab, rev_vocab, dev_set, train_set = prepare_data(gen_config)\n",
    "        \n",
    "        for set in train_set:\n",
    "            print(\"al train len: \", len(set))\n",
    "\n",
    "        train_bucket_sizes = [len(train_set[b]) for b in xrange(len(gen_config.buckets))]\n",
    "        train_total_size = float(sum(train_bucket_sizes))\n",
    "        train_buckets_scale = [sum(train_bucket_sizes[:i + 1]) / train_total_size\n",
    "                               for i in xrange(len(train_bucket_sizes))]\n",
    "        \n",
    "        ####################################################################################\n",
    "        # discriminator model\n",
    "        discriminator = Discriminator(sequence_length=30, num_classes=2, vocab_size=vocab_size, embedding_size=dis_embedding_dim, \n",
    "                                filter_sizes=dis_filter_sizes, num_filters=dis_num_filters, l2_reg_lambda=dis_l2_reg_lambda)\n",
    "        # generator model\n",
    "        gen_model = create_model(sess, gen_config, forward_only=False, name_scope=gen_config.name_model)\n",
    "\n",
    "        #####################################################################################\n",
    "        # pre_train discriminator:\n",
    "        dis_data_loader = Dis_dataloader(BATCH_SIZE)\n",
    "        negative_file = 'generated_sample.txt'\n",
    "        positive_file = 'real_sample.txt'\n",
    "        dis_data_loader.load_train_data(positive_file, negative_file)\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True\n",
    "        sess = tf.Session(config=config)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # Train 10 epoch on the generated data and do this for 10 times\n",
    "        global_step = 0\n",
    "        for current_step in range(10):\n",
    "    \n",
    "            print(\"global_step: \")\n",
    "            print(global_step)\n",
    "        \n",
    "            for _ in range(3):\n",
    "                print(' epoch: ')\n",
    "                print(_)\n",
    "                dis_data_loader.reset_pointer()\n",
    "                for it in range(dis_data_loader.num_batch):\n",
    "                    x_batch, y_batch = dis_data_loader.next_batch()\n",
    "                    feed = {\n",
    "                        discriminator.input_x: x_batch,\n",
    "                        discriminator.input_y: y_batch,\n",
    "                        discriminator.dropout_keep_prob: dis_dropout_keep_prob\n",
    "                    }\n",
    "                    _ = sess.run(discriminator.train_op, feed)\n",
    "                global_step = global_step + 1\n",
    "        ######################################################################################\n",
    "        # train GAN:\n",
    "        \n",
    "        current_step = 0\n",
    "        step_time, disc_loss, gen_loss, t_loss, batch_reward = 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "        gen_loss_summary = tf.Summary()\n",
    "        disc_loss_summary = tf.Summary()\n",
    "\n",
    "        gen_writer = tf.summary.FileWriter(gen_config.tensorboard_dir, sess.graph)\n",
    "        disc_writer = tf.summary.FileWriter(disc_config.tensorboard_dir, sess.graph)\n",
    "\n",
    "        while True:\n",
    "            current_step += 1\n",
    "            start_time = time.time()\n",
    "            random_number_01 = np.random.random_sample()\n",
    "            bucket_id = min([i for i in xrange(len(train_buckets_scale))\n",
    "                         if train_buckets_scale[i] > random_number_01])\n",
    "            \n",
    "            print(\"==================Update Generator: %d=========================\" % current_step)\n",
    "            # 1.Sample (X,Y) from real disc_data\n",
    "            update_gen_data = gen_model.get_batch(train_set, bucket_id, gen_config.batch_size)\n",
    "            encoder, decoder, weights, source_inputs, source_outputs = update_gen_data\n",
    "\n",
    "            # 2.Sample (X,Y) and (X, ^Y) through ^Y ~ G(*|X) with Monte Carlo search\n",
    "            train_query, train_answer, train_labels = disc_train_data(sess, gen_model, vocab, source_inputs, source_outputs,\n",
    "                                                                encoder, decoder, weights, bucket_id, mc_search=True)\n",
    "\n",
    "            print(\"=============================mc_search: True====================================\")\n",
    "            \n",
    "            # train query is the input of generator, in this model it is the document which need to summarize.  \n",
    "            # train_query = np.transpose(train_query)\n",
    "            # train answer is the summarization generated by generator\n",
    "            train_answer = np.transpose(train_answer)\n",
    "\n",
    "            # 3.Compute Reward r for (X, ^Y ) using D.---based on Monte Carlo search\n",
    "            #***********************************************************************#\n",
    "            reward = get_reward(sess,  train_answer, rollout_num, discriminator)\n",
    "            batch_reward += reward / gen_config.steps_per_checkpoint\n",
    "            # 为什么要除以steps_per_checkpoint??\n",
    "            print(\"step_reward: \", reward)\n",
    "            \n",
    "            #*************************************************************************#\n",
    "            # 4.Update G on (X, ^Y ) using reward r\n",
    "            gan_adjusted_loss, gen_step_loss, _ =gen_model.step(sess, encoder, decoder, weights, bucket_id, forward_only=False,\n",
    "                                           reward=reward, up_reward=True, debug=True)\n",
    "            gen_loss += gen_step_loss / gen_config.steps_per_checkpoint\n",
    "            #为什么要除以steps_per_checkpoint??\n",
    "\n",
    "            print(\"gen_step_loss: \", gen_step_loss)\n",
    "            print(\"gen_step_adjusted_loss: \", gan_adjusted_loss)\n",
    "\n",
    "            # 5.Teacher-Forcing: Update G on (X, Y )\n",
    "            t_adjusted_loss, t_step_loss, a = gen_model.step(sess, encoder, decoder, weights, bucket_id, forward_only=False)\n",
    "            t_loss += t_step_loss / gen_config.steps_per_checkpoint\n",
    "           \n",
    "            print(\"t_step_loss: \", t_step_loss)\n",
    "            print(\"t_adjusted_loss\", t_adjusted_loss)           \n",
    "\n",
    "            if current_step % gen_config.steps_per_checkpoint == 0:\n",
    "\n",
    "                step_time += (time.time() - start_time) / gen_config.steps_per_checkpoint\n",
    "\n",
    "                print(\"current_steps: %d, step time: %.4f, disc_loss: %.3f, gen_loss: %.3f, t_loss: %.3f, reward: %.3f\"\n",
    "                      %(current_step, step_time, disc_loss, gen_loss, t_loss, batch_reward))\n",
    "\n",
    "                gen_global_steps = sess.run(gen_model.global_step)\n",
    "                gen_loss_value = gen_loss_summary.value.add()\n",
    "                gen_loss_value.tag = gen_config.name_loss\n",
    "                gen_loss_value.simple_value = float(gen_loss)\n",
    "                t_loss_value = gen_loss_summary.value.add()\n",
    "                t_loss_value.tag = gen_config.teacher_loss\n",
    "                t_loss_value.simple_value = float(t_loss)\n",
    "                batch_reward_value = gen_loss_summary.value.add()\n",
    "                batch_reward_value.tag = gen_config.reward_name\n",
    "                batch_reward_value.simple_value = float(batch_reward)\n",
    "                gen_writer.add_summary(gen_loss_summary, int(gen_global_steps))\n",
    "\n",
    "                if current_step % (gen_config.steps_per_checkpoint * 2) == 0:\n",
    "                    print(\"current_steps: %d, save gen model\" % current_step)\n",
    "                    gen_ckpt_dir = os.path.abspath(os.path.join(gen_config.train_dir, \"checkpoints\"))\n",
    "                    if not os.path.exists(gen_ckpt_dir):\n",
    "                        os.makedirs(gen_ckpt_dir)\n",
    "                    gen_model_path = os.path.join(gen_ckpt_dir, \"gen.model\")\n",
    "                    gen_model.saver.save(sess, gen_model_path, global_step=gen_model.global_step)\n",
    "\n",
    "                step_time, disc_loss, gen_loss, t_loss, batch_reward = 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "                sys.stdout.flush()\n",
    "\n",
    "            \n",
    "            ## train 一段时间generator 后 train 一次discriminator：\n",
    "            ## 这个k次generator 1次discriminator 的loop 等下再写\n",
    "            \n",
    "            print(\"==================Update Discriminator: %d=====================\" % current_step)\n",
    "            # after train generator, generate new samples for training discriminator\n",
    "            decode_num_step = 5000\n",
    "            generated_data_for_D_in_ALLtrain(Flag,decode_num_step)\n",
    "            dis_data_loader.load_train_data(positive_file, negative_file)\n",
    "            # Train 10 epoch on the generated data and do this for 10 times\n",
    "            global_step = 0\n",
    "            for current_step in range(10):\n",
    "    \n",
    "                print(\"global_step: \")\n",
    "                print(global_step)\n",
    "        \n",
    "                for _ in range(3):\n",
    "                    print(' epoch: ')\n",
    "                    print(_)\n",
    "                    dis_data_loader.reset_pointer()\n",
    "                    for it in range(dis_data_loader.num_batch):\n",
    "                        x_batch, y_batch = dis_data_loader.next_batch()\n",
    "                        feed = {\n",
    "                            discriminator.input_x: x_batch,\n",
    "                            discriminator.input_y: y_batch,\n",
    "                            discriminator.dropout_keep_prob: dis_dropout_keep_prob\n",
    "                        }\n",
    "                        _ = sess.run(discriminator.train_op, feed)\n",
    "                    global_step = global_step + 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 pre train G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#train(gen_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 generate data for D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Flag = True # Flag = true: write a csv file Flag = False, don't write\n",
    "#Flag = False\n",
    "#decode_num_step = 50000\n",
    "#generated_data_for_D(Flag,decode_num_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3 pre train D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# trained in all_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 train GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# all_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: test generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Flag = True # Flag = true: write a csv file Flag = False, don't write\n",
    "#Flag = False\n",
    "#decode_num_step = 50000\n",
    "#generated_data_for_D(Flag,decode_num_step)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
