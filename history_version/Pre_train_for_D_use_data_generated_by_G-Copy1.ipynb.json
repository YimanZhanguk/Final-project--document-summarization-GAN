{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import codecs\n",
    "from sklearn.utils import shuffle\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import re\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.ops import nn_ops\n",
    "from tensorflow.python.ops import embedding_ops\n",
    "from tensorflow.python.ops import variable_scope as vs\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import init_ops\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.util import nest\n",
    "from tensorflow.python.ops import variable_scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PAD_ID = 0\n",
    "GO_ID = 1\n",
    "EOS_ID = 2\n",
    "UNK_ID = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sequence_loss_by_example(logits, targets, weights,\n",
    "                             average_across_timesteps=True,\n",
    "                             softmax_loss_function=None, name=None):\n",
    "    # print('sequence_loss_by_example')\n",
    "    if len(targets) != len(logits) or len(weights) != len(logits):\n",
    "        raise ValueError(\"Lengths of logits, weights, and targets must be the same \"\n",
    "                     \"%d, %d, %d.\" % (len(logits), len(weights), len(targets)))\n",
    "    with ops.name_scope(name, \"sequence_loss_by_example\",\n",
    "                      logits + targets + weights):\n",
    "        log_perp_list = []\n",
    "        for logit, target, weight in zip(logits, targets, weights):\n",
    "            if softmax_loss_function is None:\n",
    "                target = array_ops.reshape(target, [-1])\n",
    "                crossent = nn_ops.sparse_softmax_cross_entropy_with_logits(logit, target)\n",
    "            else:\n",
    "                crossent = softmax_loss_function(logit, target)\n",
    "            log_perp_list.append(crossent * weight)\n",
    "        log_perps = math_ops.add_n(log_perp_list)\n",
    "        if average_across_timesteps:\n",
    "            total_size = math_ops.add_n(weights)\n",
    "            total_size += 1e-12  # Just to avoid division by 0 for all-0 weights.\n",
    "            log_perps /= total_size\n",
    "    return log_perps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sequence_loss(logits, targets, weights,\n",
    "                  average_across_timesteps=True, average_across_batch=True,\n",
    "                  softmax_loss_function=None, name=None):\n",
    "    #print(\"sequence_loss\")\n",
    "    with ops.name_scope(name, \"sequence_loss\", logits + targets + weights):\n",
    "        cost = math_ops.reduce_sum(sequence_loss_by_example(\n",
    "                                                        logits, targets, weights,\n",
    "                                                        average_across_timesteps=average_across_timesteps,\n",
    "                                                        softmax_loss_function=softmax_loss_function))\n",
    "        if average_across_batch:\n",
    "            batch_size = array_ops.shape(targets[0])[0]\n",
    "            return cost / math_ops.cast(batch_size, cost.dtype)\n",
    "        else:\n",
    "            return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_BIAS_VARIABLE_NAME = \"bias\"\n",
    "_WEIGHTS_VARIABLE_NAME = \"kernel\"\n",
    "def linear(args,\n",
    "            output_size,\n",
    "            bias,\n",
    "            bias_initializer=None,\n",
    "            kernel_initializer=None):\n",
    "    if args is None or (nest.is_sequence(args) and not args):\n",
    "        raise ValueError(\"`args` must be specified\")\n",
    "    if not nest.is_sequence(args):\n",
    "        args = [args]\n",
    "\n",
    "    # Calculate the total size of arguments on dimension 1.\n",
    "    total_arg_size = 0\n",
    "    shapes = [a.get_shape() for a in args]\n",
    "    for shape in shapes:\n",
    "        if shape.ndims != 2:\n",
    "            raise ValueError(\"linear is expecting 2D arguments: %s\" % shapes)\n",
    "        if shape[1].value is None:\n",
    "            raise ValueError(\"linear expects shape[1] to be provided for shape %s, \"\n",
    "                       \"but saw %s\" % (shape, shape[1]))\n",
    "        else:\n",
    "            total_arg_size += shape[1].value\n",
    "\n",
    "    dtype = [a.dtype for a in args][0]\n",
    "    \n",
    "    # Now the computation.\n",
    "    scope = vs.get_variable_scope()\n",
    "    with vs.variable_scope(scope) as outer_scope:\n",
    "        weights = vs.get_variable(\n",
    "                                    _WEIGHTS_VARIABLE_NAME, [total_arg_size, output_size],\n",
    "                                    dtype=dtype,\n",
    "                                    initializer=kernel_initializer)\n",
    "        if len(args) == 1:\n",
    "            res = math_ops.matmul(args[0], weights)\n",
    "        else:\n",
    "            res = math_ops.matmul(array_ops.concat(args, 1), weights)\n",
    "        if not bias:\n",
    "            return res\n",
    "        with vs.variable_scope(outer_scope) as inner_scope:\n",
    "            inner_scope.set_partitioner(None)\n",
    "            if bias_initializer is None:\n",
    "                bias_initializer = init_ops.constant_initializer(0.0, dtype=dtype)\n",
    "            biases = vs.get_variable(\n",
    "                                      _BIAS_VARIABLE_NAME, [output_size],\n",
    "                                      dtype=dtype,\n",
    "                                        initializer=bias_initializer)\n",
    "        return nn_ops.bias_add(res, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def attention_decoder(decoder_inputs,\n",
    "                      initial_state,\n",
    "                      attention_states,\n",
    "                      cell,\n",
    "                      output_size=None,\n",
    "                      num_heads=1,\n",
    "                      loop_function=None,\n",
    "                      dtype=None,\n",
    "                      scope=None,\n",
    "                      initial_state_attention=False):\n",
    "\n",
    "    #print('attention_decoder')\n",
    "    if not decoder_inputs:\n",
    "        raise ValueError(\"Must provide at least 1 input to attention decoder.\")\n",
    "    if num_heads < 1:\n",
    "        raise ValueError(\"With less than 1 heads, use a non-attention decoder.\")\n",
    "    if attention_states.get_shape()[2].value is None:\n",
    "        raise ValueError(\"Shape[2] of attention_states must be known: %s\"\n",
    "                     % attention_states.get_shape())\n",
    "    if output_size is None:\n",
    "        output_size = cell.output_size\n",
    "\n",
    "    with variable_scope.variable_scope(scope or \"attention_decoder\", dtype=dtype) as scope:\n",
    "        dtype = scope.dtype\n",
    "\n",
    "        batch_size = array_ops.shape(decoder_inputs[0])[0]  # Needed for reshaping.\n",
    "        attn_length = attention_states.get_shape()[1].value\n",
    "        if attn_length is None:\n",
    "            attn_length = shape(attention_states)[1]\n",
    "        attn_size = attention_states.get_shape()[2].value\n",
    "\n",
    "        # To calculate W1 * h_t we use a 1-by-1 convolution, need to reshape before.\n",
    "        hidden = array_ops.reshape(attention_states, [-1, attn_length, 1, attn_size])\n",
    "        hidden_features = []\n",
    "        v = []\n",
    "        attention_vec_size = attn_size  # Size of query vectors for attention.\n",
    "        for a in xrange(num_heads):\n",
    "            k = variable_scope.get_variable(\"AttnW_%d\" % a,\n",
    "                                      [1, 1, attn_size, attention_vec_size])\n",
    "            hidden_features.append(nn_ops.conv2d(hidden, k, [1, 1, 1, 1], \"SAME\"))\n",
    "            v.append(variable_scope.get_variable(\"AttnV_%d\" % a, [attention_vec_size]))\n",
    "\n",
    "        state = initial_state\n",
    "\n",
    "        def attention(query):\n",
    "            \"\"\"Put attention masks on hidden using hidden_features and query.\"\"\"\n",
    "            #print(\"attention_decoder.attention\")\n",
    "            ds = []  # Results of attention reads will be stored here.\n",
    "            if nest.is_sequence(query):  # If the query is a tuple, flatten it.\n",
    "                query_list = nest.flatten(query)\n",
    "                for q in query_list:  # Check that ndims == 2 if specified.\n",
    "                    ndims = q.get_shape().ndims\n",
    "                    if ndims:\n",
    "                        assert ndims == 2\n",
    "                query = array_ops.concat(query_list,1)\n",
    "            for a in xrange(num_heads):\n",
    "                with variable_scope.variable_scope(\"Attention_%d\" % a):\n",
    "                    #print('调用linear')\n",
    "                    y = linear(query, attention_vec_size,True)\n",
    "                    y = array_ops.reshape(y, [-1, 1, 1, attention_vec_size])\n",
    "                    # Attention mask is a softmax of v^T * tanh(...).\n",
    "                    s = math_ops.reduce_sum(v[a] * math_ops.tanh(hidden_features[a] + y), [2, 3])\n",
    "                    a = nn_ops.softmax(s)\n",
    "                    # Now calculate the attention-weighted vector d.\n",
    "                    d = math_ops.reduce_sum(\n",
    "                    array_ops.reshape(a, [-1, attn_length, 1, 1]) * hidden,[1, 2])\n",
    "                    ds.append(array_ops.reshape(d, [-1, attn_size]))\n",
    "            return ds\n",
    "\n",
    "        outputs = []\n",
    "        prev = None\n",
    "        batch_attn_size = array_ops.stack([batch_size, attn_size])\n",
    "        attns = [array_ops.zeros(batch_attn_size, dtype=dtype)for _ in xrange(num_heads)]\n",
    "        for a in attns:  # Ensure the second shape of attention vectors is set.\n",
    "            a.set_shape([None, attn_size])\n",
    "        if initial_state_attention:\n",
    "            attns = attention(initial_state)\n",
    "        for i, inp in enumerate(decoder_inputs):\n",
    "            if i > 0:\n",
    "                variable_scope.get_variable_scope().reuse_variables()\n",
    "            # If loop_function is set, we use it instead of decoder_inputs.\n",
    "            if loop_function is not None and prev is not None:\n",
    "                with variable_scope.variable_scope(\"loop_function\", reuse=True):\n",
    "                    inp = loop_function(prev, i)\n",
    "            # Merge input and previous attentions into one vector of the right size.\n",
    "            input_size = inp.get_shape().with_rank(2)[1]\n",
    "            if input_size.value is None:\n",
    "                raise ValueError(\"Could not infer input size from input: %s\" % inp.name)\n",
    "                \n",
    "            #print('调用linear')\n",
    "            x = linear([inp] + attns, input_size,True)\n",
    "            cell_output, state = cell(x, state)\n",
    "            if i == 0 and initial_state_attention:\n",
    "                #print('找错1')\n",
    "                with variable_scope.variable_scope(variable_scope.get_variable_scope(),\n",
    "                                           reuse=True):\n",
    "                    #print(\"找错2\")\n",
    "                    attns = attention(state)\n",
    "            else:\n",
    "                #print(\"找错3\")\n",
    "                attns = attention(state)\n",
    "            \n",
    "            with variable_scope.variable_scope(\"AttnOutputProjection\"):\n",
    "                #print('output = linear([cell_output] + attns, output_size,True)')\n",
    "                output = linear([cell_output] + attns, output_size,True)\n",
    "            if loop_function is not None:\n",
    "                prev = output\n",
    "            #print('outputs.append(output)')\n",
    "            outputs.append(output)\n",
    "\n",
    "    return outputs, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _argmax_or_mcsearch(embedding, output_projection=None, update_embedding=True, mc_search=False):\n",
    "    def loop_function(prev, _):\n",
    "        if output_projection is not None:\n",
    "            prev = nn_ops.xw_plus_b(prev, output_projection[0], output_projection[1])\n",
    "\n",
    "\n",
    "        if isinstance(mc_search, bool):\n",
    "            prev_symbol = tf.reshape(tf.multinomial(prev, 1), [-1]) if mc_search else math_ops.argmax(prev, 1)\n",
    "        else:\n",
    "            prev_symbol = tf.cond(mc_search, lambda: tf.reshape(tf.multinomial(prev, 1), [-1]), lambda: tf.argmax(prev, 1))\n",
    "\n",
    "\n",
    "        emb_prev = embedding_ops.embedding_lookup(embedding, prev_symbol)\n",
    "        if not update_embedding:\n",
    "            emb_prev = array_ops.stop_gradient(emb_prev)\n",
    "        return emb_prev\n",
    "    return loop_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def embedding_attention_decoder(decoder_inputs,\n",
    "                                initial_state,\n",
    "                                attention_states,\n",
    "                                cell,\n",
    "                                num_symbols,\n",
    "                                embedding_size,\n",
    "                                num_heads=1,\n",
    "                                output_size=None,\n",
    "                                output_projection=None,\n",
    "                                feed_previous=False,\n",
    "                                update_embedding_for_previous=True,\n",
    "                                dtype=None,\n",
    "                                scope=None,\n",
    "                                initial_state_attention=False,\n",
    "                                mc_search = False):\n",
    "    #print('embedding_attention_decoder')\n",
    "    if output_size is None:\n",
    "        output_size = cell.output_size\n",
    "    if output_projection is not None:\n",
    "        proj_biases = ops.convert_to_tensor(output_projection[1], dtype=dtype)\n",
    "        proj_biases.get_shape().assert_is_compatible_with([num_symbols])\n",
    "\n",
    "    with variable_scope.variable_scope(scope or \"embedding_attention_decoder\", dtype=dtype) as scope:\n",
    "        embedding = variable_scope.get_variable(\"embedding\",[num_symbols, embedding_size])\n",
    "\n",
    "        loop_function = None\n",
    "        if feed_previous == True:\n",
    "            loop_function = _argmax_or_mcsearch(embedding, output_projection, update_embedding_for_previous, mc_search)\n",
    "\n",
    "        emb_inp = [embedding_ops.embedding_lookup(embedding, i) for i in decoder_inputs]\n",
    "        return attention_decoder(\n",
    "                                emb_inp,\n",
    "                                initial_state,\n",
    "                                attention_states,\n",
    "                                cell,\n",
    "                                output_size=output_size,\n",
    "                                num_heads=num_heads,\n",
    "                                loop_function=loop_function,\n",
    "                                initial_state_attention=initial_state_attention,\n",
    "                                scope=scope)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# return output, state, encoder_state\n",
    "\n",
    "def embedding_attention_seq2seq(encoder_inputs,\n",
    "                                decoder_inputs,\n",
    "                                cell,\n",
    "                                num_encoder_symbols,\n",
    "                                num_decoder_symbols,\n",
    "                                embedding_size,\n",
    "                                num_heads=1,\n",
    "                                output_projection=None,\n",
    "                                feed_previous=False,\n",
    "                                dtype=None,\n",
    "                                scope=None,\n",
    "                                initial_state_attention=False,\n",
    "                                mc_search=False):\n",
    "\n",
    "    with variable_scope.variable_scope(scope or \"embedding_attention_seq2seq\", dtype=dtype) as scope:\n",
    "        dtype = scope.dtype\n",
    "        #print('embedding_attention_seq2seq')\n",
    "        \n",
    "        # Encoder.\n",
    "        #print('encoder_cell')\n",
    "        encoder_cell = tf.contrib.rnn.core_rnn_cell.EmbeddingWrapper(\n",
    "                cell, embedding_classes=num_encoder_symbols,\n",
    "                embedding_size=embedding_size)\n",
    "        #print('encoder_outputs, encoder_state = tf.contrib.rnn.static_rnn')\n",
    "        encoder_outputs, encoder_state = tf.contrib.rnn.static_rnn(\n",
    "                encoder_cell, encoder_inputs, dtype=dtype)\n",
    "        #print('encoder_outputs')\n",
    "        \n",
    "        # First calculate a concatenation of encoder outputs to put attention on.\n",
    "        #print('top-state')\n",
    "        #top_states = [array_ops.reshape([-1, 1, cell.output_size],e)for e in encoder_outputs]\n",
    "        top_states = tf.stack(encoder_outputs)\n",
    "        top_states = tf.transpose(top_states, [1,0,2])\n",
    "        #print('attention_state')\n",
    "        attention_states = array_ops.concat(top_states,1)\n",
    "\n",
    "        # Decoder.\n",
    "        #print('decoder')\n",
    "        output_size = None\n",
    "        if output_projection is None:\n",
    "            cell = rnn_cell.OutputProjectionWrapper(cell, num_decoder_symbols)\n",
    "            output_size = num_decoder_symbols\n",
    "\n",
    "        if isinstance(feed_previous, bool):\n",
    "            outputs, state = embedding_attention_decoder(\n",
    "                          decoder_inputs,\n",
    "                          encoder_state,\n",
    "                          attention_states,\n",
    "                          cell,\n",
    "                          num_decoder_symbols,\n",
    "                          embedding_size,\n",
    "                          num_heads=num_heads,\n",
    "                          output_size=output_size,\n",
    "                          output_projection=output_projection,\n",
    "                          feed_previous=feed_previous,\n",
    "                          initial_state_attention=initial_state_attention,\n",
    "                          mc_search=mc_search,\n",
    "                          scope=scope)\n",
    "            return outputs, state, encoder_state\n",
    "\n",
    "        # If feed_previous is a Tensor, we construct 2 graphs and use cond.\n",
    "        def decoder(feed_previous_bool):\n",
    "            reuse = None if feed_previous_bool else True\n",
    "            with variable_scope.variable_scope(variable_scope.get_variable_scope(), reuse=reuse) as scope:\n",
    "                outputs, state = embedding_attention_decoder(\n",
    "                            decoder_inputs,\n",
    "                            encoder_state,\n",
    "                            attention_states,\n",
    "                            cell,\n",
    "                            num_decoder_symbols,\n",
    "                            embedding_size,\n",
    "                            num_heads=num_heads,\n",
    "                            output_size=output_size,\n",
    "                            output_projection=output_projection,\n",
    "                            feed_previous=feed_previous_bool,\n",
    "                            update_embedding_for_previous=False,\n",
    "                            initial_state_attention=initial_state_attention,\n",
    "                            mc_search=mc_search,\n",
    "                            scope=scope)\n",
    "                state_list = [state]\n",
    "                if nest.is_sequence(state):\n",
    "                    state_list = nest.flatten(state)\n",
    "                return outputs + state_list\n",
    "\n",
    "        outputs_and_state = control_flow_ops.cond(feed_previous,\n",
    "                                              lambda: decoder(True),\n",
    "                                              lambda: decoder(False))\n",
    "        outputs_len = len(decoder_inputs)  # Outputs length same as decoder inputs.\n",
    "        state_list = outputs_and_state[outputs_len:]\n",
    "        state = state_list[0]\n",
    "        if nest.is_sequence(encoder_state):\n",
    "            state = nest.pack_sequence_as(structure=encoder_state,\n",
    "                                    flat_sequence=state_list)\n",
    "        return outputs_and_state[:outputs_len], state, encoder_state\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# return outputs, losses, encoder_states\n",
    "\n",
    "def model_with_buckets(encoder_inputs, decoder_inputs, targets, weights, buckets, vocab_size, batch_size, seq2seq,\n",
    "                       output_projection=None, softmax_loss_function=None, per_example_loss=False, name=None):\n",
    "    #print('model_with_buckets')\n",
    "    if len(encoder_inputs) < buckets[-1][0]:\n",
    "        raise ValueError(\"Length of encoder_inputs (%d) must be at least that of la\"\n",
    "                     \"st bucket (%d).\" % (len(encoder_inputs), buckets[-1][0]))\n",
    "    if len(targets) < buckets[-1][1]:\n",
    "        raise ValueError(\"Length of targets (%d) must be at least that of last\"\n",
    "                     \"bucket (%d).\" % (len(targets), buckets[-1][1]))\n",
    "    if len(weights) < buckets[-1][1]:\n",
    "        raise ValueError(\"Length of weights (%d) must be at least that of last\"\n",
    "                     \"bucket (%d).\" % (len(weights), buckets[-1][1]))\n",
    "\n",
    "    all_inputs = encoder_inputs + decoder_inputs + targets + weights\n",
    "    losses = []\n",
    "    outputs = []\n",
    "    encoder_states = []\n",
    "    with ops.name_scope(name, \"model_with_buckets\", all_inputs):\n",
    "        for j, bucket in enumerate(buckets):\n",
    "            with variable_scope.variable_scope(variable_scope.get_variable_scope(),\n",
    "                                         reuse=True if j > 0 else None):\n",
    "                bucket_outputs, decoder_states, encoder_state = seq2seq(encoder_inputs[:bucket[0]],\n",
    "                                    decoder_inputs[:bucket[1]])\n",
    "                outputs.append(bucket_outputs)\n",
    "                #print(\"bucket outputs: %s\" %bucket_outputs)\n",
    "                encoder_states.append(encoder_state)\n",
    "                if per_example_loss:\n",
    "                    losses.append(sequence_loss_by_example(\n",
    "                    outputs[-1], targets[:bucket[1]], weights[:bucket[1]],\n",
    "                    softmax_loss_function=softmax_loss_function))\n",
    "                else:\n",
    "                    # losses.append(sequence_loss_by_mle(outputs[-1], targets[:bucket[1]], vocab_size, bucket[1], batch_size, output_projection))\n",
    "                    losses.append(sequence_loss(outputs[-1], targets[:bucket[1]], weights[:bucket[1]], softmax_loss_function=softmax_loss_function))\n",
    "\n",
    "    return outputs, losses, encoder_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Seq2SeqModel(object):\n",
    "    def __init__(self, config,name_scope, forward_only,use_lstm=False,num_samples=512):\n",
    "\n",
    "        dtype=tf.float32\n",
    "        \n",
    "        source_vocab_size = config.vocab_size\n",
    "        target_vocab_size = config.vocab_size\n",
    "        size= config.emb_dim\n",
    "\n",
    "        self.buckets = config.buckets\n",
    "        self.batch_size = config.batch_size\n",
    "        self.learning_rate = tf.Variable(float(config.learning_rate), trainable=False)\n",
    "        self.learning_rate_decay_op = self.learning_rate.assign(\n",
    "            self.learning_rate * config.learning_rate_decay_factor)\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        max_gradient_norm = config.max_gradient_norm\n",
    "        self.forward_only = tf.placeholder(tf.bool, name=\"forward_only\")\n",
    "        #self.num_layers = config.num_layers\n",
    "        num_layers = config.num_layers\n",
    "        \n",
    "        # If we use sampled softmax, we need an output projection.\n",
    "        output_projection = None\n",
    "        softmax_loss_function = None\n",
    "        \n",
    "        # ADD\n",
    "        self.mc_search = tf.placeholder(tf.bool, name=\"mc_search\")\n",
    "        self.up_reward = tf.placeholder(tf.bool, name=\"up_reward\")\n",
    "        self.reward_bias = tf.get_variable(\"reward_bias\", [1], dtype=tf.float32)\n",
    "        \n",
    "        # Sampled softmax only makes sense if we sample less than vocabulary size.\n",
    "        if num_samples > 0 and num_samples < target_vocab_size:\n",
    "            w = tf.get_variable(\"proj_w\", [size, target_vocab_size])\n",
    "            w_t = tf.transpose(w)\n",
    "            b = tf.get_variable(\"proj_b\", [target_vocab_size])\n",
    "            output_projection = (w, b)\n",
    "\n",
    "            def sampled_loss(inputs, labels):\n",
    "                \n",
    "                labels = tf.reshape(labels, [-1, 1])\n",
    "                local_w_t = tf.cast(w_t, tf.float32)\n",
    "                local_b = tf.cast(b, tf.float32)\n",
    "                local_inputs = tf.cast(inputs, tf.float32)\n",
    "                \n",
    "                return tf.cast(\n",
    "                    tf.nn.sampled_softmax_loss(local_w_t, local_b, labels, local_inputs,\n",
    "                                               num_samples, target_vocab_size), dtype)\n",
    "            softmax_loss_function = sampled_loss\n",
    "\n",
    "        # Create the internal multi-layer cell for our RNN.\n",
    "        single_cell = tf.contrib.rnn.GRUCell(size)\n",
    "        print('single_cell')\n",
    "        if use_lstm:\n",
    "            single_cell = tf.contrib.rnn.LSTMCell(size)\n",
    "        cell = single_cell\n",
    "        cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=0.5)\n",
    "        if num_layers > 1:\n",
    "            cell = tf.contrib.rnn.MultiRNNCell([single_cell] * num_layers)\n",
    "        # tf.contrib.seq2seq\n",
    "        # The seq2seq function: we use embedding for the input and attention.\n",
    "        def seq2seq_f(encoder_inputs, decoder_inputs, do_decode):\n",
    "            return embedding_attention_seq2seq(\n",
    "                  encoder_inputs, decoder_inputs, cell,\n",
    "                  num_encoder_symbols=source_vocab_size,\n",
    "                  num_decoder_symbols=target_vocab_size,\n",
    "                  embedding_size=size,\n",
    "                  output_projection=output_projection,\n",
    "                  feed_previous=do_decode)\n",
    "\n",
    "        # Feeds for inputs.\n",
    "        self.encoder_inputs = []\n",
    "        self.decoder_inputs = []\n",
    "        self.target_weights = []\n",
    "        for i in xrange(self.buckets[-1][0]):  # Last bucket is the biggest one.\n",
    "            self.encoder_inputs.append(tf.placeholder(tf.int32, shape=[None],\n",
    "                                                name=\"encoder{0}\".format(i)))\n",
    "        for i in xrange(self.buckets[-1][1] + 1):\n",
    "            self.decoder_inputs.append(tf.placeholder(tf.int32, shape=[None],\n",
    "                                                name=\"decoder{0}\".format(i)))\n",
    "            self.target_weights.append(tf.placeholder(tf.float32, shape=[None],\n",
    "                                                name=\"weight{0}\".format(i)))\n",
    "        self.reward = [tf.placeholder(tf.float32, name=\"reward_%i\" % i) for i in range(len(self.buckets))] # ADD\n",
    "        \n",
    "        \n",
    "        # Our targets are decoder inputs shifted by one.\n",
    "        targets = [self.decoder_inputs[i + 1]\n",
    "               for i in xrange(len(self.decoder_inputs) - 1)]\n",
    "\n",
    "        # Training outputs and losses.\n",
    "        if forward_only:\n",
    "            print('foward_only is true')\n",
    "            self.outputs, self.losses,self.encoder_state  = model_with_buckets(\n",
    "                  self.encoder_inputs, self.decoder_inputs, targets,\n",
    "                  self.target_weights, self.buckets, source_vocab_size, self.batch_size,\n",
    "                  lambda x, y: seq2seq_f(x, y, tf.where(self.forward_only, True, False)),\n",
    "                  output_projection=output_projection,\n",
    "                  softmax_loss_function=softmax_loss_function)\n",
    "            # If we use output projection, we need to project outputs for decoding.\n",
    "            if output_projection is not None:\n",
    "                for b in xrange(len(self.buckets)):\n",
    "                    self.outputs[b] = [\n",
    "                      tf.matmul(output, output_projection[0]) + output_projection[1]\n",
    "                      for output in self.outputs[b]\n",
    "                    ]\n",
    "        else:\n",
    "            print('forward_only is false')\n",
    "            self.outputs, self.losses, self.encoder_state = model_with_buckets(\n",
    "                  self.encoder_inputs, self.decoder_inputs, targets,\n",
    "                  self.target_weights, self.buckets, source_vocab_size, self.batch_size,\n",
    "                  lambda x, y: seq2seq_f(x, y, tf.where(self.forward_only, False, False)),\n",
    "                  output_projection=output_projection,\n",
    "                  softmax_loss_function=softmax_loss_function)\n",
    "            \n",
    "        if not forward_only:\n",
    "            with tf.name_scope(\"gradient_descent\"):\n",
    "                self.gradient_norms = []\n",
    "                self.updates = []\n",
    "                self.aj_losses = []\n",
    "                self.gen_params = [p for p in tf.trainable_variables() if name_scope in p.name]\n",
    "                opt = tf.train.AdamOptimizer()\n",
    "                for b in xrange(len(self.buckets)):\n",
    "                    R =  tf.sub(self.reward[b], self.reward_bias)\n",
    "                    adjusted_loss = tf.cond(self.up_reward,\n",
    "                                              lambda:tf.mul(self.losses[b], self.reward[b]),\n",
    "                                              lambda: self.losses[b])\n",
    "\n",
    "                    self.aj_losses.append(adjusted_loss)\n",
    "                    gradients = tf.gradients(adjusted_loss, self.gen_params)\n",
    "                    clipped_gradients, norm = tf.clip_by_global_norm(gradients, self.max_gradient_norm)\n",
    "                    self.gradient_norms.append(norm)\n",
    "                    self.updates.append(opt.apply_gradients(\n",
    "                        zip(clipped_gradients, self.gen_params), global_step=self.global_step))\n",
    "\n",
    "        # Gradients and SGD update operation for training the model.\n",
    "        params = tf.trainable_variables()\n",
    "        if not forward_only:\n",
    "            self.gradient_norms = []\n",
    "            self.updates = []\n",
    "            opt = tf.train.AdamOptimizer()\n",
    "            for b in xrange(len(self.buckets)):\n",
    "                gradients = tf.gradients(self.losses[b], params)\n",
    "                clipped_gradients, norm = tf.clip_by_global_norm(gradients,\n",
    "                                                         max_gradient_norm)\n",
    "                self.gradient_norms.append(norm)\n",
    "                self.updates.append(opt.apply_gradients(\n",
    "                    zip(clipped_gradients, params), global_step=self.global_step))\n",
    "\n",
    "        self.saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "    def step(self, session, encoder_inputs, decoder_inputs, target_weights,\n",
    "           bucket_id, forward_only=True):\n",
    "\n",
    "        # Check if the sizes match.\n",
    "        encoder_size, decoder_size = self.buckets[bucket_id]\n",
    "        if len(encoder_inputs) != encoder_size:\n",
    "            raise ValueError(\"Encoder length must be equal to the one in bucket,\"\n",
    "                       \" %d != %d.\" % (len(encoder_inputs), encoder_size))\n",
    "        if len(decoder_inputs) != decoder_size:\n",
    "            raise ValueError(\"Decoder length must be equal to the one in bucket,\"\n",
    "                       \" %d != %d.\" % (len(decoder_inputs), decoder_size))\n",
    "        if len(target_weights) != decoder_size:\n",
    "            raise ValueError(\"Weights length must be equal to the one in bucket,\"\n",
    "                       \" %d != %d.\" % (len(target_weights), decoder_size))\n",
    "\n",
    "        # Input feed: encoder inputs, decoder inputs, target_weights, as provided.\n",
    "        input_feed = {self.forward_only.name: forward_only,\n",
    "                      self.up_reward.name:  up_reward,\n",
    "                      self.mc_search.name: mc_search\n",
    "                     }\n",
    "        for l in xrange(len(self.buckets)):\n",
    "            input_feed[self.reward[l].name] = reward\n",
    "        for l in xrange(encoder_size):\n",
    "            input_feed[self.encoder_inputs[l].name] = encoder_inputs[l]\n",
    "        for l in xrange(decoder_size):\n",
    "            input_feed[self.decoder_inputs[l].name] = decoder_inputs[l]\n",
    "            input_feed[self.target_weights[l].name] = target_weights[l]\n",
    "\n",
    "        # Since our targets are decoder inputs shifted by one, we need one more.\n",
    "        last_target = self.decoder_inputs[decoder_size].name\n",
    "        input_feed[last_target] = np.zeros([self.batch_size], dtype=np.int32)\n",
    "\n",
    "        # Output feed: depends on whether we do a backward step or not.\n",
    "        if not forward_only:\n",
    "            output_feed = [self.updates[bucket_id],  # Update Op that does SGD.\n",
    "                     self.gradient_norms[bucket_id],  # Gradient norm.\n",
    "                     self.losses[bucket_id]]  # Loss for this batch.\n",
    "        else:\n",
    "            output_feed = [self.losses[bucket_id]]  # Loss for this batch.\n",
    "            for l in xrange(decoder_size):  # Output logits.\n",
    "                output_feed.append(self.outputs[bucket_id][l])\n",
    "\n",
    "        outputs = session.run(output_feed, input_feed)\n",
    "        if not forward_only:\n",
    "              return outputs[1], outputs[2], None  # Gradient norm, loss, no outputs.\n",
    "        else:\n",
    "              return None, outputs[0], outputs[1:]  # No gradient norm, loss, outputs.\n",
    "\n",
    "    def get_batch(self, data, bucket_id):\n",
    "\n",
    "        encoder_size, decoder_size = self.buckets[bucket_id]\n",
    "        encoder_inputs, decoder_inputs = [], []\n",
    "\n",
    "        for _ in xrange(self.batch_size):\n",
    "            encoder_input, decoder_input = random.choice(data[bucket_id])\n",
    "\n",
    "            # Encoder inputs are padded and then reversed.\n",
    "            encoder_pad = [PAD_ID] * (encoder_size - len(encoder_input))\n",
    "            encoder_inputs.append(list(reversed(encoder_input + encoder_pad)))\n",
    "\n",
    "            # Decoder inputs get an extra \"GO\" symbol, and are padded then.\n",
    "            decoder_pad_size = decoder_size - len(decoder_input) - 1\n",
    "            decoder_inputs.append([GO_ID] + decoder_input +\n",
    "                            [PAD_ID] * decoder_pad_size)\n",
    "\n",
    "        # Now we create batch-major vectors from the data selected above.\n",
    "        batch_encoder_inputs, batch_decoder_inputs, batch_weights = [], [], []\n",
    "\n",
    "        # Batch encoder inputs are just re-indexed encoder_inputs.\n",
    "        for length_idx in xrange(encoder_size):\n",
    "            batch_encoder_inputs.append(\n",
    "                np.array([encoder_inputs[batch_idx][length_idx]\n",
    "                    for batch_idx in xrange(self.batch_size)], dtype=np.int32))\n",
    "\n",
    "        # Batch decoder inputs are re-indexed decoder_inputs, we create weights.\n",
    "        for length_idx in xrange(decoder_size):\n",
    "            batch_decoder_inputs.append(\n",
    "                np.array([decoder_inputs[batch_idx][length_idx]\n",
    "                    for batch_idx in xrange(self.batch_size)], dtype=np.int32))\n",
    "\n",
    "            # Create target_weights to be 0 for targets that are padding.\n",
    "            batch_weight = np.ones(self.batch_size, dtype=np.float32)\n",
    "            for batch_idx in xrange(self.batch_size):\n",
    "                # We set weight to 0 if the corresponding target is a PAD symbol.\n",
    "                # The corresponding target is decoder_input shifted by 1 forward.\n",
    "                if length_idx < decoder_size - 1:\n",
    "                    target = decoder_inputs[batch_idx][length_idx + 1]\n",
    "                if length_idx == decoder_size - 1 or target == PAD_ID:\n",
    "                    batch_weight[batch_idx] = 0.0\n",
    "            batch_weights.append(batch_weight)\n",
    "        return batch_encoder_inputs, batch_decoder_inputs, batch_weights, encoder_inputs,decoder_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(config):\n",
    "\n",
    "    train_data_set = [[] for _ in config.buckets]\n",
    "    dev_data_set = [[] for _ in config.buckets]\n",
    "\n",
    "    s2s_data = pd.read_csv('_news_data.csv')\n",
    "    train_data = s2s_data[0:900000]\n",
    "    dev_data = s2s_data[900000:1000000]\n",
    "    \n",
    "    train_source = train_data['news_content']\n",
    "    train_target = train_data['news_summary']\n",
    "    \n",
    "    dev_source = dev_data['news_content']\n",
    "    dev_target = dev_data['news_summary']\n",
    "    \n",
    "    vocabulary = json.load(open('newdataset_vocabulary.json','r'))\n",
    "    \n",
    "    for m in range(0,len(train_source)):\n",
    "        if m % 200000 == 0:\n",
    "            print(\"  reading train data line %d\" % m)\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "        source_sen = train_source[m]\n",
    "        target_sen = train_target[m]\n",
    "            \n",
    "        source_word = []\n",
    "        target_word = []\n",
    "        \n",
    "        try:\n",
    "            s_w = source_sen.split(' ')\n",
    "        except:\n",
    "            s_w = ['float']\n",
    "        for i in range(0,len(s_w)):\n",
    "            s_w[i] = s_w[i].replace('.','')\n",
    "            s_w[i] = s_w[i].replace(',','')\n",
    "            s_w[i] = s_w[i].replace('?','')\n",
    "            s_w[i] = s_w[i].replace('(','')\n",
    "            s_w[i] = s_w[i].replace(')','')\n",
    "            s_w[i] = s_w[i].replace('!','')\n",
    "            s_w[i] = s_w[i].lower()\n",
    "            source_word.append(s_w[i])\n",
    "            \n",
    "        s_w = target_sen.split(' ')\n",
    "        for i in range(0,len(s_w)):\n",
    "            s_w[i] = s_w[i].replace('.','')\n",
    "            s_w[i] = s_w[i].replace(',','')\n",
    "            s_w[i] = s_w[i].replace('?','')\n",
    "            s_w[i] = s_w[i].replace('(','')\n",
    "            s_w[i] = s_w[i].replace(')','')\n",
    "            s_w[i] = s_w[i].replace('!','')\n",
    "            s_w[i] = s_w[i].lower()\n",
    "            target_word.append(s_w[i])            \n",
    "            \n",
    "        source_ids = []\n",
    "        target_ids = []\n",
    "            \n",
    "        # convert to number\n",
    "        \n",
    "        for w in source_word:\n",
    "            try:\n",
    "                source_ids.append(vocabulary[w])\n",
    "            except:\n",
    "                source_ids.append(3)# 3 is UNK token\n",
    "                \n",
    "        # add start tag to target sentence\n",
    "        #target_ids.append(1) # 1 is _GO: start token\n",
    "  \n",
    "        for w in target_word:\n",
    "            try:\n",
    "                target_ids.append(vocabulary[w])\n",
    "            except:\n",
    "                target_ids.append(3)# 3 is UNK token\n",
    "\n",
    "        # add finish tag to target sentence. \n",
    "        target_ids.append(EOS_ID) # EOS_ID is end token\n",
    "        \n",
    "        \n",
    "        # bucket data\n",
    "        #config.buckets:  [bucket_id, (source_size, target_size)]\n",
    "        for bucket_id, (source_size, target_size) in enumerate(config.buckets): #[bucket_id, (source_size, target_size)]\n",
    "            if len(source_ids) < source_size and len(target_ids) < target_size:\n",
    "                train_data_set[bucket_id].append([source_ids, target_ids])\n",
    "                break\n",
    "                 \n",
    "    for m in range(900000,900000+len(dev_source)):\n",
    "        if m % 200000 == 0:\n",
    "            print(\"  reading dev data line %d\" % m)\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "        source_sen = dev_source[m]\n",
    "        target_sen = dev_target[m]\n",
    " \n",
    "        source_word = []\n",
    "        target_word = []\n",
    "        \n",
    "        try:\n",
    "            s_w = source_sen.split(' ')\n",
    "        except:\n",
    "            s_w = ['float']\n",
    "        for i in range(0,len(s_w)):\n",
    "            s_w[i] = s_w[i].replace('.','')\n",
    "            s_w[i] = s_w[i].replace(',','')\n",
    "            s_w[i] = s_w[i].replace('?','')\n",
    "            s_w[i] = s_w[i].replace('(','')\n",
    "            s_w[i] = s_w[i].replace(')','')\n",
    "            s_w[i] = s_w[i].replace('!','')\n",
    "            s_w[i] = s_w[i].lower()\n",
    "            source_word.append(s_w[i])\n",
    "            \n",
    "        s_w = target_sen.split(' ')\n",
    "        for i in range(0,len(s_w)):\n",
    "            s_w[i] = s_w[i].replace('.','')\n",
    "            s_w[i] = s_w[i].replace(',','')\n",
    "            s_w[i] = s_w[i].replace('?','')\n",
    "            s_w[i] = s_w[i].replace('(','')\n",
    "            s_w[i] = s_w[i].replace(')','')\n",
    "            s_w[i] = s_w[i].replace('!','')\n",
    "            s_w[i] = s_w[i].lower()\n",
    "            target_word.append(s_w[i])    \n",
    "            \n",
    "        source_ids = []\n",
    "        target_ids = []\n",
    "            \n",
    "        # convert to number\n",
    "        for w in source_word:\n",
    "            try:\n",
    "                source_ids.append(vocabulary[w])\n",
    "            except:\n",
    "                source_ids.append(3)\n",
    "                \n",
    "        #target_ids.append(1) # 1 is _GO: start token\n",
    "        for w in target_word:\n",
    "            try:\n",
    "                target_ids.append(vocabulary[w])\n",
    "            except:\n",
    "                target_ids.append(3)\n",
    "\n",
    "        # add finish tag to target sentence. \n",
    "        target_ids.append(EOS_ID) # End token\n",
    "                \n",
    "        # bucket data\n",
    "        #config.buckets:  [bucket_id, (source_size, target_size)]\n",
    "        for bucket_id, (source_size, target_size) in enumerate(config.buckets): #[bucket_id, (source_size, target_size)]\n",
    "            if len(source_ids) < source_size and len(target_ids) < target_size:\n",
    "                dev_data_set[bucket_id].append([source_ids, target_ids])\n",
    "                break\n",
    "                                \n",
    "    return vocabulary,train_data_set,dev_data_set\n",
    "    #,train_query,dev_query\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_data(gen_config):\n",
    "    vocab, train_set,dev_set = read_data(gen_config)\n",
    "    rev_vocab = {v: k for k, v in vocab.items()}\n",
    "    return vocab, rev_vocab, dev_set, train_set\n",
    "    #,train_query,dev_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_model(session, gen_config, forward_only,name_scope, initializer=None):\n",
    "    \"\"\"Create translation model and initialize or load parameters in session.\"\"\"\n",
    "    with tf.variable_scope(name_or_scope=name_scope, initializer=initializer):\n",
    "        model = Seq2SeqModel(gen_config, name_scope=name_scope, forward_only=forward_only)\n",
    "        \n",
    "        # checkpoint\n",
    "        gen_ckpt_dir = os.path.abspath(os.path.join(gen_config.train_dir, \"checkpoints\"))\n",
    "        # os.path.abspath: 返回path规范化的绝对路径。 \n",
    "        ckpt = tf.train.get_checkpoint_state(gen_ckpt_dir)\n",
    "        \n",
    "        if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\n",
    "            print(\"Reading Gen model parameters from %s\" % ckpt.model_checkpoint_path)\n",
    "            model.saver.restore(session, ckpt.model_checkpoint_path)\n",
    "            #return model\n",
    "        else:\n",
    "            print(\"Created Gen model with fresh parameters.\")\n",
    "            gen_global_variables = [gv for gv in tf.global_variables() if name_scope in gv.name]\n",
    "            session.run(tf.variables_initializer(gen_global_variables))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(gen_config):\n",
    "    \n",
    "    vocab, rev_vocab, dev_set, train_set = prepare_data(gen_config)\n",
    "    \n",
    "    for b_set in train_set:\n",
    "        print(\"bucket_set: \", len(b_set))\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "    #with tf.device(\"/gpu:1\"):\n",
    "        # Create model.\n",
    "        print(\"Creating %d layers of %d units.\" % (gen_config.num_layers, gen_config.emb_dim))\n",
    "        model = create_model(sess, gen_config, forward_only=False,name_scope=gen_config.name_model)\n",
    "        \n",
    "        #size of each bucket in the train_dataset.\n",
    "        train_bucket_sizes = [len(train_set[b]) for b in xrange(len(gen_config.buckets))]\n",
    "        # add them to get total train dataset size\n",
    "        train_total_size = float(sum(train_bucket_sizes))\n",
    "        # 0~1 --> id of bucket data.\n",
    "        train_buckets_scale = [sum(train_bucket_sizes[:i + 1]) / train_total_size\n",
    "                               for i in xrange(len(train_bucket_sizes))]\n",
    "\n",
    "        # This is the training loop.\n",
    "        step_time, loss = 0.0, 0.0\n",
    "        current_step = 0\n",
    "        previous_losses = []\n",
    "        gen_loss_summary = tf.Summary()\n",
    "        gen_writer = tf.summary.FileWriter(gen_config.tensorboard_dir, sess.graph)\n",
    "        print(\"training.......\")\n",
    "        while True:\n",
    "            # Choose a bucket.\n",
    "            # Choose a bucket according to data distribution. We pick a random number\n",
    "            # in [0, 1] and use the corresponding interval in train_buckets_scale.\n",
    "            random_number_01 = np.random.random_sample()\n",
    "            bucket_id = min([i for i in xrange(len(train_buckets_scale)) if train_buckets_scale[i] > random_number_01])\n",
    "\n",
    "            \n",
    "            # Get a batch and make a step.\n",
    "            start_time = time.time()\n",
    "            encoder_inputs, decoder_inputs, target_weights = model.get_batch(\n",
    "                train_set, bucket_id)\n",
    "            # model.get_batch 和 model.step 都是seq2seq里的函数。\n",
    "            _, step_loss, _ = model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id, forward_only=False)\n",
    "\n",
    "            step_time += (time.time() - start_time) / gen_config.steps_per_checkpoint\n",
    "            loss += step_loss / gen_config.steps_per_checkpoint\n",
    "            current_step += 1\n",
    "\n",
    "            \n",
    "            \n",
    "            if current_step % gen_config.steps_per_checkpoint == 0:\n",
    "                bucket_value = gen_loss_summary.value.add()\n",
    "                bucket_value.tag = gen_config.name_loss\n",
    "                bucket_value.simple_value = float(loss)\n",
    "                gen_writer.add_summary(gen_loss_summary, int(model.global_step.eval()))\n",
    "\n",
    "                # Print statistics for the previous epoch.\n",
    "                perplexity = math.exp(loss) if loss < 300 else float('inf')\n",
    "                print (\"global step %d learning rate %.4f step-time %.2f perplexity \"\n",
    "                       \"%.2f\" % (model.global_step.eval(), model.learning_rate.eval(),\n",
    "                                 step_time, perplexity))\n",
    "\n",
    "                # Decrease learning rate if no improvement was seen over last 3 times.\n",
    "                if len(previous_losses) > 2 and loss > max(previous_losses[-3:]):\n",
    "                    sess.run(model.learning_rate_decay_op)\n",
    "                previous_losses.append(loss)\n",
    "                \n",
    "                if current_step % (gen_config.steps_per_checkpoint * 3) == 0:\n",
    "                    print(\"current_step: %d, save model\" %(current_step))\n",
    "                    gen_ckpt_dir = os.path.abspath(os.path.join(gen_config.train_dir, \"checkpoints\"))\n",
    "                    if not os.path.exists(gen_ckpt_dir):\n",
    "                        #os.makedirs() 方法用于递归创建目录\n",
    "                        os.makedirs(gen_ckpt_dir)\n",
    "                    checkpoint_path = os.path.join(gen_ckpt_dir, \"chitchat.model\")\n",
    "                    model.saver.save(sess, checkpoint_path, global_step=model.global_step)\n",
    "\n",
    "                step_time, loss = 0.0, 0.0\n",
    "                sys.stdout.flush()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class gen_config(object):\n",
    "    beam_size = 10\n",
    "    learning_rate = 0.5\n",
    "    learning_rate_decay_factor = 0.99\n",
    "    max_gradient_norm = 2.0\n",
    "    batch_size = 64\n",
    "    emb_dim = 256\n",
    "    num_layers = 4\n",
    "    vocab_size = 250000\n",
    "    name_model = \"st_model\"\n",
    "    train_dir = \"./t_gen_data/\"\n",
    "    tensorboard_dir = \"./t_tensorboard/gen_log/\"\n",
    "    name_loss = \"gen_loss\"\n",
    "    teacher_loss = \"teacher_loss\"\n",
    "    reward_name = \"reward\"\n",
    "    max_train_data_size = 0\n",
    "    steps_per_checkpoint = 100\n",
    "    buckets = [(125,10),(125,12),(125,15),(125,30)]\n",
    "    buckets_concat = [(125,10),(125,12),(125,15),(125,30)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#train(gen_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## decode (generate data for discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class gen_config(object):\n",
    "    beam_size = 10\n",
    "    learning_rate = 0.5\n",
    "    learning_rate_decay_factor = 0.99\n",
    "    max_gradient_norm = 2.0\n",
    "    batch_size = 1\n",
    "    emb_dim = 256\n",
    "    num_layers = 4\n",
    "    vocab_size = 250000\n",
    "    name_model = \"st_model\"\n",
    "    train_dir = \"./t_gen_data/\"\n",
    "    tensorboard_dir = \"./t_tensorboard/gen_log/\"\n",
    "    name_loss = \"gen_loss\"\n",
    "    teacher_loss = \"teacher_loss\"\n",
    "    reward_name = \"reward\"\n",
    "    max_train_data_size = 0\n",
    "    steps_per_checkpoint = 100\n",
    "    buckets = [(125,10),(125,12),(125,15),(125,30)]\n",
    "    buckets_concat = [(125,10),(125,12),(125,15),(125,30)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode(decode_num_step):\n",
    "    \n",
    "    # Load vocabularies.\n",
    "    vocab, rev_vocab, dev_set, train_set = prepare_data(gen_config)\n",
    "        \n",
    "        \n",
    "    train_bucket_sizes = [len(train_set[b]) for b in xrange(len(gen_config.buckets))]\n",
    "    train_total_size = float(sum(train_bucket_sizes))\n",
    "    train_buckets_scale = [sum(train_bucket_sizes[:i + 1]) / train_total_size\n",
    "                           for i in xrange(len(train_bucket_sizes))]\n",
    "    \n",
    "    input_document = []\n",
    "    target_summary = []\n",
    "    generated_summary = []\n",
    "    with tf.Session() as sess:\n",
    "        # Create model and load parameters.\n",
    "        model = create_model(sess, gen_config, forward_only=True, name_scope=gen_config.name_model)\n",
    "\n",
    "        num_step = 0\n",
    "        while num_step < decode_num_step:\n",
    "            print(\"generating num_step: \", num_step)\n",
    "            random_number_01 = np.random.random_sample()\n",
    "            bucket_id = min([i for i in xrange(len(train_buckets_scale))\n",
    "                             if train_buckets_scale[i] > random_number_01])\n",
    "            # Get a 1-element batch to feed the sentence to the model.\n",
    "            encoder_inputs, decoder_inputs, target_weights, _encoder_inputs, _decoder_inputs = model.get_batch(\n",
    "                  train_set, bucket_id)#get_batch(train_set,bucket_id)\n",
    "            # Get output logits for the sentence.\n",
    "            _, _, output_logits = model.step(sess, encoder_inputs, decoder_inputs,target_weights, bucket_id, True)\n",
    "            # This is a greedy decoder - outputs are just argmaxes of output_logits.\n",
    "            outputs = [int(np.argmax(logit, axis=1)) for logit in output_logits]\n",
    "            #print('batch_encoder_input')\n",
    "            #print(\" \".join([str(rev_vocab[an]) for an in encoder_inputs]))\n",
    "            # If there is an EOS symbol in outputs, cut them at that point.\n",
    "            if EOS_ID in outputs:\n",
    "                outputs = outputs[:outputs.index(EOS_ID)]\n",
    "            # Print out French sentence corresponding to outputs.\n",
    "            summary_g = \" \".join([tf.compat.as_str(rev_vocab[output]) for output in outputs])\n",
    "            print(summary_g)\n",
    "            generated_summary.append(summary_g)\n",
    "            \n",
    "            for query, answer,outputs in zip(_encoder_inputs, _decoder_inputs,outputs):\n",
    "\n",
    "                answer_str = \" \".join([str(rev_vocab[an]) for an in answer])\n",
    "                answer_str = answer_str.replace('_GO ','')\n",
    "                answer_str = answer_str.replace(' _EOS','')\n",
    "                answer_str = answer_str.replace(' _PAD','')\n",
    "                #print(answer_str)\n",
    "                target_summary.append(answer_str)\n",
    "                \n",
    "                query_str = \" \".join([str(rev_vocab[qu]) for qu in query])\n",
    "                query_str = query_str.replace(' _PAD','')\n",
    "                #print(query_str)\n",
    "                document_str = query_str.split(' ')\n",
    "                i = len(document_str)-1\n",
    "                d_str = ''\n",
    "                while i>0:\n",
    "                    d_str = d_str + document_str[i]\n",
    "                    d_str = d_str + ' '\n",
    "                    i = i-1\n",
    "                #print(d_str)\n",
    "                input_document.append(d_str)\n",
    "            num_step +=1\n",
    "    return input_document, target_summary, generated_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generated_data_for_D(Flag,decode_num_step):\n",
    "    list1,list2,list3 = decode(decode_num_step)\n",
    "    with open('generated_sample.txt','w')as f:\n",
    "        for i in range(0,len(list3)):\n",
    "            f.write(list3[i])\n",
    "            f.write('\\n')\n",
    "    with open('real_sample.txt','w')as f:\n",
    "        for i in range(0,len(list3)):\n",
    "            f.write(list2[i])\n",
    "            f.write('\\n')\n",
    "    with open('document_sample.txt','w')as f:\n",
    "        for i in range(0,len(list1)):\n",
    "            f.write(list1[i])\n",
    "            f.write('\\n')\n",
    "    if Flag == True:\n",
    "        with open('generated_data_for_D.csv', 'w') as datacsv:\n",
    "            writer = csv.writer(datacsv,dialect=(\"excel\"))\n",
    "            writer.writerow(['generated_id','document_content','target_summary','generated_summary'])\n",
    "            for i in range(0,len(list1)):\n",
    "                writer.writerow([i,list1[i],list2[i],list3[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('generated_data_for_D.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>generated_id</th>\n",
       "      <th>document_content</th>\n",
       "      <th>target_summary</th>\n",
       "      <th>generated_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>tottenham hotspur defens midfield eric dier ha...</td>\n",
       "      <td>dier agre new tottenham contract</td>\n",
       "      <td>kevin brom sign tottenham to retir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>claudia magnussen jacob _UNK pana high school ...</td>\n",
       "      <td>magnussen _UNK lead phs cross countri reviv</td>\n",
       "      <td>_UNK _UNK school school</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>buck counti audubon societi bcas will open the...</td>\n",
       "      <td>reconnect with natur seri featur john c mertz ...</td>\n",
       "      <td>_UNK the librari</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>montreal canada septemb 1 st 2015 – _UNK inc a...</td>\n",
       "      <td>_UNK announc correl 3 d version 62</td>\n",
       "      <td>_UNK announc new websit for the _UNK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>the likelihood of australia two test seri agai...</td>\n",
       "      <td>australian cricket tour of bangladesh doubt af...</td>\n",
       "      <td>australia defend tough test of australia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>thousand will gather in sydney today to honor ...</td>\n",
       "      <td>thousand expect at state funer of bart cum today</td>\n",
       "      <td>cum to rememb in the cum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>new york n y – depom inc on monday reject hori...</td>\n",
       "      <td>depom reject horizon pharma latest 24 billion ...</td>\n",
       "      <td>mylan announc sharehold of sharehold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>merced formula one driver lewi hamilton of bri...</td>\n",
       "      <td>monza grid hit by plethora of penalti</td>\n",
       "      <td>suzuka suzuka suzuka suzuka suzuka suzuka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>this is a transcript of the freakonom radio po...</td>\n",
       "      <td>freakonom » the presid of harvard will see you...</td>\n",
       "      <td>_UNK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>steward – a second person has been charg in co...</td>\n",
       "      <td>second arrest made in steward pot grow</td>\n",
       "      <td>man accus of drug charg in home</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   generated_id                                   document_content  \\\n",
       "0             0  tottenham hotspur defens midfield eric dier ha...   \n",
       "1             1  claudia magnussen jacob _UNK pana high school ...   \n",
       "2             2  buck counti audubon societi bcas will open the...   \n",
       "3             3  montreal canada septemb 1 st 2015 – _UNK inc a...   \n",
       "4             4  the likelihood of australia two test seri agai...   \n",
       "5             5  thousand will gather in sydney today to honor ...   \n",
       "6             6  new york n y – depom inc on monday reject hori...   \n",
       "7             7  merced formula one driver lewi hamilton of bri...   \n",
       "8             8  this is a transcript of the freakonom radio po...   \n",
       "9             9  steward – a second person has been charg in co...   \n",
       "\n",
       "                                      target_summary  \\\n",
       "0                  dier agre new tottenham contract    \n",
       "1       magnussen _UNK lead phs cross countri reviv    \n",
       "2  reconnect with natur seri featur john c mertz ...   \n",
       "3                _UNK announc correl 3 d version 62    \n",
       "4  australian cricket tour of bangladesh doubt af...   \n",
       "5  thousand expect at state funer of bart cum today    \n",
       "6  depom reject horizon pharma latest 24 billion ...   \n",
       "7             monza grid hit by plethora of penalti    \n",
       "8  freakonom » the presid of harvard will see you...   \n",
       "9            second arrest made in steward pot grow    \n",
       "\n",
       "                            generated_summary  \n",
       "0         kevin brom sign tottenham to retir   \n",
       "1                    _UNK _UNK school school   \n",
       "2                           _UNK the librari   \n",
       "3       _UNK announc new websit for the _UNK   \n",
       "4   australia defend tough test of australia   \n",
       "5                   cum to rememb in the cum   \n",
       "6       mylan announc sharehold of sharehold   \n",
       "7  suzuka suzuka suzuka suzuka suzuka suzuka   \n",
       "8                                       _UNK   \n",
       "9            man accus of drug charg in home   "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 写入real_sample.txt 和 generated_sample.txt 中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# experiment CNN 输入：\n",
    "# - document + summary 判断\n",
    "# - summary 判断。\n",
    "# 看哪个结果好。\n",
    "# 先试summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  reading train data line 0\n",
      "  reading train data line 200000\n",
      "  reading train data line 400000\n",
      "  reading train data line 600000\n",
      "  reading train data line 800000\n",
      "single_cell\n",
      "foward_only is true\n",
      "Reading Gen model parameters from /Users/zhangyiman/final project/ Seq2Seq_GAN_Try_Again/t_gen_data/checkpoints/chitchat.model-22200\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Key st_model/reward_bias not found in checkpoint\n\t [[Node: st_model/save/RestoreV2_46 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_st_model/save/Const_0, st_model/save/RestoreV2_46/tensor_names, st_model/save/RestoreV2_46/shape_and_slices)]]\n\nCaused by op 'st_model/save/RestoreV2_46', defined at:\n  File \"//anaconda/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"//anaconda/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"//anaconda/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"//anaconda/lib/python3.5/site-packages/traitlets/config/application.py\", line 596, in launch_instance\n    app.start()\n  File \"//anaconda/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"//anaconda/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"//anaconda/lib/python3.5/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"//anaconda/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"//anaconda/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"//anaconda/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"//anaconda/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"//anaconda/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"//anaconda/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"//anaconda/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"//anaconda/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"//anaconda/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"//anaconda/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"//anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"//anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2827, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"//anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-25-e18e5dcab31e>\", line 4, in <module>\n    generated_data_for_D(Flag,decode_num_step)\n  File \"<ipython-input-21-51ac2be10727>\", line 2, in generated_data_for_D\n    list1,list2,list3 = decode(decode_num_step)\n  File \"<ipython-input-20-c939ceb71ab5>\", line 17, in decode\n    model = create_model(sess, gen_config, forward_only=True, name_scope=gen_config.name_model)\n  File \"<ipython-input-15-ee39e7399370>\", line 4, in create_model\n    model = Seq2SeqModel(gen_config, name_scope=name_scope, forward_only=forward_only)\n  File \"<ipython-input-12-9327f3c5f18a>\", line 147, in __init__\n    self.saver = tf.train.Saver(tf.global_variables())\n  File \"//anaconda/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 1040, in __init__\n    self.build()\n  File \"//anaconda/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 1070, in build\n    restore_sequentially=self._restore_sequentially)\n  File \"//anaconda/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 675, in build\n    restore_sequentially, reshape)\n  File \"//anaconda/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 402, in _AddRestoreOps\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\n  File \"//anaconda/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 242, in restore_op\n    [spec.tensor.dtype])[0])\n  File \"//anaconda/lib/python3.5/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 668, in restore_v2\n    dtypes=dtypes, name=name)\n  File \"//anaconda/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"//anaconda/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2327, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"//anaconda/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1226, in __init__\n    self._traceback = _extract_stack()\n\nNotFoundError (see above for traceback): Key st_model/reward_bias not found in checkpoint\n\t [[Node: st_model/save/RestoreV2_46 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_st_model/save/Const_0, st_model/save/RestoreV2_46/tensor_names, st_model/save/RestoreV2_46/shape_and_slices)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Key st_model/reward_bias not found in checkpoint\n\t [[Node: st_model/save/RestoreV2_46 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_st_model/save/Const_0, st_model/save/RestoreV2_46/tensor_names, st_model/save/RestoreV2_46/shape_and_slices)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-e18e5dcab31e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#Flag = False\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdecode_num_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mgenerated_data_for_D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFlag\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdecode_num_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-21-51ac2be10727>\u001b[0m in \u001b[0;36mgenerated_data_for_D\u001b[0;34m(Flag, decode_num_step)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgenerated_data_for_D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFlag\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdecode_num_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mlist1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlist2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlist3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecode_num_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'generated_sample.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist3\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-c939ceb71ab5>\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(decode_num_step)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# Create model and load parameters.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname_scope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgen_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mnum_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-ee39e7399370>\u001b[0m in \u001b[0;36mcreate_model\u001b[0;34m(session, gen_config, forward_only, name_scope, initializer)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mckpt\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Reading Gen model parameters from %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mckpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mckpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_checkpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0;31m#return model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1426\u001b[0m       \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1427\u001b[0m     sess.run(self.saver_def.restore_op_name,\n\u001b[0;32m-> 1428\u001b[0;31m              {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[1;32m   1429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1430\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m//anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1035\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1036\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Key st_model/reward_bias not found in checkpoint\n\t [[Node: st_model/save/RestoreV2_46 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_st_model/save/Const_0, st_model/save/RestoreV2_46/tensor_names, st_model/save/RestoreV2_46/shape_and_slices)]]\n\nCaused by op 'st_model/save/RestoreV2_46', defined at:\n  File \"//anaconda/lib/python3.5/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"//anaconda/lib/python3.5/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"//anaconda/lib/python3.5/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"//anaconda/lib/python3.5/site-packages/traitlets/config/application.py\", line 596, in launch_instance\n    app.start()\n  File \"//anaconda/lib/python3.5/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"//anaconda/lib/python3.5/site-packages/zmq/eventloop/ioloop.py\", line 162, in start\n    super(ZMQIOLoop, self).start()\n  File \"//anaconda/lib/python3.5/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"//anaconda/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"//anaconda/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"//anaconda/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"//anaconda/lib/python3.5/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"//anaconda/lib/python3.5/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"//anaconda/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"//anaconda/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"//anaconda/lib/python3.5/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"//anaconda/lib/python3.5/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"//anaconda/lib/python3.5/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"//anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"//anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2827, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"//anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-25-e18e5dcab31e>\", line 4, in <module>\n    generated_data_for_D(Flag,decode_num_step)\n  File \"<ipython-input-21-51ac2be10727>\", line 2, in generated_data_for_D\n    list1,list2,list3 = decode(decode_num_step)\n  File \"<ipython-input-20-c939ceb71ab5>\", line 17, in decode\n    model = create_model(sess, gen_config, forward_only=True, name_scope=gen_config.name_model)\n  File \"<ipython-input-15-ee39e7399370>\", line 4, in create_model\n    model = Seq2SeqModel(gen_config, name_scope=name_scope, forward_only=forward_only)\n  File \"<ipython-input-12-9327f3c5f18a>\", line 147, in __init__\n    self.saver = tf.train.Saver(tf.global_variables())\n  File \"//anaconda/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 1040, in __init__\n    self.build()\n  File \"//anaconda/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 1070, in build\n    restore_sequentially=self._restore_sequentially)\n  File \"//anaconda/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 675, in build\n    restore_sequentially, reshape)\n  File \"//anaconda/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 402, in _AddRestoreOps\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\n  File \"//anaconda/lib/python3.5/site-packages/tensorflow/python/training/saver.py\", line 242, in restore_op\n    [spec.tensor.dtype])[0])\n  File \"//anaconda/lib/python3.5/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 668, in restore_v2\n    dtypes=dtypes, name=name)\n  File \"//anaconda/lib/python3.5/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"//anaconda/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 2327, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"//anaconda/lib/python3.5/site-packages/tensorflow/python/framework/ops.py\", line 1226, in __init__\n    self._traceback = _extract_stack()\n\nNotFoundError (see above for traceback): Key st_model/reward_bias not found in checkpoint\n\t [[Node: st_model/save/RestoreV2_46 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_st_model/save/Const_0, st_model/save/RestoreV2_46/tensor_names, st_model/save/RestoreV2_46/shape_and_slices)]]\n"
     ]
    }
   ],
   "source": [
    "Flag = True # Flag = true: write a csv file Flag = False, don't write\n",
    "#Flag = False\n",
    "decode_num_step = 50000\n",
    "generated_data_for_D(Flag,decode_num_step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# pre-train discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocabulary = json.load(open('newdataset_vocabulary.json','r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dis_dataloader():\n",
    "    def __init__(self, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        self.sentences = np.array([])\n",
    "        self.labels = np.array([])\n",
    "\n",
    "    def load_train_data(self,positive_file, negative_file):\n",
    "        # Load data\n",
    "        positive_examples = []\n",
    "        negative_examples = []\n",
    "        \n",
    "        positive_examples_word = []\n",
    "        positive_examples = list(codecs.open(positive_file, \"r\", \"utf-8\").readlines())\n",
    "        for s in positive_examples:\n",
    "            #p_w = re.split(r'(;|,|\\s)\\s*', s)\n",
    "            p_w = s.split()\n",
    "            positive_examples_word.append(p_w)\n",
    "                \n",
    "        negative_examples_word = []\n",
    "        negative_examples = list(codecs.open(negative_file, \"r\", \"utf-8\").readlines())\n",
    "        for s in negative_examples:\n",
    "            #n_w = re.split(r'(;|,|\\s)\\s*', s)\n",
    "            #n_w = n_w.remove(' ')\n",
    "            n_w = s.split()\n",
    "            negative_examples_word.append(n_w)        \n",
    "        \n",
    "        \n",
    "        # use part of data to test the model\n",
    "        positive_examples_word = positive_examples_word[:5000]\n",
    "        negative_examples_word = negative_examples_word[:5000]\n",
    "        \n",
    "        #Pads all sentences to the same length.#这里需要改为 全都变为一个长度 比如400之类的 然后把dis传入的参数\n",
    "        #里的 senquence length改为一样的。\n",
    "        padding_word=\"<PAD/>\"\n",
    "\n",
    "        positive_length = max(len(x) for x in positive_examples_word)\n",
    "        negative_length = max(len(x) for x in negative_examples_word)\n",
    "        if positive_length > negative_length:\n",
    "            sequence_length = positive_length\n",
    "        else:\n",
    "            sequence_length = negative_length\n",
    "            \n",
    "        padded_positive_examples = []\n",
    "        for i in range(len(positive_examples_word)):\n",
    "            sentence = positive_examples_word[i]\n",
    "            num_padding = sequence_length - len(sentence)\n",
    "            new_sentence = sentence + [padding_word] * num_padding\n",
    "            padded_positive_examples.append(new_sentence)\n",
    "        \n",
    "        padded_negative_examples = []\n",
    "        for i in range(len(negative_examples_word)):\n",
    "            sentence = negative_examples_word[i]\n",
    "            num_padding = sequence_length - len(sentence)\n",
    "            new_sentence = sentence + [padding_word] * num_padding\n",
    "            padded_negative_examples.append(new_sentence)\n",
    "        \n",
    "        self.sentences = padded_positive_examples + padded_negative_examples\n",
    "        \n",
    "        # Generate labels\n",
    "        positive_labels = [[0, 1] for _ in positive_examples_word]\n",
    "        negative_labels = [[1, 0] for _ in negative_examples_word]\n",
    "        #############################################################################\n",
    "        self.labels = np.concatenate([positive_labels, negative_labels], 0)\n",
    "\n",
    "        a = self.sentences\n",
    "        b = self.labels\n",
    "        \n",
    "        a,b = shuffle(a,b)\n",
    "        \n",
    "        self.sentences = a\n",
    "        self.lables = b\n",
    "        \n",
    "        #############################################################################\n",
    "        sentences = self.sentences\n",
    "        for sentence in sentences:\n",
    "            for i in range(0,len(sentence)):\n",
    "                sentence[i] = sentence[i].replace('.','')\n",
    "                sentence[i] = sentence[i].replace(',','')\n",
    "                sentence[i] = sentence[i].replace('?','')\n",
    "                sentence[i] = sentence[i].replace('(','')\n",
    "                sentence[i] = sentence[i].replace(')','')\n",
    "                sentence[i] = sentence[i].replace('!','')\n",
    "                sentence[i] = sentence[i].lower()\n",
    "                \n",
    "        for sentence in sentences:\n",
    "            for i in range(0,len(sentence)):\n",
    "                try: \n",
    "                    sentence[i] = vocabulary[sentence[i]]\n",
    "                except: \n",
    "                    sentence[i] = 436921 # len(vocabulary) + 1\n",
    "        self.sentences = np.array(sentences)\n",
    "        \n",
    "        # Split batches\n",
    "        self.num_batch = int(len(self.labels) / self.batch_size)\n",
    "        self.sentences = self.sentences[:self.num_batch * self.batch_size]\n",
    "        self.labels = self.labels[:self.num_batch * self.batch_size]\n",
    "        self.sentences_batches = np.split(self.sentences, self.num_batch, 0)\n",
    "        self.labels_batches = np.split(self.labels, self.num_batch, 0)\n",
    "\n",
    "        self.pointer = 0\n",
    "\n",
    "\n",
    "    def next_batch(self):\n",
    "        ret = self.sentences_batches[self.pointer], self.labels_batches[self.pointer]\n",
    "        self.pointer = (self.pointer + 1) % self.num_batch\n",
    "        return ret\n",
    "\n",
    "    def reset_pointer(self):\n",
    "        self.pointer = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# An alternative to tf.nn.rnn_cell._linear function, which has been removed in Tensorfow 1.0.1\n",
    "# The highway layer is borrowed from https://github.com/mkroutikov/tf-lstm-char-cnn\n",
    "def dis_linear(input_, output_size, scope=None):\n",
    "    '''\n",
    "    Linear map: output[k] = sum_i(Matrix[k, i] * input_[i] ) + Bias[k]\n",
    "    Args:\n",
    "    input_: a tensor or a list of 2D, batch x n, Tensors.\n",
    "    output_size: int, second dimension of W[i].\n",
    "    scope: VariableScope for the created subgraph; defaults to \"Linear\".\n",
    "  Returns:\n",
    "    A 2D Tensor with shape [batch x output_size] equal to\n",
    "    sum_i(input_[i] * W[i]), where W[i]s are newly created matrices.\n",
    "  Raises:\n",
    "    ValueError: if some of the arguments has unspecified or wrong shape.\n",
    "  '''\n",
    "\n",
    "    shape = input_.get_shape().as_list()\n",
    "    if len(shape) != 2:\n",
    "        raise ValueError(\"Linear is expecting 2D arguments: %s\" % str(shape))\n",
    "    if not shape[1]:\n",
    "        raise ValueError(\"Linear expects shape[1] of arguments: %s\" % str(shape))\n",
    "    input_size = shape[1]\n",
    "\n",
    "    # Now the computation.\n",
    "    with tf.variable_scope(scope or \"SimpleLinear\"):\n",
    "        matrix = tf.get_variable(\"Matrix\", [output_size, input_size], dtype=input_.dtype)\n",
    "        bias_term = tf.get_variable(\"Bias\", [output_size], dtype=input_.dtype)\n",
    "\n",
    "    return tf.matmul(input_, tf.transpose(matrix)) + bias_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def highway(input_, size, num_layers=1, bias=-2.0, f=tf.nn.relu, scope='Highway'):\n",
    "    \"\"\"Highway Network (cf. http://arxiv.org/abs/1505.00387).\n",
    "    t = sigmoid(Wy + b)\n",
    "    z = t * g(Wy + b) + (1 - t) * y\n",
    "    where g is nonlinearity, t is transform gate, and (1 - t) is carry gate.\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.variable_scope(scope):\n",
    "        for idx in range(num_layers):\n",
    "            g = f(dis_linear(input_, size, scope='highway_lin_%d' % idx))\n",
    "\n",
    "            t = tf.sigmoid(dis_linear(input_, size, scope='highway_gate_%d' % idx) + bias)\n",
    "\n",
    "            output = t * g + (1. - t) * input_\n",
    "            input_ = output\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Discriminator(object):\n",
    "    '''A CNN for classification: \n",
    "    Distinguish between the ground truth answer and the generated answer\n",
    "    CNN: embedding layer --> convolutional, max pooling-->softmax '''\n",
    "    \n",
    "    # parameters\n",
    "    def __init__(\n",
    "                self, \n",
    "                sequence_length, \n",
    "                num_classes, \n",
    "                vocab_size,\n",
    "                embedding_size, \n",
    "                filter_sizes, \n",
    "                num_filters, \n",
    "                l2_reg_lambda=0.0):\n",
    "        \n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "        \n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        l2_loss = tf.constant(0.0)\n",
    "        \n",
    "        with tf.variable_scope('discriminator'):\n",
    "            # Embedding layer\n",
    "            with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "                self.W = tf.Variable(\n",
    "                    tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\n",
    "                    name=\"W\")\n",
    "                self.embedded_chars = tf.nn.embedding_lookup(self.W, self.input_x)\n",
    "                self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "\n",
    "\n",
    "            # Create a convolution + maxpool layer for each filter size\n",
    "            pooled_outputs = []\n",
    "            for filter_size, num_filter in zip(filter_sizes, num_filters):\n",
    "                with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                    # Convolution Layer\n",
    "                    filter_shape = [filter_size, embedding_size, 1, num_filter]\n",
    "                    W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                    b = tf.Variable(tf.constant(0.1, shape=[num_filter]), name=\"b\")\n",
    "                    conv = tf.nn.conv2d(\n",
    "                        self.embedded_chars_expanded,\n",
    "                        W,\n",
    "                        strides=[1, 1, 1, 1],\n",
    "                        padding=\"VALID\",\n",
    "                        name=\"conv\")\n",
    "                    # Apply nonlinearity\n",
    "                    h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                    # Maxpooling over the outputs\n",
    "                    pooled = tf.nn.max_pool(\n",
    "                        h,\n",
    "                        ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                        strides=[1, 1, 1, 1],\n",
    "                        padding='VALID',\n",
    "                        name=\"pool\")\n",
    "                    pooled_outputs.append(pooled)\n",
    "            \n",
    "            # Combine all the pooled features\n",
    "            num_filters_total = sum(num_filters)\n",
    "            self.h_pool = tf.concat(pooled_outputs, 3)\n",
    "            self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "\n",
    "            # Add highway\n",
    "            with tf.name_scope(\"highway\"):\n",
    "                self.h_highway = highway(self.h_pool_flat, self.h_pool_flat.get_shape()[1], 1, 0)\n",
    "\n",
    "            # Add dropout\n",
    "            with tf.name_scope(\"dropout\"):\n",
    "                self.h_drop = tf.nn.dropout(self.h_highway, self.dropout_keep_prob)\n",
    "            \n",
    "            \n",
    "            # Final (unnormalized) scores and predictions\n",
    "            with tf.name_scope(\"output\"):\n",
    "                W = tf.Variable(tf.truncated_normal([num_filters_total, num_classes], stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "                l2_loss += tf.nn.l2_loss(W)\n",
    "                l2_loss += tf.nn.l2_loss(b)\n",
    "                self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "                self.ypred_for_auc = tf.nn.softmax(self.scores)\n",
    "                self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "                \n",
    "            # CalculateMean cross-entropy loss\n",
    "            with tf.name_scope(\"loss\"):\n",
    "                losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
    "                self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss                \n",
    "            print(self.loss)\n",
    "            \n",
    "        self.params = [param for param in tf.trainable_variables() if 'discriminator' in param.name]\n",
    "        d_optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "        grads_and_vars = d_optimizer.compute_gradients(self.loss, self.params, aggregation_method=2)\n",
    "        self.train_op = d_optimizer.apply_gradients(grads_and_vars)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  Discriminator  Hyper-parameters\n",
    "#########################################################################################\n",
    "dis_embedding_dim = 64\n",
    "dis_filter_sizes = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20]\n",
    "dis_num_filters = [100, 200, 200, 200, 200, 100, 100, 100, 100, 100, 160, 160]\n",
    "dis_dropout_keep_prob = 0.75\n",
    "dis_l2_reg_lambda = 0.2\n",
    "dis_batch_size = 64\n",
    "\n",
    "vocab_size = 436922\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"discriminator/loss/add:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "discriminator = Discriminator(sequence_length=30, num_classes=2, vocab_size=vocab_size, embedding_size=dis_embedding_dim, \n",
    "                                filter_sizes=dis_filter_sizes, num_filters=dis_num_filters, l2_reg_lambda=dis_l2_reg_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dis_train():\n",
    "    dis_data_loader = Dis_dataloader(BATCH_SIZE)\n",
    "    \n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    \n",
    "    sess = tf.Session(config=config)\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    #ckpt = tf.train.get_checkpoint_state('./model/path')\n",
    "    #if ckpt and ckpt.model_checkpoint_path:\n",
    "    #    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "    #else:\n",
    "    #    print('no checkpoint found, start with fresh parameter')\n",
    "    \n",
    "    # Train 3 epoch on the generated data and do this for 50 times\n",
    "    global_step = 0\n",
    "    for current_step in range(50):\n",
    "    \n",
    "        negative_file = 'generated_sample.txt'\n",
    "        positive_file = 'real_sample.txt'\n",
    "    \n",
    "        dis_data_loader.load_train_data(positive_file, negative_file)\n",
    "    \n",
    "        print(\"global_step: \")\n",
    "        print(global_step)\n",
    "        \n",
    "        for _ in range(3):\n",
    "            print(' epoch: ')\n",
    "            print(_)\n",
    "            dis_data_loader.reset_pointer()\n",
    "            for it in range(dis_data_loader.num_batch):\n",
    "                x_batch, y_batch = dis_data_loader.next_batch()\n",
    "                feed = {\n",
    "                    discriminator.input_x: x_batch,\n",
    "                    discriminator.input_y: y_batch,\n",
    "                    discriminator.dropout_keep_prob: dis_dropout_keep_prob\n",
    "                }\n",
    "                _ = sess.run(discriminator.train_op, feed)\n",
    "                global_step = global_step + 1\n",
    "        #print(\"current_step: %d, save model\" %(current_step))\n",
    "        #checkpoint_path = os.path.join('./model/path')\n",
    "        #discriminator.saver.save(sess, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch: \n",
      "0\n",
      " epoch: \n",
      "1\n",
      " epoch: \n",
      "2\n",
      " epoch: \n",
      "0\n",
      " epoch: \n",
      "1\n",
      " epoch: \n",
      "2\n",
      " epoch: \n",
      "0\n",
      " epoch: \n",
      "1\n",
      " epoch: \n",
      "2\n",
      " epoch: \n",
      "0\n",
      " epoch: \n",
      "1\n",
      " epoch: \n",
      "2\n",
      " epoch: \n",
      "0\n",
      " epoch: \n",
      "1\n",
      " epoch: \n",
      "2\n",
      " epoch: \n",
      "0\n",
      " epoch: \n",
      "1\n",
      " epoch: \n",
      "2\n",
      " epoch: \n",
      "0\n",
      " epoch: \n",
      "1\n",
      " epoch: \n",
      "2\n",
      " epoch: \n",
      "0\n",
      " epoch: \n",
      "1\n",
      " epoch: \n",
      "2\n",
      " epoch: \n",
      "0\n",
      " epoch: \n",
      "1\n",
      " epoch: \n",
      "2\n",
      " epoch: \n",
      "0\n",
      " epoch: \n",
      "1\n",
      " epoch: \n",
      "2\n",
      " epoch: \n",
      "0\n",
      " epoch: \n",
      "1\n",
      " epoch: \n",
      "2\n",
      " epoch: \n",
      "0\n",
      " epoch: \n",
      "1\n",
      " epoch: \n",
      "2\n",
      " epoch: \n",
      "0\n",
      " epoch: \n",
      "1\n",
      " epoch: \n",
      "2\n",
      " epoch: \n",
      "0\n",
      " epoch: \n",
      "1\n",
      " epoch: \n",
      "2\n",
      " epoch: \n",
      "0\n",
      " epoch: \n",
      "1\n",
      " epoch: \n",
      "2\n",
      " epoch: \n",
      "0\n",
      " epoch: \n",
      "1\n",
      " epoch: \n",
      "2\n",
      " epoch: \n",
      "0\n",
      " epoch: \n",
      "1\n",
      " epoch: \n",
      "2\n",
      " epoch: \n",
      "0\n",
      " epoch: \n",
      "1\n",
      " epoch: \n",
      "2\n",
      " epoch: \n",
      "0\n",
      " epoch: \n",
      "1\n",
      " epoch: \n",
      "2\n",
      " epoch: \n",
      "0\n",
      " epoch: \n",
      "1\n",
      " epoch: \n",
      "2\n"
     ]
    }
   ],
   "source": [
    "dis_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    prob = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare disc_data for discriminator and generator\n",
    "def prepare_data(sess, gen_model, vocab, source_inputs, source_outputs,\n",
    "                    encoder_inputs, decoder_inputs, target_weights, bucket_id, mc_search=False):\n",
    "    train_query, train_answer = [], []\n",
    "    query_len = gen_config.buckets[bucket_id][0]\n",
    "    answer_len = gen_config.buckets[bucket_id][1]\n",
    "\n",
    "    for query, answer in zip(source_inputs, source_outputs):\n",
    "        query = query[:query_len] + [int(data_utils.PAD_ID)] * (query_len - len(query) if query_len > len(query) else 0)\n",
    "        train_query.append(query)\n",
    "        answer = answer[:-1] # del tag EOS\n",
    "        answer = answer[:answer_len] + [int(data_utils.PAD_ID)] * (answer_len - len(answer) if answer_len > len(answer) else 0)\n",
    "        train_answer.append(answer)\n",
    "        train_labels = [1 for _ in source_inputs]\n",
    "\n",
    "\n",
    "    def decoder(num_roll):\n",
    "        for _ in xrange(num_roll):\n",
    "            _, _, output_logits = gen_model.step(sess, encoder_inputs, decoder_inputs, target_weights, bucket_id,\n",
    "                                                 forward_only=True, mc_search=mc_search)\n",
    "\n",
    "            seq_tokens = []\n",
    "            resps = []\n",
    "            for seq in output_logits:\n",
    "                row_token = []\n",
    "                for t in seq:\n",
    "                    row_token.append(int(np.argmax(t, axis=0)))\n",
    "                seq_tokens.append(row_token)\n",
    "\n",
    "            seq_tokens_t = []\n",
    "            for col in range(len(seq_tokens[0])):\n",
    "                seq_tokens_t.append([seq_tokens[row][col] for row in range(len(seq_tokens))])\n",
    "\n",
    "            for seq in seq_tokens_t:\n",
    "                if data_utils.EOS_ID in seq:\n",
    "                    resps.append(seq[:seq.index(data_utils.EOS_ID)][:gen_config.buckets[bucket_id][1]])\n",
    "                else:\n",
    "                    resps.append(seq[:gen_config.buckets[bucket_id][1]])\n",
    "\n",
    "            for i, output in enumerate(resps):\n",
    "                output = output[:answer_len] + [data_utils.PAD_ID] * (answer_len - len(output) if answer_len > len(output) else 0)\n",
    "                train_query.append(train_query[i])\n",
    "                train_answer.append(output)\n",
    "                train_labels.append(0)\n",
    "\n",
    "        return train_query, train_answer, train_labels\n",
    "\n",
    "    if mc_search:\n",
    "        train_query, train_answer, train_labels = decoder(gen_config.beam_size)\n",
    "    else:\n",
    "        train_query, train_answer, train_labels = decoder(1)\n",
    "\n",
    "    return train_query, train_answer, train_labels\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# discriminator api\n",
    "def disc_step(sess, bucket_id, disc_model, train_query, train_answer, train_labels, forward_only=False):\n",
    "    feed_dict={}\n",
    "\n",
    "    for i in xrange(len(train_query)):\n",
    "\n",
    "        feed_dict[disc_model.query[i].name] = train_query[i]\n",
    "\n",
    "    for i in xrange(len(train_answer)):\n",
    "        feed_dict[disc_model.answer[i].name] = train_answer[i]\n",
    "\n",
    "    feed_dict[disc_model.target.name]=train_labels\n",
    "\n",
    "    loss = 0.0\n",
    "    if forward_only:\n",
    "        fetches = [disc_model.b_logits[bucket_id]]\n",
    "        logits = sess.run(fetches, feed_dict)\n",
    "        logits = logits[0]\n",
    "    else:\n",
    "        fetches = [disc_model.b_train_op[bucket_id], disc_model.b_loss[bucket_id], disc_model.b_logits[bucket_id]]\n",
    "        train_op, loss, logits = sess.run(fetches,feed_dict)\n",
    "\n",
    "    # softmax operation\n",
    "    logits = np.transpose(softmax(np.transpose(logits)))\n",
    "\n",
    "    reward, gen_num = 0.0, 0\n",
    "    for logit, label in zip(logits, train_labels):\n",
    "        if int(label) == 0:\n",
    "            reward += logit[1]\n",
    "            gen_num += 1\n",
    "    reward = reward / gen_num\n",
    "\n",
    "    return reward, loss\n",
    "\n",
    "\n",
    "# Adversarial Learning for Neural Dialogue Generation\n",
    "def al_train():\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        vocab, rev_vocab, dev_set, train_set = gens.prepare_data(gen_config)\n",
    "        for set in train_set:\n",
    "            print(\"al train len: \", len(set))\n",
    "\n",
    "        train_bucket_sizes = [len(train_set[b]) for b in xrange(len(gen_config.buckets))]\n",
    "        train_total_size = float(sum(train_bucket_sizes))\n",
    "        train_buckets_scale = [sum(train_bucket_sizes[:i + 1]) / train_total_size\n",
    "                               for i in xrange(len(train_bucket_sizes))]\n",
    "\n",
    "        disc_model = h_disc.create_model(sess, disc_config, disc_config.name_model)\n",
    "        gen_model = gens.create_model(sess, gen_config, forward_only=False, name_scope=gen_config.name_model)\n",
    "\n",
    "        current_step = 0\n",
    "        step_time, disc_loss, gen_loss, t_loss, batch_reward = 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "        gen_loss_summary = tf.Summary()\n",
    "        disc_loss_summary = tf.Summary()\n",
    "\n",
    "        gen_writer = tf.summary.FileWriter(gen_config.tensorboard_dir, sess.graph)\n",
    "        disc_writer = tf.summary.FileWriter(disc_config.tensorboard_dir, sess.graph)\n",
    "\n",
    "        while True:\n",
    "            current_step += 1\n",
    "            start_time = time.time()\n",
    "            random_number_01 = np.random.random_sample()\n",
    "            bucket_id = min([i for i in xrange(len(train_buckets_scale))\n",
    "                         if train_buckets_scale[i] > random_number_01])\n",
    "            # disc_config.max_len = gen_config.buckets[bucket_id][0] + gen_config.buckets[bucket_id][1]\n",
    "\n",
    "            print(\"==================Update Discriminator: %d=====================\" % current_step)\n",
    "            # 1.Sample (X,Y) from real disc_data\n",
    "            # print(\"bucket_id: %d\" %bucket_id)\n",
    "            encoder_inputs, decoder_inputs, target_weights, source_inputs, source_outputs = gen_model.get_batch(train_set, bucket_id, gen_config.batch_size)\n",
    "\n",
    "            # 2.Sample (X,Y) and (X, ^Y) through ^Y ~ G(*|X)\n",
    "            train_query, train_answer, train_labels = disc_train_data(sess, gen_model, vocab, source_inputs, source_outputs,\n",
    "                                                        encoder_inputs, decoder_inputs, target_weights, bucket_id, mc_search=False)\n",
    "            print(\"==============================mc_search: False===================================\")\n",
    "            if current_step % 200 == 0:\n",
    "                print(\"train_query: \", len(train_query))\n",
    "                print(\"train_answer: \", len(train_answer))\n",
    "                print(\"train_labels: \", len(train_labels))\n",
    "                for i in xrange(len(train_query)):\n",
    "                    print(\"lable: \", train_labels[i])\n",
    "                    print(\"train_answer_sentence: \", train_answer[i])\n",
    "                    print(\" \".join([tf.compat.as_str(rev_vocab[output]) for output in train_answer[i]]))\n",
    "\n",
    "            train_query = np.transpose(train_query)\n",
    "            train_answer = np.transpose(train_answer)\n",
    "\n",
    "            # 3.Update D using (X, Y ) as positive examples and(X, ^Y) as negative examples\n",
    "            _, disc_step_loss = disc_step(sess, bucket_id, disc_model, train_query, train_answer, train_labels, forward_only=False)\n",
    "            disc_loss += disc_step_loss / disc_config.steps_per_checkpoint\n",
    "\n",
    "            print(\"==================Update Generator: %d=========================\" % current_step)\n",
    "            # 1.Sample (X,Y) from real disc_data\n",
    "            update_gen_data = gen_model.get_batch(train_set, bucket_id, gen_config.batch_size)\n",
    "            encoder, decoder, weights, source_inputs, source_outputs = update_gen_data\n",
    "\n",
    "            # 2.Sample (X,Y) and (X, ^Y) through ^Y ~ G(*|X) with Monte Carlo search\n",
    "            train_query, train_answer, train_labels = disc_train_data(sess, gen_model, vocab, source_inputs, source_outputs,\n",
    "                                                                encoder, decoder, weights, bucket_id, mc_search=True)\n",
    "\n",
    "            print(\"=============================mc_search: True====================================\")\n",
    "            if current_step % 200 == 0:\n",
    "                for i in xrange(len(train_query)):\n",
    "                    print(\"lable: \", train_labels[i])\n",
    "                    print(\" \".join([tf.compat.as_str(rev_vocab[output]) for output in train_answer[i]]))\n",
    "\n",
    "            train_query = np.transpose(train_query)\n",
    "            train_answer = np.transpose(train_answer)\n",
    "\n",
    "            # 3.Compute Reward r for (X, ^Y ) using D.---based on Monte Carlo search\n",
    "            reward, _ = disc_step(sess, bucket_id, disc_model, train_query, train_answer, train_labels, forward_only=True)\n",
    "            batch_reward += reward / gen_config.steps_per_checkpoint\n",
    "            print(\"step_reward: \", reward)\n",
    "\n",
    "            # 4.Update G on (X, ^Y ) using reward r\n",
    "            gan_adjusted_loss, gen_step_loss, _ =gen_model.step(sess, encoder, decoder, weights, bucket_id, forward_only=False,\n",
    "                                           reward=reward, up_reward=True, debug=True)\n",
    "            gen_loss += gen_step_loss / gen_config.steps_per_checkpoint\n",
    "\n",
    "            print(\"gen_step_loss: \", gen_step_loss)\n",
    "            print(\"gen_step_adjusted_loss: \", gan_adjusted_loss)\n",
    "\n",
    "            # 5.Teacher-Forcing: Update G on (X, Y )\n",
    "            t_adjusted_loss, t_step_loss, a = gen_model.step(sess, encoder, decoder, weights, bucket_id, forward_only=False)\n",
    "            t_loss += t_step_loss / gen_config.steps_per_checkpoint\n",
    "           \n",
    "            print(\"t_step_loss: \", t_step_loss)\n",
    "            print(\"t_adjusted_loss\", t_adjusted_loss)           # print(\"normal: \", a)\n",
    "\n",
    "            if current_step % gen_config.steps_per_checkpoint == 0:\n",
    "\n",
    "                step_time += (time.time() - start_time) / gen_config.steps_per_checkpoint\n",
    "\n",
    "                print(\"current_steps: %d, step time: %.4f, disc_loss: %.3f, gen_loss: %.3f, t_loss: %.3f, reward: %.3f\"\n",
    "                      %(current_step, step_time, disc_loss, gen_loss, t_loss, batch_reward))\n",
    "\n",
    "                disc_loss_value = disc_loss_summary.value.add()\n",
    "                disc_loss_value.tag = disc_config.name_loss\n",
    "                disc_loss_value.simple_value = float(disc_loss)\n",
    "                disc_writer.add_summary(disc_loss_summary, int(sess.run(disc_model.global_step)))\n",
    "\n",
    "                gen_global_steps = sess.run(gen_model.global_step)\n",
    "                gen_loss_value = gen_loss_summary.value.add()\n",
    "                gen_loss_value.tag = gen_config.name_loss\n",
    "                gen_loss_value.simple_value = float(gen_loss)\n",
    "                t_loss_value = gen_loss_summary.value.add()\n",
    "                t_loss_value.tag = gen_config.teacher_loss\n",
    "                t_loss_value.simple_value = float(t_loss)\n",
    "                batch_reward_value = gen_loss_summary.value.add()\n",
    "                batch_reward_value.tag = gen_config.reward_name\n",
    "                batch_reward_value.simple_value = float(batch_reward)\n",
    "                gen_writer.add_summary(gen_loss_summary, int(gen_global_steps))\n",
    "\n",
    "                if current_step % (gen_config.steps_per_checkpoint * 2) == 0:\n",
    "                    print(\"current_steps: %d, save disc model\" % current_step)\n",
    "                    disc_ckpt_dir = os.path.abspath(os.path.join(disc_config.train_dir, \"checkpoints\"))\n",
    "                    if not os.path.exists(disc_ckpt_dir):\n",
    "                        os.makedirs(disc_ckpt_dir)\n",
    "                    disc_model_path = os.path.join(disc_ckpt_dir, \"disc.model\")\n",
    "                    disc_model.saver.save(sess, disc_model_path, global_step=disc_model.global_step)\n",
    "\n",
    "                    print(\"current_steps: %d, save gen model\" % current_step)\n",
    "                    gen_ckpt_dir = os.path.abspath(os.path.join(gen_config.train_dir, \"checkpoints\"))\n",
    "                    if not os.path.exists(gen_ckpt_dir):\n",
    "                        os.makedirs(gen_ckpt_dir)\n",
    "                    gen_model_path = os.path.join(gen_ckpt_dir, \"gen.model\")\n",
    "                    gen_model.saver.save(sess, gen_model_path, global_step=gen_model.global_step)\n",
    "\n",
    "                step_time, disc_loss, gen_loss, t_loss, batch_reward = 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "                sys.stdout.flush()\n",
    "\n",
    "\n",
    "def main(_):\n",
    "    # step_1 training gen model\n",
    "    gen_pre_train()\n",
    "\n",
    "    # model test\n",
    "    # gen_test()\n",
    "\n",
    "    # step_2 gen training data for disc\n",
    "    # gen_disc()\n",
    "\n",
    "    # step_3 training disc model\n",
    "    # disc_pre_train()\n",
    "\n",
    "    # step_4 training al model\n",
    "    # al_train()\n",
    "\n",
    "    # model test\n",
    "    # gen_test()\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
