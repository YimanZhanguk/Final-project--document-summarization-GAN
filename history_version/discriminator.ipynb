{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load dataset\n",
    "##### in real model, the dataset for discriminator should be: generated answer: 0, ground truth answer: 1\n",
    "##### here: lexRank selected sentences: 0, ground truth answer: 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "mydata = json.load(open('/Users/zhangyiman/Desktop/nfL6.json','r'))\n",
    "\n",
    "questions = []\n",
    "bestAnswers = []\n",
    "NotBestAnswers = []\n",
    "main_categorys = []\n",
    "q_id = []\n",
    "\n",
    "for q_a in mydata:\n",
    "    questions.append(q_a['question'])\n",
    "    bestAnswers.append(q_a['answer'])\n",
    "    NotBestAnswers.append(q_a['nbestanswers'])\n",
    "    main_categorys.append(q_a['main_category'])\n",
    "    q_id.append(q_a['id'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the U.S Invade Iraq ?\n",
      "A small group of politicians believed strongly that the fact that Saddam Hussien remained in power after the first Gulf War was a signal of weakness to the rest of the world, one that invited attacks and terrorism. Shortly after taking power with George Bush in 2000 and after the attack on 9/11, they were able to use the terrorist attacks to justify war with Iraq on this basis and exaggerated threats of the development of weapons of mass destruction. The military strength of the U.S. and the brutality of Saddam's regime led them to imagine that the military and political victory would be relatively easy.\n"
     ]
    }
   ],
   "source": [
    "#print('question[0]')\n",
    "print(questions[0])\n",
    "#print('ground_truth answer for this question')\n",
    "print(bestAnswers[0])\n",
    "\n",
    "# use question + bestAnswer as the real data to train the discriminator\n",
    "# need to write it into real_QA.txt line by line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sentences candidate\n",
    "\n",
    "query_id = []\n",
    "query = []\n",
    "category = []\n",
    "sentence_id = []\n",
    "sentence = []\n",
    "whether_groundTruth = []\n",
    "for j in range (0,len(NotBestAnswers)):\n",
    "    # for every question, \n",
    "    sentence_candidate = []\n",
    "    Ground_Truth_sentence = []\n",
    "    \n",
    "    # there are len_sen answers for this question\n",
    "    len_sen = len(NotBestAnswers[j])\n",
    "    for a in range(0,len_sen):\n",
    "        str = NotBestAnswers[j][a]\n",
    "        str_split = str.split('. ')\n",
    "        # reduce if sentence is null\n",
    "        for i in range(0,len(str_split)):\n",
    "            if len(str_split[i])>=2:\n",
    "                sentence_candidate.append(str_split[i])\n",
    "                \n",
    "    #Ground Truth sentences\n",
    "    str = bestAnswers[j]\n",
    "    str_split = str.split('. ')\n",
    "    for i in range(0,len(str_split)):\n",
    "        Ground_Truth_sentence.append(str_split[i])\n",
    "        \n",
    "    #Whether ground truth\n",
    "    for i in range(0,len(sentence_candidate)):\n",
    "        if sentence_candidate[i] not in Ground_Truth_sentence:\n",
    "            Ground_Truth = 0\n",
    "        else:\n",
    "            Ground_Truth = 1\n",
    "        query_id.append(j)\n",
    "        query.append(questions[j])\n",
    "        category.append(main_categorys[j])\n",
    "        sentence_id.append(i)\n",
    "        sentence.append(sentence_candidate[i])\n",
    "        whether_groundTruth.append(Ground_Truth)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### use question + bestAnswer as the real data to train the discriminator\n",
    "##### need to write it into real_QA.txt line by line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Call an area apiarist.  They should be able to help you and would most likely remove them at no charge in exchange for the hive.  The bees have value and they now belong to you.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestAnswers[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87362"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function TextIOWrapper.close>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('real_QA.txt','w') as f:\n",
    "    \n",
    "    for i in range(0,70000):\n",
    "        f.write(questions[i])\n",
    "        f.write(' ')\n",
    "        f.write(bestAnswers[i])\n",
    "        f.write('\\n')\n",
    "f.close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data : positive, negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "positive_examples = []\n",
    "negative_examples = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "positive_examples = list(codecs.open(\"real_QA.txt\", \"r\", \"utf-8\").readlines())\n",
    "positive_examples = [s.split() for s in positive_examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "negative_examples = list(codecs.open('generated_QA.txt', \"r\", \"utf-8\").readlines())\n",
    "negative_examples = [s.split() for s in negative_examples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70021"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(positive_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## use part of data to test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "positive_examples = positive_examples[:5000]\n",
    "negative_examples = negative_examples[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Pads all sentences to the same length.\n",
    "padding_word=\"<PAD/>\"\n",
    "\n",
    "positive_length = max(len(x) for x in positive_examples)\n",
    "negative_length = max(len(x) for x in negative_examples)\n",
    "if positive_length > negative_length:\n",
    "    sequence_length = positive_length\n",
    "else:\n",
    "    sequence_length = negative_length\n",
    "            \n",
    "padded_positive_examples = []\n",
    "for i in range(len(positive_examples)):\n",
    "    sentence = positive_examples[i]\n",
    "    num_padding = sequence_length - len(sentence)\n",
    "    new_sentence = sentence + [padding_word] * num_padding\n",
    "    padded_positive_examples.append(new_sentence)\n",
    "        \n",
    "padded_negative_examples = []\n",
    "for i in range(len(negative_examples)):\n",
    "    sentence = negative_examples[i]\n",
    "    num_padding = sequence_length - len(sentence)\n",
    "    new_sentence = sentence + [padding_word] * num_padding\n",
    "    padded_negative_examples.append(new_sentence)\n",
    "        \n",
    "        \n",
    "#build_vocab\n",
    "#Builds a vocabulary mapping from word to index based on the sentences.\n",
    "sentences = padded_positive_examples + padded_negative_examples\n",
    "word_counts = Counter(itertools.chain(*sentences))\n",
    "# Mapping from index to word\n",
    "vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
    "# Mapping from word to index\n",
    "vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "\n",
    "_sentences = np.array([[vocabulary[word] for word in sentence] for sentence in sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58158"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(_sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentences = _sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "positive_labels = [[0, 1] for _ in positive_examples]\n",
    "negative_labels = [[1, 0] for _ in negative_examples]\n",
    "labels = np.concatenate([positive_labels, negative_labels], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = sentences\n",
    "b = labels\n",
    "        \n",
    "a,b = shuffle(a,b)\n",
    "        \n",
    "sentences = a\n",
    "lables = b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### generates_QA.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('sentence_QA.csv', 'w') as datacsv:\n",
    "    writer = csv.writer(datacsv,dialect=(\"excel\"))\n",
    "    writer.writerow(['query_id','query','main_category','sentence_id','sentence','whether_groundTruth'])\n",
    "    for i in range(0,len(query_id)):\n",
    "        writer.writerow([query_id[i],query[i],category[i],sentence_id[i],sentence[i],whether_groundTruth[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence_data = pd.read_csv('sentence_QA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:6: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "//anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:13: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    }
   ],
   "source": [
    "sentence_selected = defaultdict(list)\n",
    "for i in range(0,87362):\n",
    "    if len(sentence_data[sentence_data['query_id']== i]) > 3: \n",
    "        sen_num = random.sample(range(len(sentence_data[sentence_data['query_id']== i])),3)\n",
    "        for j in range(0,3):\n",
    "            sub = sentence_data[sentence_data['query_id']==i][sentence_data['sentence_id'] ==sen_num[j]]\n",
    "            sentence_sel = sub['sentence'].values[0]\n",
    "            sentence_selected[i].append(sentence_sel)\n",
    "    else :\n",
    "        if len(sentence_data[sentence_data['query_id']== i]) >= 1 :\n",
    "            sen_num = random.sample(range(len(sentence_data[sentence_data['query_id']== i])),1)\n",
    "            for j in range(0,1):\n",
    "                sub = sentence_data[sentence_data['query_id']==i][sentence_data['sentence_id'] ==sen_num[j]]\n",
    "                sentence_sel = sub['sentence'].values[0]\n",
    "                sentence_selected[i].append(sentence_sel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'He then collects the honey in the empty bag, pulls the hive into the bag of coals.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_selected[1][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87353"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentence_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "neg_examples = []\n",
    "for i in range(0,len(sentence_selected)):\n",
    "    str_sen = ''\n",
    "    str_neg = ''\n",
    "    for j in range(0,len(sentence_selected[i])):\n",
    "        str_sen = str_sen + sentence_selected[i][j]\n",
    "    str_neg = str_neg + questions[i]\n",
    "    str_neg = str_neg + str_sen\n",
    "    neg_examples.append(str_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"what is the answer to: P=2L + 2w for w ?If you want a formula that will yield W, it's: (P-2L)/2 = WThat depends on what L and P areperimeter of a rectangle with length L and width w is =2L+2w\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_examples[50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87353"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(neg_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write negative examples and positive examples into the txt file\n",
    "In GAN, we need to get the negative examples from the generator. \n",
    "Then use two txt file to train the discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('generated_QA.txt','w') as f:\n",
    "    for i in range(0,len(neg_examples)):\n",
    "        f.write(neg_examples[i])\n",
    "        f.write('\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data for Discriminator\n",
    "#### from positive_file and negative_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "positive_file = 'real_QA.txt'\n",
    "negative_file = 'generated_QA.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Dis_dataloader():\n",
    "    def __init__(self, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        self.sentences = np.array([])\n",
    "        self.labels = np.array([])\n",
    "\n",
    "    def load_train_data(self,positive_file, negative_file):\n",
    "        # Load data\n",
    "        positive_examples = []\n",
    "        negative_examples = []\n",
    "\n",
    "        positive_examples = list(codecs.open(positive_file, \"r\", \"utf-8\").readlines())\n",
    "        positive_examples = [s.split() for s in positive_examples]\n",
    "                \n",
    "        \n",
    "        negative_examples = list(codecs.open(negative_file, \"r\", \"utf-8\").readlines())\n",
    "        negative_examples = [s.split() for s in negative_examples]\n",
    "        \n",
    "        # use part of data to test the model\n",
    "        positive_examples = positive_examples[:5000]\n",
    "        negative_examples = negative_examples[:5000]\n",
    "        \n",
    "        #Pads all sentences to the same length.\n",
    "        padding_word=\"<PAD/>\"\n",
    "\n",
    "        positive_length = max(len(x) for x in positive_examples)\n",
    "        negative_length = max(len(x) for x in negative_examples)\n",
    "        if positive_length > negative_length:\n",
    "            sequence_length = positive_length\n",
    "        else:\n",
    "            sequence_length = negative_length\n",
    "            \n",
    "        padded_positive_examples = []\n",
    "        for i in range(len(positive_examples)):\n",
    "            sentence = positive_examples[i]\n",
    "            num_padding = sequence_length - len(sentence)\n",
    "            new_sentence = sentence + [padding_word] * num_padding\n",
    "            padded_positive_examples.append(new_sentence)\n",
    "        \n",
    "        padded_negative_examples = []\n",
    "        for i in range(len(negative_examples)):\n",
    "            sentence = negative_examples[i]\n",
    "            num_padding = sequence_length - len(sentence)\n",
    "            new_sentence = sentence + [padding_word] * num_padding\n",
    "            padded_negative_examples.append(new_sentence)\n",
    "        \n",
    "        self.sentences = padded_positive_examples + padded_negative_examples\n",
    "        \n",
    "        # Generate labels\n",
    "        positive_labels = [[0, 1] for _ in positive_examples]\n",
    "        negative_labels = [[1, 0] for _ in negative_examples]\n",
    "        #############################################################################\n",
    "        self.labels = np.concatenate([positive_labels, negative_labels], 0)\n",
    "\n",
    "        a = self.sentences\n",
    "        b = self.labels\n",
    "        \n",
    "        a,b = shuffle(a,b)\n",
    "        \n",
    "        self.sentences = a\n",
    "        self.lables = b\n",
    "\n",
    "        \n",
    "        #build_vocab\n",
    "        #Builds a vocabulary mapping from word to index based on the sentences.\n",
    "        sentences = self.sentences\n",
    "        word_counts = Counter(itertools.chain(*sentences))\n",
    "        # Mapping from index to word\n",
    "        vocabulary_inv = [x[0] for x in word_counts.most_common()]\n",
    "        # Mapping from word to index\n",
    "        vocabulary = {x: i for i, x in enumerate(vocabulary_inv)}\n",
    "\n",
    "        #############################################################################\n",
    "        self.sentences = np.array([[vocabulary[word] for word in sentence] for sentence in sentences])\n",
    "        \n",
    "        # Split batches\n",
    "        self.num_batch = int(len(self.labels) / self.batch_size)\n",
    "        self.sentences = self.sentences[:self.num_batch * self.batch_size]\n",
    "        self.labels = self.labels[:self.num_batch * self.batch_size]\n",
    "        self.sentences_batches = np.split(self.sentences, self.num_batch, 0)\n",
    "        self.labels_batches = np.split(self.labels, self.num_batch, 0)\n",
    "\n",
    "        self.pointer = 0\n",
    "\n",
    "\n",
    "    def next_batch(self):\n",
    "        ret = self.sentences_batches[self.pointer], self.labels_batches[self.pointer]\n",
    "        self.pointer = (self.pointer + 1) % self.num_batch\n",
    "        return ret\n",
    "\n",
    "    def reset_pointer(self):\n",
    "        self.pointer = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TOTAL_BATCH = 200\n",
    "#positive_file = 'save/real_data.txt'\n",
    "#negative_file = 'save/generator_sample.txt'\n",
    "#eval_file = 'save/eval_file.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator: CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# An alternative to tf.nn.rnn_cell._linear function, which has been removed in Tensorfow 1.0.1\n",
    "# The highway layer is borrowed from https://github.com/mkroutikov/tf-lstm-char-cnn\n",
    "def linear(input_, output_size, scope=None):\n",
    "    '''\n",
    "    Linear map: output[k] = sum_i(Matrix[k, i] * input_[i] ) + Bias[k]\n",
    "    Args:\n",
    "    input_: a tensor or a list of 2D, batch x n, Tensors.\n",
    "    output_size: int, second dimension of W[i].\n",
    "    scope: VariableScope for the created subgraph; defaults to \"Linear\".\n",
    "  Returns:\n",
    "    A 2D Tensor with shape [batch x output_size] equal to\n",
    "    sum_i(input_[i] * W[i]), where W[i]s are newly created matrices.\n",
    "  Raises:\n",
    "    ValueError: if some of the arguments has unspecified or wrong shape.\n",
    "  '''\n",
    "\n",
    "    shape = input_.get_shape().as_list()\n",
    "    if len(shape) != 2:\n",
    "        raise ValueError(\"Linear is expecting 2D arguments: %s\" % str(shape))\n",
    "    if not shape[1]:\n",
    "        raise ValueError(\"Linear expects shape[1] of arguments: %s\" % str(shape))\n",
    "    input_size = shape[1]\n",
    "\n",
    "    # Now the computation.\n",
    "    with tf.variable_scope(scope or \"SimpleLinear\"):\n",
    "        matrix = tf.get_variable(\"Matrix\", [output_size, input_size], dtype=input_.dtype)\n",
    "        bias_term = tf.get_variable(\"Bias\", [output_size], dtype=input_.dtype)\n",
    "\n",
    "    return tf.matmul(input_, tf.transpose(matrix)) + bias_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def highway(input_, size, num_layers=1, bias=-2.0, f=tf.nn.relu, scope='Highway'):\n",
    "    \"\"\"Highway Network (cf. http://arxiv.org/abs/1505.00387).\n",
    "    t = sigmoid(Wy + b)\n",
    "    z = t * g(Wy + b) + (1 - t) * y\n",
    "    where g is nonlinearity, t is transform gate, and (1 - t) is carry gate.\n",
    "    \"\"\"\n",
    "\n",
    "    with tf.variable_scope(scope):\n",
    "        for idx in range(num_layers):\n",
    "            g = f(linear(input_, size, scope='highway_lin_%d' % idx))\n",
    "\n",
    "            t = tf.sigmoid(linear(input_, size, scope='highway_gate_%d' % idx) + bias)\n",
    "\n",
    "            output = t * g + (1. - t) * input_\n",
    "            input_ = output\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Discriminator(object):\n",
    "    '''A CNN for classification: \n",
    "    Distinguish between the ground truth answer and the generated answer\n",
    "    CNN: embedding layer --> convolutional, max pooling-->softmax '''\n",
    "    \n",
    "    # parameters\n",
    "    def __init__(\n",
    "                self, \n",
    "                sequence_length, \n",
    "                num_classes, \n",
    "                vocab_size,\n",
    "                embedding_size, \n",
    "                filter_sizes, \n",
    "                num_filters, \n",
    "                l2_reg_lambda=0.0):\n",
    "        \n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "        \n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        l2_loss = tf.constant(0.0)\n",
    "        \n",
    "        with tf.variable_scope('discriminator'):\n",
    "            # Embedding layer\n",
    "            with tf.device('/cpu:0'), tf.name_scope(\"embedding\"):\n",
    "                self.W = tf.Variable(\n",
    "                    tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0),\n",
    "                    name=\"W\")\n",
    "                self.embedded_chars = tf.nn.embedding_lookup(self.W, self.input_x)\n",
    "                self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "\n",
    "\n",
    "            # Create a convolution + maxpool layer for each filter size\n",
    "            pooled_outputs = []\n",
    "            for filter_size, num_filter in zip(filter_sizes, num_filters):\n",
    "                with tf.name_scope(\"conv-maxpool-%s\" % filter_size):\n",
    "                    # Convolution Layer\n",
    "                    filter_shape = [filter_size, embedding_size, 1, num_filter]\n",
    "                    W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                    b = tf.Variable(tf.constant(0.1, shape=[num_filter]), name=\"b\")\n",
    "                    conv = tf.nn.conv2d(\n",
    "                        self.embedded_chars_expanded,\n",
    "                        W,\n",
    "                        strides=[1, 1, 1, 1],\n",
    "                        padding=\"VALID\",\n",
    "                        name=\"conv\")\n",
    "                    # Apply nonlinearity\n",
    "                    h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                    # Maxpooling over the outputs\n",
    "                    pooled = tf.nn.max_pool(\n",
    "                        h,\n",
    "                        ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                        strides=[1, 1, 1, 1],\n",
    "                        padding='VALID',\n",
    "                        name=\"pool\")\n",
    "                    pooled_outputs.append(pooled)\n",
    "            \n",
    "            # Combine all the pooled features\n",
    "            num_filters_total = sum(num_filters)\n",
    "            self.h_pool = tf.concat(pooled_outputs, 3)\n",
    "            self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "\n",
    "            # Add highway\n",
    "            with tf.name_scope(\"highway\"):\n",
    "                self.h_highway = highway(self.h_pool_flat, self.h_pool_flat.get_shape()[1], 1, 0)\n",
    "\n",
    "            # Add dropout\n",
    "            with tf.name_scope(\"dropout\"):\n",
    "                self.h_drop = tf.nn.dropout(self.h_highway, self.dropout_keep_prob)\n",
    "            \n",
    "            \n",
    "            # Final (unnormalized) scores and predictions\n",
    "            with tf.name_scope(\"output\"):\n",
    "                W = tf.Variable(tf.truncated_normal([num_filters_total, num_classes], stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "                l2_loss += tf.nn.l2_loss(W)\n",
    "                l2_loss += tf.nn.l2_loss(b)\n",
    "                self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "                self.ypred_for_auc = tf.nn.softmax(self.scores)\n",
    "                self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "                \n",
    "            # CalculateMean cross-entropy loss\n",
    "            with tf.name_scope(\"loss\"):\n",
    "                losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
    "                self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss                \n",
    "                \n",
    "            \n",
    "        self.params = [param for param in tf.trainable_variables() if 'discriminator' in param.name]\n",
    "        d_optimizer = tf.train.AdamOptimizer(1e-4)\n",
    "        grads_and_vars = d_optimizer.compute_gradients(self.loss, self.params, aggregation_method=2)\n",
    "        self.train_op = d_optimizer.apply_gradients(grads_and_vars)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Discriminator  Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  Discriminator  Hyper-parameters\n",
    "#########################################################################################\n",
    "dis_embedding_dim = 64\n",
    "dis_filter_sizes = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 15, 20]\n",
    "dis_num_filters = [100, 200, 200, 200, 200, 100, 100, 100, 100, 100, 160, 160]\n",
    "dis_dropout_keep_prob = 0.75\n",
    "dis_l2_reg_lambda = 0.2\n",
    "dis_batch_size = 64\n",
    "\n",
    "vocab_size = 58158"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "discriminator = Discriminator(sequence_length=217, num_classes=2, vocab_size=vocab_size, embedding_size=dis_embedding_dim, \n",
    "                                filter_sizes=dis_filter_sizes, num_filters=dis_num_filters, l2_reg_lambda=dis_l2_reg_lambda)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the discriminator..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dis_data_loader = Dis_dataloader(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## some parameters must change to make sure the model can run: for example sequence length and vocabulary size.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch: \n",
      "0\n",
      " epoch: \n",
      "1\n",
      " epoch: \n",
      "2\n",
      " epoch: \n",
      "0\n",
      " epoch: \n",
      "1\n",
      " epoch: \n",
      "2\n",
      " epoch: \n",
      "0\n",
      " epoch: \n",
      "1\n",
      " epoch: \n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# Train 3 epoch on the generated data and do this for 50 times\n",
    "for _ in range(3):\n",
    "    \n",
    "    negative_file = 'generated_QA.txt'\n",
    "    positive_file = 'real_QA.txt'\n",
    "    \n",
    "    dis_data_loader.load_train_data(positive_file, negative_file)\n",
    "    \n",
    "    for _ in range(3):\n",
    "        print(' epoch: ')\n",
    "        print(_)\n",
    "        dis_data_loader.reset_pointer()\n",
    "        #for it in xrange(dis_data_loader.num_batch):\n",
    "        for it in range(dis_data_loader.num_batch):\n",
    "            x_batch, y_batch = dis_data_loader.next_batch()\n",
    "            feed = {\n",
    "                discriminator.input_x: x_batch,\n",
    "                discriminator.input_y: y_batch,\n",
    "                discriminator.dropout_keep_prob: dis_dropout_keep_prob\n",
    "            }\n",
    "            _ = sess.run(discriminator.train_op, feed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
