{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import tensor_array_ops, control_flow_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### input: sentences candidate for each question \n",
    "#### --> CNN get deep feature\n",
    "#### --> LSTM selector : classification: contain or not contain\n",
    "#### --> wirte result in read_QA.txt file to train discriminator. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### update for generator: \n",
    "#### the loss of generator: score from discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) pre-train generator (use a small part of dataset) (supervised training) (label --> contain or not contain)                  \n",
    "(generator is a classfier to classify which sentence should be contained in answer)\n",
    "\n",
    "2) use the pre-trained generator to generate the answer for each question.           \n",
    "input: sentences for a question \n",
    "generator: give each sentence a score for contain or not contain.(classification 0 or 1)\n",
    "\n",
    "* the loss of generator. \n",
    "use the pre-trained discriminator to classfy this answer. \n",
    "for each sentence, the reward is score/n.\n",
    "\n",
    "\n",
    "[use the sentences which labeled by generator as 1. write into the generated_QA file and use this file to train discrimnator. ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data for generator\n",
    "for a query --> sentences --> select. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "mydata = json.load(open('/Users/zhangyiman/Desktop/nfL6.json','r'))\n",
    "\n",
    "questions = []\n",
    "bestAnswers = []\n",
    "NotBestAnswers = []\n",
    "main_categorys = []\n",
    "q_id = []\n",
    "\n",
    "for q_a in mydata:\n",
    "    questions.append(q_a['question'])\n",
    "    bestAnswers.append(q_a['answer'])\n",
    "    NotBestAnswers.append(q_a['nbestanswers'])\n",
    "    main_categorys.append(q_a['main_category'])\n",
    "    q_id.append(q_a['id'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(questions[0])\n",
    "#print(NotBestAnswers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sentences candidate\n",
    "\n",
    "query_id = []\n",
    "query = []\n",
    "category = []\n",
    "sentence_id = []\n",
    "sentence = []\n",
    "whether_groundTruth = []\n",
    "for j in range (0,len(NotBestAnswers)):\n",
    "    # for every question, \n",
    "    sentence_candidate = []\n",
    "    Ground_Truth_sentence = []\n",
    "    \n",
    "    # there are len_sen answers for this question\n",
    "    len_sen = len(NotBestAnswers[j])\n",
    "    for a in range(0,len_sen):\n",
    "        str = NotBestAnswers[j][a]\n",
    "        str_split = str.split('. ')\n",
    "        # reduce if sentence is null\n",
    "        for i in range(0,len(str_split)):\n",
    "            if len(str_split[i])>=2:\n",
    "                sentence_candidate.append(str_split[i])\n",
    "                \n",
    "    #Ground Truth sentences\n",
    "    str = bestAnswers[j]\n",
    "    str_split = str.split('. ')\n",
    "    for i in range(0,len(str_split)):\n",
    "        Ground_Truth_sentence.append(str_split[i])\n",
    "        \n",
    "    #Whether ground truth\n",
    "    for i in range(0,len(sentence_candidate)):\n",
    "        if sentence_candidate[i] not in Ground_Truth_sentence:\n",
    "            Ground_Truth = 0\n",
    "        else:\n",
    "            Ground_Truth = 1\n",
    "        query_id.append(j)\n",
    "        query.append(questions[j])\n",
    "        category.append(main_categorys[j])\n",
    "        sentence_id.append(i)\n",
    "        sentence.append(sentence_candidate[i])\n",
    "        whether_groundTruth.append(Ground_Truth)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why did the U.S Invade Iraq ? The military strength of the U.S'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query[2]+' '+sentence[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 一个batch存一个query的句子们\n",
    "#这样就可以每次训练一个batch的，把结果为1的句子们写入generate——QA txt里，供以后来更新D了\n",
    "\n",
    "\n",
    "# for the supervise training part of generator: \n",
    "# the data need to be this form: \n",
    "# sentences: (all sentences in NotBestAnswer[] for one question)\n",
    "# label: (contain or not contain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Gen_Data_loader():\n",
    "    def __init__(self, batch_size):\n",
    "        self.batch_size = batch_size\n",
    "        self.sentences = np.array([])\n",
    "        self.labels = np.array([])\n",
    "\n",
    "    def create_batches(self, data_file):\n",
    "        self.token_stream = []\n",
    "        with open(data_file, 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                line = line.split()\n",
    "                parse_line = [int(x) for x in line]\n",
    "                if len(parse_line) == 20:\n",
    "                    self.token_stream.append(parse_line)\n",
    "\n",
    "        self.num_batch = int(len(self.token_stream) / self.batch_size)\n",
    "        self.token_stream = self.token_stream[:self.num_batch * self.batch_size]\n",
    "        self.sequence_batch = np.split(np.array(self.token_stream), self.num_batch, 0)\n",
    "        self.pointer = 0\n",
    "\n",
    "    def next_batch(self):\n",
    "        ret = self.sequence_batch[self.pointer]\n",
    "        self.pointer = (self.pointer + 1) % self.num_batch\n",
    "        return ret\n",
    "\n",
    "    def reset_pointer(self):\n",
    "        self.pointer = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Generator(object):\n",
    "    def __init__(self,num_emb,batch_size,emb_dim,hidden_dim,\n",
    "                sequence_length,start_sentence,\n",
    "                learning_rate = 0.01, reward_gamma = 0.95):\n",
    "        self.num_emb = num_emb\n",
    "        self.batch_size = batch_size\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.sequence_length = sequence_length\n",
    "        self.learning_rate = tf.Variable(float(learning_rate), trainable=False)\n",
    "        self.reward_gamma = reward_gamma\n",
    "        \n",
    "        ################################################################################\n",
    "        #self.start_token = tf.constant([start_token] * self.batch_size, dtype=tf.int32)\n",
    "        self.start_sentence = \n",
    "        \n",
    "        self.g_params = []\n",
    "        self.d_params = []\n",
    "        self.temperature = 1.0\n",
    "        self.grad_clip = 5.0\n",
    "        \n",
    "        self.expected_reward = tf.Variable(tf.zeros([self.sequence_length]))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Generator(object):\n",
    "    def __init__(self, num_emb, batch_size, emb_dim, hidden_dim,\n",
    "                 sequence_length, start_token,\n",
    "                 learning_rate=0.01, reward_gamma=0.95):\n",
    "        self.num_emb = num_emb\n",
    "        self.batch_size = batch_size\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.sequence_length = sequence_length\n",
    "        self.start_token = tf.constant([start_token] * self.batch_size, dtype=tf.int32)\n",
    "        self.learning_rate = tf.Variable(float(learning_rate), trainable=False)\n",
    "        self.reward_gamma = reward_gamma\n",
    "        self.g_params = []\n",
    "        self.d_params = []\n",
    "        self.temperature = 1.0\n",
    "        self.grad_clip = 5.0\n",
    "\n",
    "        self.expected_reward = tf.Variable(tf.zeros([self.sequence_length]))\n",
    "\n",
    "        with tf.variable_scope('generator'):\n",
    "            self.g_embeddings = tf.Variable(self.init_matrix([self.num_emb, self.emb_dim]))\n",
    "            self.g_params.append(self.g_embeddings)\n",
    "            \n",
    "            self.g_recurrent_unit = self.create_recurrent_unit(self.g_params)  # maps h_tm1 to h_t for generator\n",
    "            self.g_output_unit = self.create_output_unit(self.g_params)  # maps h_t to o_t (output token logits)\n",
    "\n",
    "        # placeholder definition\n",
    "        self.x = tf.placeholder(tf.int32, shape=[self.batch_size, self.sequence_length]) # sequence of tokens generated by generator\n",
    "        self.rewards = tf.placeholder(tf.float32, shape=[self.batch_size, self.sequence_length]) # get from rollout policy and discriminator\n",
    "\n",
    "        # processed for batch\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            self.processed_x = tf.transpose(tf.nn.embedding_lookup(self.g_embeddings, self.x), perm=[1, 0, 2])  # seq_length x batch_size x emb_dim\n",
    "\n",
    "        # Initial states\n",
    "        self.h0 = tf.zeros([self.batch_size, self.hidden_dim])\n",
    "        self.h0 = tf.stack([self.h0, self.h0])\n",
    "\n",
    "        gen_o = tensor_array_ops.TensorArray(dtype=tf.float32, size=self.sequence_length,\n",
    "                                             dynamic_size=False, infer_shape=True)\n",
    "        gen_x = tensor_array_ops.TensorArray(dtype=tf.int32, size=self.sequence_length,\n",
    "                                             dynamic_size=False, infer_shape=True)\n",
    "\n",
    "        def _g_recurrence(i, x_t, h_tm1, gen_o, gen_x):\n",
    "            h_t = self.g_recurrent_unit(x_t, h_tm1)  # hidden_memory_tuple\n",
    "            o_t = self.g_output_unit(h_t)  # batch x vocab , logits not prob\n",
    "            log_prob = tf.log(tf.nn.softmax(o_t))\n",
    "            next_token = tf.cast(tf.reshape(tf.multinomial(log_prob, 1), [self.batch_size]), tf.int32)\n",
    "            x_tp1 = tf.nn.embedding_lookup(self.g_embeddings, next_token)  # batch x emb_dim\n",
    "            gen_o = gen_o.write(i, tf.reduce_sum(tf.multiply(tf.one_hot(next_token, self.num_emb, 1.0, 0.0),\n",
    "                                                             tf.nn.softmax(o_t)), 1))  # [batch_size] , prob\n",
    "            gen_x = gen_x.write(i, next_token)  # indices, batch_size\n",
    "            return i + 1, x_tp1, h_t, gen_o, gen_x\n",
    "\n",
    "        _, _, _, self.gen_o, self.gen_x = control_flow_ops.while_loop(\n",
    "            cond=lambda i, _1, _2, _3, _4: i < self.sequence_length,\n",
    "            body=_g_recurrence,\n",
    "            loop_vars=(tf.constant(0, dtype=tf.int32),\n",
    "                       tf.nn.embedding_lookup(self.g_embeddings, self.start_token), self.h0, gen_o, gen_x))\n",
    "\n",
    "        self.gen_x = self.gen_x.stack()  # seq_length x batch_size\n",
    "        self.gen_x = tf.transpose(self.gen_x, perm=[1, 0])  # batch_size x seq_length\n",
    "\n",
    "        # supervised pretraining for generator\n",
    "        g_predictions = tensor_array_ops.TensorArray(\n",
    "            dtype=tf.float32, size=self.sequence_length,\n",
    "            dynamic_size=False, infer_shape=True)\n",
    "\n",
    "        ta_emb_x = tensor_array_ops.TensorArray(\n",
    "            dtype=tf.float32, size=self.sequence_length)\n",
    "        ta_emb_x = ta_emb_x.unstack(self.processed_x)\n",
    "\n",
    "        def _pretrain_recurrence(i, x_t, h_tm1, g_predictions):\n",
    "            h_t = self.g_recurrent_unit(x_t, h_tm1)\n",
    "            o_t = self.g_output_unit(h_t)\n",
    "            g_predictions = g_predictions.write(i, tf.nn.softmax(o_t))  # batch x vocab_size\n",
    "            x_tp1 = ta_emb_x.read(i)\n",
    "            return i + 1, x_tp1, h_t, g_predictions\n",
    "\n",
    "        _, _, _, self.g_predictions = control_flow_ops.while_loop(\n",
    "            cond=lambda i, _1, _2, _3: i < self.sequence_length,\n",
    "            body=_pretrain_recurrence,\n",
    "            loop_vars=(tf.constant(0, dtype=tf.int32),\n",
    "                       tf.nn.embedding_lookup(self.g_embeddings, self.start_token),\n",
    "                       self.h0, g_predictions))\n",
    "\n",
    "        self.g_predictions = tf.transpose(self.g_predictions.stack(), perm=[1, 0, 2])  # batch_size x seq_length x vocab_size\n",
    "\n",
    "        # pretraining loss\n",
    "        self.pretrain_loss = -tf.reduce_sum(\n",
    "            tf.one_hot(tf.to_int32(tf.reshape(self.x, [-1])), self.num_emb, 1.0, 0.0) * tf.log(\n",
    "                tf.clip_by_value(tf.reshape(self.g_predictions, [-1, self.num_emb]), 1e-20, 1.0)\n",
    "            )\n",
    "        ) / (self.sequence_length * self.batch_size)\n",
    "\n",
    "        # training updates\n",
    "        pretrain_opt = self.g_optimizer(self.learning_rate)\n",
    "\n",
    "        self.pretrain_grad, _ = tf.clip_by_global_norm(tf.gradients(self.pretrain_loss, self.g_params), self.grad_clip)\n",
    "        self.pretrain_updates = pretrain_opt.apply_gradients(zip(self.pretrain_grad, self.g_params))\n",
    "\n",
    "        #######################################################################################################\n",
    "        #  Unsupervised Training\n",
    "        #######################################################################################################\n",
    "        self.g_loss = -tf.reduce_sum(\n",
    "            tf.reduce_sum(\n",
    "                tf.one_hot(tf.to_int32(tf.reshape(self.x, [-1])), self.num_emb, 1.0, 0.0) * tf.log(\n",
    "                    tf.clip_by_value(tf.reshape(self.g_predictions, [-1, self.num_emb]), 1e-20, 1.0)\n",
    "                ), 1) * tf.reshape(self.rewards, [-1])\n",
    "        )\n",
    "\n",
    "        g_opt = self.g_optimizer(self.learning_rate)\n",
    "\n",
    "        self.g_grad, _ = tf.clip_by_global_norm(tf.gradients(self.g_loss, self.g_params), self.grad_clip)\n",
    "        self.g_updates = g_opt.apply_gradients(zip(self.g_grad, self.g_params))\n",
    "\n",
    "    def generate(self, sess):\n",
    "        outputs = sess.run(self.gen_x)\n",
    "        return outputs\n",
    "\n",
    "    def pretrain_step(self, sess, x):\n",
    "        outputs = sess.run([self.pretrain_updates, self.pretrain_loss], feed_dict={self.x: x})\n",
    "        return outputs\n",
    "\n",
    "    def init_matrix(self, shape):\n",
    "        return tf.random_normal(shape, stddev=0.1)\n",
    "\n",
    "    def init_vector(self, shape):\n",
    "        return tf.zeros(shape)\n",
    "\n",
    "    def create_recurrent_unit(self, params):\n",
    "        # Weights and Bias for input and hidden tensor\n",
    "        self.Wi = tf.Variable(self.init_matrix([self.emb_dim, self.hidden_dim]))\n",
    "        self.Ui = tf.Variable(self.init_matrix([self.hidden_dim, self.hidden_dim]))\n",
    "        self.bi = tf.Variable(self.init_matrix([self.hidden_dim]))\n",
    "\n",
    "        self.Wf = tf.Variable(self.init_matrix([self.emb_dim, self.hidden_dim]))\n",
    "        self.Uf = tf.Variable(self.init_matrix([self.hidden_dim, self.hidden_dim]))\n",
    "        self.bf = tf.Variable(self.init_matrix([self.hidden_dim]))\n",
    "\n",
    "        self.Wog = tf.Variable(self.init_matrix([self.emb_dim, self.hidden_dim]))\n",
    "        self.Uog = tf.Variable(self.init_matrix([self.hidden_dim, self.hidden_dim]))\n",
    "        self.bog = tf.Variable(self.init_matrix([self.hidden_dim]))\n",
    "\n",
    "        self.Wc = tf.Variable(self.init_matrix([self.emb_dim, self.hidden_dim]))\n",
    "        self.Uc = tf.Variable(self.init_matrix([self.hidden_dim, self.hidden_dim]))\n",
    "        self.bc = tf.Variable(self.init_matrix([self.hidden_dim]))\n",
    "        params.extend([\n",
    "            self.Wi, self.Ui, self.bi,\n",
    "            self.Wf, self.Uf, self.bf,\n",
    "            self.Wog, self.Uog, self.bog,\n",
    "            self.Wc, self.Uc, self.bc])\n",
    "\n",
    "        def unit(x, hidden_memory_tm1):\n",
    "            previous_hidden_state, c_prev = tf.unstack(hidden_memory_tm1)\n",
    "\n",
    "            # Input Gate\n",
    "            i = tf.sigmoid(\n",
    "                tf.matmul(x, self.Wi) +\n",
    "                tf.matmul(previous_hidden_state, self.Ui) + self.bi\n",
    "            )\n",
    "\n",
    "            # Forget Gate\n",
    "            f = tf.sigmoid(\n",
    "                tf.matmul(x, self.Wf) +\n",
    "                tf.matmul(previous_hidden_state, self.Uf) + self.bf\n",
    "            )\n",
    "\n",
    "            # Output Gate\n",
    "            o = tf.sigmoid(\n",
    "                tf.matmul(x, self.Wog) +\n",
    "                tf.matmul(previous_hidden_state, self.Uog) + self.bog\n",
    "            )\n",
    "\n",
    "            # New Memory Cell\n",
    "            c_ = tf.nn.tanh(\n",
    "                tf.matmul(x, self.Wc) +\n",
    "                tf.matmul(previous_hidden_state, self.Uc) + self.bc\n",
    "            )\n",
    "\n",
    "            # Final Memory cell\n",
    "            c = f * c_prev + i * c_\n",
    "\n",
    "            # Current Hidden state\n",
    "            current_hidden_state = o * tf.nn.tanh(c)\n",
    "\n",
    "            return tf.stack([current_hidden_state, c])\n",
    "\n",
    "        return unit\n",
    "\n",
    "    def create_output_unit(self, params):\n",
    "        self.Wo = tf.Variable(self.init_matrix([self.hidden_dim, self.num_emb]))\n",
    "        self.bo = tf.Variable(self.init_matrix([self.num_emb]))\n",
    "        params.extend([self.Wo, self.bo])\n",
    "\n",
    "        def unit(hidden_memory_tuple):\n",
    "            hidden_state, c_prev = tf.unstack(hidden_memory_tuple)\n",
    "            # hidden_state : batch x hidden_dim\n",
    "            logits = tf.matmul(hidden_state, self.Wo) + self.bo\n",
    "            # output = tf.nn.softmax(logits)\n",
    "            return logits\n",
    "\n",
    "        return unit\n",
    "\n",
    "    def g_optimizer(self, *args, **kwargs):\n",
    "        return tf.train.AdamOptimizer(*args, **kwargs)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
