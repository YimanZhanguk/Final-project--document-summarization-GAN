{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## baseline implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### dataset：\n",
    "https://ciir.cs.umass.edu/downloads/nfL6/\n",
    "\n",
    "todo: use another dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### baselines: \n",
    "1) random choose     \n",
    "2) SVM as classifier      \n",
    "3) learning to rank(LexRank) reference: http://tangra.si.umich.edu/%7Eradev/lexrank/lexrank.pdf             \n",
    "4) Luhn - heurestic method   reference: http://ieeexplore.ieee.org/document/5392672/?arnumber=5392672           \n",
    "\n",
    "todo: \n",
    "CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### evaluation\n",
    "ROUGE-1    \n",
    "ROUGE-2    \n",
    "ROUGE-L     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## dataset_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mydata = json.load(open('/Users/zhangyiman/Desktop/nfL6.json','r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#mydata[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "questions = []\n",
    "bestAnswers = []\n",
    "NotBestAnswers = []\n",
    "main_categorys = []\n",
    "q_id = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for q_a in mydata:\n",
    "    questions.append(q_a['question'])\n",
    "    bestAnswers.append(q_a['answer'])\n",
    "    NotBestAnswers.append(q_a['nbestanswers'])\n",
    "    main_categorys.append(q_a['main_category'])\n",
    "    q_id.append(q_a['id'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Why did the U.S Invade Iraq ?'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A small group of politicians believed strongly that the fact that Saddam Hussien remained in power after the first Gulf War was a signal of weakness to the rest of the world, one that invited attacks and terrorism. Shortly after taking power with George Bush in 2000 and after the attack on 9/11, they were able to use the terrorist attacks to justify war with Iraq on this basis and exaggerated threats of the development of weapons of mass destruction. The military strength of the U.S. and the brutality of Saddam's regime led them to imagine that the military and political victory would be relatively easy.\""
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestAnswers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"A small group of politicians believed strongly that the fact that Saddam Hussien remained in power after the first Gulf War was a signal of weakness to the rest of the world, one that invited attacks and terrorism. Shortly after taking power with George Bush in 2000 and after the attack on 9/11, they were able to use the terrorist attacks to justify war with Iraq on this basis and exaggerated threats of the development of weapons of mass destruction. The military strength of the U.S. and the brutality of Saddam's regime led them to imagine that the military and political victory would be relatively easy.\",\n",
       " 'Because there is a lot of oil in Iraq.',\n",
       " 'It is tempting to say that the US invaded Iraq because it has lots of oil, but the US is not a country in a deep economic problem that capturing other country’s oil is an actual need for survival. It is more likely that the Iraq invading Kuwait scenario would fall under that assumption.. I think that the US government has come to a conclusion that we are on the verge of a war of religions, or more likely ideologies. It would be presumptuous to try and determent a one cause to the coming war. . I think that the world wide spread of the media with its many forms (Cable, Satellite, Internet, etc.)  have pushed the Moslem regimes to the extreme, fearing that secularity and democratic influence is penetrating their country and will result in an up raising against them. One of the best way to maintain the power that you have and even gain more of it, is by hatred. When the common man is occupied hating an outside enemy, its hatred is kept out side the county and would not be directed towards the regime. . So- I believe that the US understands that the fanatic Moslem regimes have already started a war on the democratic world and now is the time to try a fight it.. . So why invade Iraq? Because it is a huge, week Moslem country that thought to be easy to defeat. . This is exactly the same reason why Afghanistan was first and Syria is next in line.',\n",
       " 'I think Yuval is pretty spot on. It\\'s a proving ground and a focal point for terror activity that\\'s not on American soil. And, because no one liked Saddam Hussein, no other countries (even in the Middle East) were about to rise up and join his side.. . Rabid speculation: now the Pentagon has a model that says it takes ~5 years, ~$200B and ~2,000 casualties to \"rebuild\" a dictatorship into a democracy. Who\\'s next on the list?']"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NotBestAnswers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'News & Events'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_categorys[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sentences candidate\n",
    "\n",
    "query_id = []\n",
    "query = []\n",
    "category = []\n",
    "sentence_id = []\n",
    "sentence = []\n",
    "whether_groundTruth = []\n",
    "for j in range (0,len(NotBestAnswers)):\n",
    "    # for every question, \n",
    "    sentence_candidate = []\n",
    "    Ground_Truth_sentence = []\n",
    "    \n",
    "    # there are len_sen answers for this question\n",
    "    len_sen = len(NotBestAnswers[j])\n",
    "    for a in range(0,len_sen):\n",
    "        str = NotBestAnswers[j][a]\n",
    "        str_split = str.split('. ')\n",
    "        # reduce if sentence is null\n",
    "        for i in range(0,len(str_split)):\n",
    "            if len(str_split[i])>=2:\n",
    "                sentence_candidate.append(str_split[i])\n",
    "                \n",
    "    #Ground Truth sentences\n",
    "    str = bestAnswers[j]\n",
    "    str_split = str.split('. ')\n",
    "    for i in range(0,len(str_split)):\n",
    "        Ground_Truth_sentence.append(str_split[i])\n",
    "        \n",
    "    #Whether ground truth\n",
    "    for i in range(0,len(sentence_candidate)):\n",
    "        if sentence_candidate[i] not in Ground_Truth_sentence:\n",
    "            Ground_Truth = 0\n",
    "        else:\n",
    "            Ground_Truth = 1\n",
    "        query_id.append(j)\n",
    "        query.append(questions[j])\n",
    "        category.append(main_categorys[j])\n",
    "        sentence_id.append(i)\n",
    "        sentence.append(sentence_candidate[i])\n",
    "        whether_groundTruth.append(Ground_Truth)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A dataframe for sentence candidate pool: \n",
    "transfer the data format into: \n",
    "A dataFrame:      \n",
    "\n",
    "query, main_category, sentence_id, sentence, whether_groudTruth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('sentence_QA.csv', 'w') as datacsv:\n",
    "    writer = csv.writer(datacsv,dialect=(\"excel\"))\n",
    "    writer.writerow(['query_id','query','main_category','sentence_id','sentence','whether_groundTruth'])\n",
    "    for i in range(0,len(query_id)):\n",
    "        writer.writerow([query_id[i],query[i],category[i],sentence_id[i],sentence[i],whether_groundTruth[i]])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentence_data = pd.read_csv('sentence_QA.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>query</th>\n",
       "      <th>main_category</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>whether_groundTruth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Why did the U.S Invade Iraq ?</td>\n",
       "      <td>News &amp; Events</td>\n",
       "      <td>0</td>\n",
       "      <td>A small group of politicians believed strongly...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Why did the U.S Invade Iraq ?</td>\n",
       "      <td>News &amp; Events</td>\n",
       "      <td>1</td>\n",
       "      <td>Shortly after taking power with George Bush in...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Why did the U.S Invade Iraq ?</td>\n",
       "      <td>News &amp; Events</td>\n",
       "      <td>2</td>\n",
       "      <td>The military strength of the U.S</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Why did the U.S Invade Iraq ?</td>\n",
       "      <td>News &amp; Events</td>\n",
       "      <td>3</td>\n",
       "      <td>and the brutality of Saddam's regime led them ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Why did the U.S Invade Iraq ?</td>\n",
       "      <td>News &amp; Events</td>\n",
       "      <td>4</td>\n",
       "      <td>Because there is a lot of oil in Iraq.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>Why did the U.S Invade Iraq ?</td>\n",
       "      <td>News &amp; Events</td>\n",
       "      <td>5</td>\n",
       "      <td>It is tempting to say that the US invaded Iraq...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>Why did the U.S Invade Iraq ?</td>\n",
       "      <td>News &amp; Events</td>\n",
       "      <td>6</td>\n",
       "      <td>It is more likely that the Iraq invading Kuwai...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>Why did the U.S Invade Iraq ?</td>\n",
       "      <td>News &amp; Events</td>\n",
       "      <td>7</td>\n",
       "      <td>I think that the US government has come to a c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>Why did the U.S Invade Iraq ?</td>\n",
       "      <td>News &amp; Events</td>\n",
       "      <td>8</td>\n",
       "      <td>It would be presumptuous to try and determent ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>Why did the U.S Invade Iraq ?</td>\n",
       "      <td>News &amp; Events</td>\n",
       "      <td>9</td>\n",
       "      <td>I think that the world wide spread of the medi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   query_id                          query  main_category  sentence_id  \\\n",
       "0         0  Why did the U.S Invade Iraq ?  News & Events            0   \n",
       "1         0  Why did the U.S Invade Iraq ?  News & Events            1   \n",
       "2         0  Why did the U.S Invade Iraq ?  News & Events            2   \n",
       "3         0  Why did the U.S Invade Iraq ?  News & Events            3   \n",
       "4         0  Why did the U.S Invade Iraq ?  News & Events            4   \n",
       "5         0  Why did the U.S Invade Iraq ?  News & Events            5   \n",
       "6         0  Why did the U.S Invade Iraq ?  News & Events            6   \n",
       "7         0  Why did the U.S Invade Iraq ?  News & Events            7   \n",
       "8         0  Why did the U.S Invade Iraq ?  News & Events            8   \n",
       "9         0  Why did the U.S Invade Iraq ?  News & Events            9   \n",
       "\n",
       "                                            sentence  whether_groundTruth  \n",
       "0  A small group of politicians believed strongly...                    1  \n",
       "1  Shortly after taking power with George Bush in...                    1  \n",
       "2                   The military strength of the U.S                    1  \n",
       "3  and the brutality of Saddam's regime led them ...                    1  \n",
       "4             Because there is a lot of oil in Iraq.                    0  \n",
       "5  It is tempting to say that the US invaded Iraq...                    0  \n",
       "6  It is more likely that the Iraq invading Kuwai...                    0  \n",
       "7  I think that the US government has come to a c...                    0  \n",
       "8  It would be presumptuous to try and determent ...                    0  \n",
       "9  I think that the world wide spread of the medi...                    0  "
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_data[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>query</th>\n",
       "      <th>main_category</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>whether_groundTruth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1438096</th>\n",
       "      <td>87361</td>\n",
       "      <td>How can I finance investment property?</td>\n",
       "      <td>Business &amp; Finance</td>\n",
       "      <td>43</td>\n",
       "      <td>Ask your loan officer about this</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1438097</th>\n",
       "      <td>87361</td>\n",
       "      <td>How can I finance investment property?</td>\n",
       "      <td>Business &amp; Finance</td>\n",
       "      <td>44</td>\n",
       "      <td>They may also be able to offer you other alter...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1438098</th>\n",
       "      <td>87361</td>\n",
       "      <td>How can I finance investment property?</td>\n",
       "      <td>Business &amp; Finance</td>\n",
       "      <td>45</td>\n",
       "      <td>Good luck! :)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         query_id                                   query       main_category  \\\n",
       "1438096     87361  How can I finance investment property?  Business & Finance   \n",
       "1438097     87361  How can I finance investment property?  Business & Finance   \n",
       "1438098     87361  How can I finance investment property?  Business & Finance   \n",
       "\n",
       "         sentence_id                                           sentence  \\\n",
       "1438096           43                   Ask your loan officer about this   \n",
       "1438097           44  They may also be able to offer you other alter...   \n",
       "1438098           45                                      Good luck! :)   \n",
       "\n",
       "         whether_groundTruth  \n",
       "1438096                    0  \n",
       "1438097                    0  \n",
       "1438098                    0  "
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_data.tail(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Random select sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## random select, random select id of sentences as a subset sentences contained in answer, the length of answer\n",
    "## is set to the average of groud truth answer. \n",
    "## Then use ROUGE to evaluate the performance of random choose sentences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### average ground truth answer length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len_a = 0\n",
    "num = 0\n",
    "for i in range(0,len(bestAnswers)):\n",
    "    str = bestAnswers[i]\n",
    "    str_split = str.split('. ')\n",
    "    len_a = len_a + len(str_split)\n",
    "    num = num + 1\n",
    "avg_len = len_a/i   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0053914218014905"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:6: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "//anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:13: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    }
   ],
   "source": [
    "sentence_selected = defaultdict(list)\n",
    "for i in range(0,87362):\n",
    "    if len(sentence_data[sentence_data['query_id']== i]) > 3: \n",
    "        sen_num = random.sample(range(len(sentence_data[sentence_data['query_id']== i])),3)\n",
    "        for j in range(0,3):\n",
    "            sub = sentence_data[sentence_data['query_id']==i][sentence_data['sentence_id'] ==sen_num[j]]\n",
    "            sentence_sel = sub['sentence'].values[0]\n",
    "            sentence_selected[i].append(sentence_sel)\n",
    "    else :\n",
    "        if len(sentence_data[sentence_data['query_id']== i]) >= 1 :\n",
    "            sen_num = random.sample(range(len(sentence_data[sentence_data['query_id']== i])),1)\n",
    "            for j in range(0,1):\n",
    "                sub = sentence_data[sentence_data['query_id']==i][sentence_data['sentence_id'] ==sen_num[j]]\n",
    "                sentence_sel = sub['sentence'].values[0]\n",
    "                sentence_selected[i].append(sentence_sel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from rouge import Rouge\n",
    "#hypothesis = str\n",
    "#reference = bestAnswers[2]\n",
    "#rouge = Rouge()\n",
    "#scores = rouge.get_scores(reference, hypothesis)\n",
    "#scores\n",
    "#scores[0]\n",
    "#scores[0]['rouge-1']['f']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROUGE evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rouge_1_f_sum = 0\n",
    "rouge_1_p_sum = 0\n",
    "rouge_1_r_sum = 0\n",
    "rouge_2_f_sum = 0\n",
    "rouge_2_p_sum = 0\n",
    "rouge_2_r_sum = 0\n",
    "rouge_l_f_sum = 0\n",
    "rouge_l_p_sum = 0\n",
    "rouge_l_r_sum = 0\n",
    "num = 0\n",
    "for i in range(0,len(sentence_selected)):\n",
    "    str_sen = ''\n",
    "    for j in range(0,len(sentence_selected[i])):\n",
    "        str_sen = str_sen + sentence_selected[i][j]\n",
    "    # str_sen\n",
    "    hypothesis = str_sen\n",
    "    reference = bestAnswers[i]\n",
    "    rouge = Rouge()\n",
    "    scores = rouge.get_scores(reference, hypothesis)\n",
    "    rouge_1_f = scores[0]['rouge-1']['f']\n",
    "    rouge_1_p = scores[0]['rouge-1']['p']\n",
    "    rouge_1_r = scores[0]['rouge-1']['r']\n",
    "    rouge_2_f = scores[0]['rouge-2']['f']\n",
    "    rouge_2_p = scores[0]['rouge-2']['p']\n",
    "    rouge_2_r = scores[0]['rouge-2']['r']\n",
    "    rouge_l_f = scores[0]['rouge-l']['f']\n",
    "    rouge_l_p = scores[0]['rouge-l']['p']\n",
    "    rouge_l_r = scores[0]['rouge-l']['r']\n",
    "    \n",
    "    rouge_1_f_sum = rouge_1_f_sum + rouge_1_f\n",
    "    rouge_1_p_sum = rouge_1_p_sum + rouge_1_p\n",
    "    rouge_1_r_sum = rouge_1_r_sum + rouge_1_r\n",
    "    rouge_2_f_sum = rouge_2_f_sum + rouge_2_f\n",
    "    rouge_2_p_sum = rouge_2_p_sum + rouge_2_p\n",
    "    rouge_2_r_sum = rouge_2_r_sum + rouge_2_r\n",
    "    rouge_l_f_sum = rouge_l_f_sum + rouge_l_f\n",
    "    rouge_l_p_sum = rouge_l_p_sum + rouge_l_p\n",
    "    rouge_l_r_sum = rouge_l_r_sum + rouge_l_r\n",
    "    \n",
    "    num = num + 1\n",
    "\n",
    "rouge_1_F = rouge_1_f_sum/num\n",
    "rouge_1_P = rouge_1_p_sum/num\n",
    "rouge_1_R = rouge_1_r_sum/num\n",
    "rouge_2_F = rouge_2_f_sum/num\n",
    "rouge_2_P = rouge_2_p_sum/num\n",
    "rouge_2_R = rouge_2_r_sum/num\n",
    "rouge_l_F = rouge_l_f_sum/num\n",
    "rouge_l_P = rouge_l_p_sum/num\n",
    "rouge_l_R = rouge_l_r_sum/num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge_1: f: \n",
      "0.3727697344547275\n",
      "rouge_1: p: \n",
      "0.3785854143176447\n",
      "rouge_1: r: \n",
      "0.4102658319113501\n",
      "rouge_2: f: \n",
      "0.27221121931996173\n",
      "rouge_2: p: \n",
      "0.2749610022289372\n",
      "rouge_2: r: \n",
      "0.304958320306709\n",
      "rouge_L: f: \n",
      "0.28202534345593805\n",
      "rouge_L: p: \n",
      "0.31278125318227223\n",
      "rouge_L: r: \n",
      "0.34635741256111297\n"
     ]
    }
   ],
   "source": [
    "print(\"rouge_1: f: \")\n",
    "print(rouge_1_F)\n",
    "print(\"rouge_1: p: \")\n",
    "print(rouge_1_P)\n",
    "print(\"rouge_1: r: \")\n",
    "print(rouge_1_R)\n",
    "\n",
    "print(\"rouge_2: f: \")\n",
    "print(rouge_2_F)\n",
    "print(\"rouge_2: p: \")\n",
    "print(rouge_2_P)\n",
    "print(\"rouge_2: r: \")\n",
    "print(rouge_2_R)\n",
    "      \n",
    "print(\"rouge_L: f: \")\n",
    "print(rouge_l_F)\n",
    "print(\"rouge_L: p: \")\n",
    "print(rouge_l_P)\n",
    "print(\"rouge_L: r: \")\n",
    "print(rouge_l_R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM as a classifier\n",
    "### to classify the important sentences and unimportance sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load word2vec\n",
    "# Sentence vector = word+word+...+word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "#import datetime\n",
    "from sklearn.svm import SVC\n",
    "#import os\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "#cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load word2Vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open('/Users/zhangyiman/Desktop/word2vec_pre/glove.6B.50d.txt')\n",
    "embeddings_index = {}\n",
    "count = 0\n",
    "\n",
    "for line in f:\n",
    "        values_int = line.split('\\t')\n",
    "        #print(values_int[0])\n",
    "        #values = values_int[0].split(' ',1)\n",
    "        values = values_int[0].split(' ')\n",
    "        id = values[0]\n",
    "        #print(id)\n",
    "        #coefs = np.asarray(values[1:])\n",
    "        coefs = values[1:]\n",
    "        #print(coefs)\n",
    "        embeddings_index[id] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sentence representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sentence representation\n",
    "# simply use the average of word vectors as a sentence vector\n",
    "\n",
    "# for some word did not exist in the Word2Vec pre trained dataset, i just ignore them.\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## add spammer in this part \n",
    "def sentence_vector(sentence_str):\n",
    "    str = sentence_str\n",
    "    #str = str.replace(',',' ,')\n",
    "    str = str.replace(\"'s\",'')\n",
    "    str = str.replace(\"determent\",'deter')\n",
    "    str = str.replace(\"200B\",'200 b')\n",
    "    token = re.compile(\"[\\w]+(?=n't)|n't|\\'m|\\'ll|[\\w]+|[.?!;,\\-\\(\\)—\\:']\")\n",
    "    tokens = token.findall(str)\n",
    "    \n",
    "    #str_convert = ''.join(tokens[0])\n",
    "    a = [0]*50\n",
    "    for i in range(0,len(tokens)):\n",
    "            for j in range(0,50):\n",
    "                str_convert = ''.join(tokens[i])\n",
    "                str_lower = str_convert.lower()\n",
    "                try: \n",
    "                    m = float(embeddings_index[str_lower][j])\n",
    "                except:\n",
    "                    m = 0\n",
    "                a[j] = a[j] + m\n",
    "    for i in range(0,50):\n",
    "        a[i] = a[i]/len(tokens)\n",
    "    return(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(0,len(query)):\n",
    "    str = query[i]\n",
    "    query[i] = sentence_vector(str)\n",
    "    str = sentence[i]\n",
    "    sentence[i] = sentence_vector(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.5268430000000004, 1.4918339999999999, 3.135254, 1.0666099999999998, 3.604157, -2.1879049, -0.3421600000000001, 0.22391999999999995, -1.7088212299999996, -1.75622, -0.8935339999999999, 1.8553359999999999, -3.5713099999999995, -1.6307234999999995, 3.5461504999999995, 4.276671, 0.680471, -0.8523640000000001, -1.3991343, -0.42590000000000006, -1.443699, 0.5682040000000002, 3.280732, -0.196004, 1.678147, -15.376930000000003, -5.447674, -0.3224519999999999, 4.207715, -2.92, 24.51156, 1.3777384000000001, -6.00834, -0.6052999999999998, -0.7777028699999999, -2.3512020999999996, 1.620501, -1.8215824999999999, -1.3276729999999999, -1.3349399999999998, -0.562433681, 0.016354000000000035, 0.863548, -0.787318, 0.14728599999999997, 2.6399543999999997, -1.7218171, 0.014062999999999881, -1.2874889999999999, 1.9438699999999998]\n",
      "2.5268430000000004\n"
     ]
    }
   ],
   "source": [
    "print(query[1])\n",
    "print(query[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Query = np.array(query) \n",
    "#Sentence = np.array(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('sentence_QA_vector.csv', 'w') as datacsv:\n",
    "    writer = csv.writer(datacsv,dialect=(\"excel\"))\n",
    "    writer.writerow(['query_id','query_dim0','query_dim1','query_dim2','query_dim3','query_dim4','query_dim5','query_dim6','query_dim7','query_dim8','query_dim9','query_dim10','query_dim11','query_dim12','query_dim13','query_dim14','query_dim15','query_dim16','query_dim17','query_dim18','query_dim19','query_dim20','query_dim21','query_dim22','query_dim23','query_dim24','query_dim25','query_dim26','query_dim27','query_dim28','query_dim29','query_dim30','query_dim31','query_dim32','query_dim33','query_dim34','query_dim35','query_dim36','query_dim37','query_dim38','query_dim39','query_dim40','query_dim41','query_dim42','query_dim43','query_dim44','query_dim45','query_dim46','query_dim47','query_dim48','query_dim49','main_category','sentence_id','sentence_dim0','sentence_dim1','sentence_dim2','sentence_dim3','sentence_dim4','sentence_dim5','sentence_dim6','sentence_dim7','sentence_dim8','sentence_dim9','sentence_dim10','sentence_dim11','sentence_dim12','sentence_dim13','sentence_dim14','sentence_dim15','sentence_dim16','sentence_dim17','sentence_dim18','sentence_dim19','sentence_dim20','sentence_dim21','sentence_dim22','sentence_dim23','sentence_dim24','sentence_dim25','sentence_dim26','sentence_dim27','sentence_dim28','sentence_dim29','sentence_dim30','sentence_dim31','sentence_dim32','sentence_dim33','sentence_dim34','sentence_dim35','sentence_dim36','sentence_dim37','sentence_dim38','sentence_dim39','sentence_dim40','sentence_dim41','sentence_dim42','sentence_dim43','sentence_dim44','sentence_dim45','sentence_dim46','sentence_dim47','sentence_dim48','sentence_dim49','whether_groundTruth'])\n",
    "    for i in range(0,len(query_id)):\n",
    "        writer.writerow([query_id[i],query[i][0],query[i][1],query[i][2],query[i][3],query[i][4],query[i][5],query[i][6],query[i][7],query[i][8],query[i][9],query[i][10],query[i][11],query[i][12],query[i][13],query[i][14],query[i][15],query[i][16],query[i][17],query[i][18],query[i][19],query[i][20],query[i][21],query[i][22],query[i][23],query[i][24],query[i][25],query[i][26],query[i][27],query[i][28],query[i][29],query[i][30],query[i][31],query[i][32],query[i][33],query[i][34],query[i][35],query[i][36],query[i][37],query[i][38],query[i][39],query[i][40],query[i][41],query[i][42],query[i][43],query[i][44],query[i][45],query[i][46],query[i][47],query[i][48],query[i][49],category[i],sentence_id[i],sentence[i][0],sentence[i][1],sentence[i][2],sentence[i][3],sentence[i][4],sentence[i][5],sentence[i][6],sentence[i][7],sentence[i][8],sentence[i][9],sentence[i][10],sentence[i][11],sentence[i][12],sentence[i][13],sentence[i][14],sentence[i][15],sentence[i][16],sentence[i][17],sentence[i][18],sentence[i][19],sentence[i][20],sentence[i][21],sentence[i][22],sentence[i][23],sentence[i][24],sentence[i][25],sentence[i][26],sentence[i][27],sentence[i][28],sentence[i][29],sentence[i][30],sentence[i][31],sentence[i][32],sentence[i][33],sentence[i][34],sentence[i][35],sentence[i][36],sentence[i][37],sentence[i][38],sentence[i][39],sentence[i][40],sentence[i][41],sentence[i][42],sentence[i][43],sentence[i][44],sentence[i][45],sentence[i][46],sentence[i][47],sentence[i][48],sentence[i][49],whether_groundTruth[i]])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>query_dim0</th>\n",
       "      <th>query_dim1</th>\n",
       "      <th>query_dim2</th>\n",
       "      <th>query_dim3</th>\n",
       "      <th>query_dim4</th>\n",
       "      <th>query_dim5</th>\n",
       "      <th>query_dim6</th>\n",
       "      <th>query_dim7</th>\n",
       "      <th>query_dim8</th>\n",
       "      <th>...</th>\n",
       "      <th>sentence_dim41</th>\n",
       "      <th>sentence_dim42</th>\n",
       "      <th>sentence_dim43</th>\n",
       "      <th>sentence_dim44</th>\n",
       "      <th>sentence_dim45</th>\n",
       "      <th>sentence_dim46</th>\n",
       "      <th>sentence_dim47</th>\n",
       "      <th>sentence_dim48</th>\n",
       "      <th>sentence_dim49</th>\n",
       "      <th>whether_groundTruth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2.526843</td>\n",
       "      <td>1.491834</td>\n",
       "      <td>3.135254</td>\n",
       "      <td>1.06661</td>\n",
       "      <td>3.604157</td>\n",
       "      <td>-2.187905</td>\n",
       "      <td>-0.34216</td>\n",
       "      <td>0.22392</td>\n",
       "      <td>-1.708821</td>\n",
       "      <td>...</td>\n",
       "      <td>4.296646</td>\n",
       "      <td>-1.247253</td>\n",
       "      <td>-3.063499</td>\n",
       "      <td>2.485940</td>\n",
       "      <td>5.341059</td>\n",
       "      <td>-15.323900</td>\n",
       "      <td>-0.907039</td>\n",
       "      <td>-5.777957</td>\n",
       "      <td>-12.966610</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2.526843</td>\n",
       "      <td>1.491834</td>\n",
       "      <td>3.135254</td>\n",
       "      <td>1.06661</td>\n",
       "      <td>3.604157</td>\n",
       "      <td>-2.187905</td>\n",
       "      <td>-0.34216</td>\n",
       "      <td>0.22392</td>\n",
       "      <td>-1.708821</td>\n",
       "      <td>...</td>\n",
       "      <td>1.544746</td>\n",
       "      <td>5.874265</td>\n",
       "      <td>-7.135296</td>\n",
       "      <td>4.294561</td>\n",
       "      <td>5.767480</td>\n",
       "      <td>-11.559292</td>\n",
       "      <td>4.163268</td>\n",
       "      <td>-3.945968</td>\n",
       "      <td>-11.521741</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2.526843</td>\n",
       "      <td>1.491834</td>\n",
       "      <td>3.135254</td>\n",
       "      <td>1.06661</td>\n",
       "      <td>3.604157</td>\n",
       "      <td>-2.187905</td>\n",
       "      <td>-0.34216</td>\n",
       "      <td>0.22392</td>\n",
       "      <td>-1.708821</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208306</td>\n",
       "      <td>-0.446918</td>\n",
       "      <td>-1.764933</td>\n",
       "      <td>-1.832892</td>\n",
       "      <td>0.818763</td>\n",
       "      <td>-0.267611</td>\n",
       "      <td>-0.016267</td>\n",
       "      <td>-0.911197</td>\n",
       "      <td>-1.375337</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2.526843</td>\n",
       "      <td>1.491834</td>\n",
       "      <td>3.135254</td>\n",
       "      <td>1.06661</td>\n",
       "      <td>3.604157</td>\n",
       "      <td>-2.187905</td>\n",
       "      <td>-0.34216</td>\n",
       "      <td>0.22392</td>\n",
       "      <td>-1.708821</td>\n",
       "      <td>...</td>\n",
       "      <td>3.613724</td>\n",
       "      <td>1.293993</td>\n",
       "      <td>1.943949</td>\n",
       "      <td>0.361942</td>\n",
       "      <td>1.395195</td>\n",
       "      <td>-7.706854</td>\n",
       "      <td>-0.992035</td>\n",
       "      <td>-3.255798</td>\n",
       "      <td>-5.949074</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2.526843</td>\n",
       "      <td>1.491834</td>\n",
       "      <td>3.135254</td>\n",
       "      <td>1.06661</td>\n",
       "      <td>3.604157</td>\n",
       "      <td>-2.187905</td>\n",
       "      <td>-0.34216</td>\n",
       "      <td>0.22392</td>\n",
       "      <td>-1.708821</td>\n",
       "      <td>...</td>\n",
       "      <td>2.217443</td>\n",
       "      <td>1.150270</td>\n",
       "      <td>-0.494607</td>\n",
       "      <td>-1.077803</td>\n",
       "      <td>2.025698</td>\n",
       "      <td>-3.443631</td>\n",
       "      <td>1.278419</td>\n",
       "      <td>-1.155921</td>\n",
       "      <td>-0.710945</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 104 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   query_id  query_dim0  query_dim1  query_dim2  query_dim3  query_dim4  \\\n",
       "0         0    2.526843    1.491834    3.135254     1.06661    3.604157   \n",
       "1         0    2.526843    1.491834    3.135254     1.06661    3.604157   \n",
       "2         0    2.526843    1.491834    3.135254     1.06661    3.604157   \n",
       "3         0    2.526843    1.491834    3.135254     1.06661    3.604157   \n",
       "4         0    2.526843    1.491834    3.135254     1.06661    3.604157   \n",
       "\n",
       "   query_dim5  query_dim6  query_dim7  query_dim8         ...           \\\n",
       "0   -2.187905    -0.34216     0.22392   -1.708821         ...            \n",
       "1   -2.187905    -0.34216     0.22392   -1.708821         ...            \n",
       "2   -2.187905    -0.34216     0.22392   -1.708821         ...            \n",
       "3   -2.187905    -0.34216     0.22392   -1.708821         ...            \n",
       "4   -2.187905    -0.34216     0.22392   -1.708821         ...            \n",
       "\n",
       "   sentence_dim41  sentence_dim42  sentence_dim43  sentence_dim44  \\\n",
       "0        4.296646       -1.247253       -3.063499        2.485940   \n",
       "1        1.544746        5.874265       -7.135296        4.294561   \n",
       "2       -0.208306       -0.446918       -1.764933       -1.832892   \n",
       "3        3.613724        1.293993        1.943949        0.361942   \n",
       "4        2.217443        1.150270       -0.494607       -1.077803   \n",
       "\n",
       "   sentence_dim45  sentence_dim46  sentence_dim47  sentence_dim48  \\\n",
       "0        5.341059      -15.323900       -0.907039       -5.777957   \n",
       "1        5.767480      -11.559292        4.163268       -3.945968   \n",
       "2        0.818763       -0.267611       -0.016267       -0.911197   \n",
       "3        1.395195       -7.706854       -0.992035       -3.255798   \n",
       "4        2.025698       -3.443631        1.278419       -1.155921   \n",
       "\n",
       "   sentence_dim49  whether_groundTruth  \n",
       "0      -12.966610                    1  \n",
       "1      -11.521741                    1  \n",
       "2       -1.375337                    1  \n",
       "3       -5.949074                    1  \n",
       "4       -0.710945                    0  \n",
       "\n",
       "[5 rows x 104 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "sentence_vector_data = pd.read_csv('sentence_QA_vector.csv')\n",
    "sentence_vector_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sentence_vector_data.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use SVM as a classifer to train\n",
    "x: main category, query_vector, sentence_vector\n",
    "y: whether_groundTruth\n",
    "dataset: 80: train, 20: test\n",
    "for test: use ROUGE to evaluate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### just use a small part of dataset to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "np.random.seed(12)\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "sentence_vector_data[\"main_category\"] = label_encoder.fit_transform(sentence_vector_data[\"main_category\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1438099"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentence_vector_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sentence_vector_data[5900:5950]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use a small part of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_train = 50000\n",
    "num_test = 60000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentence_train = sentence_vector_data.iloc[:num_train]\n",
    "sentence_test = sentence_vector_data.iloc[num_train:num_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train = sentence_train['whether_groundTruth'].values\n",
    "X_train = sentence_train.drop(['query_id','sentence_id','whether_groundTruth'],axis = 1).values\n",
    "X_test = sentence_test.drop(['query_id','sentence_id','whether_groundTruth'],axis = 1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********************training************************\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm = SVC()\n",
    "print('***********************training************************')\n",
    "svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********************predict*************************\n"
     ]
    }
   ],
   "source": [
    "print('***********************predict*************************')\n",
    "prediction = svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rouge evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### a problem in SVM model: the predict result for most sentences are \"not contained in answer\". even there are sometimes for a query, there are no sentences predicted in the answer..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction[170:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rouge_1_F = []\n",
    "rouge_1_P = []\n",
    "rouge_1_R = []\n",
    "rouge_2_F = []\n",
    "rouge_2_P = []\n",
    "rouge_2_R = []\n",
    "rouge_l_F = []\n",
    "rouge_l_P = []\n",
    "rouge_l_R = []\n",
    "\n",
    "train_num = 50000\n",
    "test_num = 60000\n",
    "\n",
    "question_flag = sentence_data['query_id'][train_num]\n",
    "sentence_predicted = ''\n",
    "\n",
    "for i in range(0,len(prediction)):\n",
    "    if prediction[i] == 1:\n",
    "        \n",
    "        sentence_predicted_in_answer = sentence_data['sentence'][train_num+i]\n",
    "        question_id = sentence_data['query_id'][train_num+i]\n",
    "        \n",
    "        if question_id == question_flag:\n",
    "            sentence_predicted = sentence_predicted + sentence_predicted_in_answer\n",
    "        else:\n",
    "            #for this question_flag: \n",
    "            hypothesis = sentence_predicted\n",
    "            reference = bestAnswers[question_flag]\n",
    "            rouge = Rouge()\n",
    "            scores = rouge.get_scores(reference, hypothesis)\n",
    "            rouge_1_f = scores[0]['rouge-1']['f']\n",
    "            rouge_1_p = scores[0]['rouge-1']['p']\n",
    "            rouge_1_r = scores[0]['rouge-1']['r']\n",
    "            rouge_2_f = scores[0]['rouge-2']['f']\n",
    "            rouge_2_p = scores[0]['rouge-2']['p']\n",
    "            rouge_2_r = scores[0]['rouge-2']['r']\n",
    "            rouge_l_f = scores[0]['rouge-l']['f']\n",
    "            rouge_l_p = scores[0]['rouge-l']['p']\n",
    "            rouge_l_r = scores[0]['rouge-l']['r']\n",
    "            \n",
    "            rouge_1_F.append(rouge_1_f)\n",
    "            rouge_1_P.append(rouge_1_p)\n",
    "            rouge_1_R.append(rouge_1_r)\n",
    "            rouge_2_F.append(rouge_2_f)\n",
    "            rouge_2_P.append(rouge_2_p)\n",
    "            rouge_2_R.append(rouge_2_r)\n",
    "            rouge_l_F.append(rouge_l_f)\n",
    "            rouge_l_P.append(rouge_l_p)\n",
    "            rouge_l_R.append(rouge_l_r)\n",
    "            \n",
    "            #update question_flag:\n",
    "            question_flag = sentence_data['query_id'][train_num+i]\n",
    "            #update sentence_predicted:\n",
    "            sentence_predicted = ''\n",
    "            sentence_predicted = sentence_predicted + sentence_predicted_in_answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge_1: f: \n",
      "0.5438155787113726\n",
      "rouge_1: p: \n",
      "0.5180162711084332\n",
      "rouge_1: Rr: \n",
      "0.711356760051562\n",
      "rouge_2: f: \n",
      "0.49016099411627007\n",
      "rouge_2: p: \n",
      "0.46842395444796503\n",
      "rouge_2: Rr: \n",
      "0.6016974835641854\n",
      "rouge_L: f: \n",
      "0.4942657575598574\n",
      "rouge_L: p: \n",
      "0.5004499786698657\n",
      "rouge_L: Rr: \n",
      "0.6995877675898268\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "for i in range(0,len(rouge_1_F)):\n",
    "    sum = sum + rouge_1_F[i]\n",
    "_rouge_1_f = sum/len(rouge_1_F)\n",
    "print(\"rouge_1: f: \")\n",
    "print(_rouge_1_f)\n",
    "sum = 0\n",
    "for i in range(0,len(rouge_1_P)):\n",
    "    sum = sum + rouge_1_P[i]\n",
    "_rouge_1_p = sum/len(rouge_1_P)\n",
    "print(\"rouge_1: p: \")\n",
    "print(_rouge_1_p)\n",
    "sum = 0\n",
    "for i in range(0,len(rouge_1_R)):\n",
    "    sum = sum + rouge_1_R[i]\n",
    "_rouge_1_r = sum/len(rouge_1_R)\n",
    "print(\"rouge_1: Rr: \")\n",
    "print(_rouge_1_r)\n",
    "\n",
    "sum = 0\n",
    "for i in range(0,len(rouge_2_F)):\n",
    "    sum = sum + rouge_2_F[i]\n",
    "_rouge_2_f = sum/len(rouge_2_F)\n",
    "print(\"rouge_2: f: \")\n",
    "print(_rouge_2_f)\n",
    "sum = 0\n",
    "for i in range(0,len(rouge_2_P)):\n",
    "    sum = sum + rouge_2_P[i]\n",
    "_rouge_2_p = sum/len(rouge_2_P)\n",
    "print(\"rouge_2: p: \")\n",
    "print(_rouge_2_p)\n",
    "sum = 0\n",
    "for i in range(0,len(rouge_2_R)):\n",
    "    sum = sum + rouge_2_R[i]\n",
    "_rouge_2_r = sum/len(rouge_2_R)\n",
    "print(\"rouge_2: Rr: \")\n",
    "print(_rouge_2_r)\n",
    "\n",
    "sum = 0\n",
    "for i in range(0,len(rouge_l_F)):\n",
    "    sum = sum + rouge_l_F[i]\n",
    "_rouge_l_f = sum/len(rouge_l_F)\n",
    "print(\"rouge_L: f: \")\n",
    "print(_rouge_l_f)\n",
    "sum = 0\n",
    "for i in range(0,len(rouge_l_P)):\n",
    "    sum = sum + rouge_l_P[i]\n",
    "_rouge_l_p = sum/len(rouge_l_P)\n",
    "print(\"rouge_L: p: \")\n",
    "print(_rouge_l_p)\n",
    "sum = 0\n",
    "for i in range(0,len(rouge_l_R)):\n",
    "    sum = sum + rouge_l_R[i]\n",
    "_rouge_l_r = sum/len(rouge_l_R)\n",
    "print(\"rouge_L: Rr: \")\n",
    "print(_rouge_l_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LexRank\n",
    "used A python package for Document Summarization models:         \n",
    "sumy 0.6.0 from: Michal Belica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "mydata = json.load(open('/Users/zhangyiman/Desktop/nfL6.json','r'))\n",
    "questions = []\n",
    "bestAnswers = []\n",
    "NotBestAnswers = []\n",
    "main_categorys = []\n",
    "q_id = []\n",
    "for q_a in mydata:\n",
    "    questions.append(q_a['question'])\n",
    "    bestAnswers.append(q_a['answer'])\n",
    "    NotBestAnswers.append(q_a['nbestanswers'])\n",
    "    main_categorys.append(q_a['main_category'])\n",
    "    q_id.append(q_a['id'])\n",
    "    \n",
    "# sentences candidate\n",
    "\n",
    "query_id = []\n",
    "query = []\n",
    "category = []\n",
    "sentence_id = []\n",
    "sentence = []\n",
    "whether_groundTruth = []\n",
    "for j in range (0,len(NotBestAnswers)):\n",
    "    # for every question, \n",
    "    sentence_candidate = []\n",
    "    Ground_Truth_sentence = []\n",
    "    \n",
    "    # there are len_sen answers for this question\n",
    "    len_sen = len(NotBestAnswers[j])\n",
    "    for a in range(0,len_sen):\n",
    "        str = NotBestAnswers[j][a]\n",
    "        str_split = str.split('. ')\n",
    "        # reduce if sentence is null\n",
    "        for i in range(0,len(str_split)):\n",
    "            if len(str_split[i])>=2:\n",
    "                sentence_candidate.append(str_split[i])\n",
    "                \n",
    "    #Ground Truth sentences\n",
    "    str = bestAnswers[j]\n",
    "    str_split = str.split('. ')\n",
    "    for i in range(0,len(str_split)):\n",
    "        Ground_Truth_sentence.append(str_split[i])\n",
    "        \n",
    "    #Whether ground truth\n",
    "    for i in range(0,len(sentence_candidate)):\n",
    "        if sentence_candidate[i] not in Ground_Truth_sentence:\n",
    "            Ground_Truth = 0\n",
    "        else:\n",
    "            Ground_Truth = 1\n",
    "        query_id.append(j)\n",
    "        query.append(questions[j])\n",
    "        category.append(main_categorys[j])\n",
    "        sentence_id.append(i)\n",
    "        sentence.append(sentence_candidate[i])\n",
    "        whether_groundTruth.append(Ground_Truth)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "from sumy.parsers.html import HtmlParser\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer as Summarizer\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.utils import get_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LANGUAGE = \"english\"\n",
    "SENTENCES_COUNT = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## For each qustion: add sentences candidates in a txt file. \n",
    "## Then use the lexRank to summarize this document.\n",
    "## calculate the rouge, for each question. and use the average score as ths score of this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use part of dataset to evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1438099"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(query_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10125, 10125, 10125, 10125, 10125, 10125, 10125, 10126, 10126, 10126]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_id[100020:100030]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query_id = query_id[0:100026]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rouge_1_F = []\n",
    "rouge_1_P = []\n",
    "rouge_1_R = []\n",
    "rouge_2_F = []\n",
    "rouge_2_P = []\n",
    "rouge_2_R = []\n",
    "rouge_l_F = []\n",
    "rouge_l_P = []\n",
    "rouge_l_R = []\n",
    "\n",
    "query_id_flag = 0\n",
    "\n",
    "hypothesis = ' '\n",
    "\n",
    "with open('document.txt','w') as f:\n",
    "    f.write(' ')\n",
    "f.close()\n",
    "\n",
    "for i in range(0,len(query_id)):\n",
    "    #for one query\n",
    "    if query_id[i] == query_id_flag:\n",
    "        with open('document.txt','a') as f:\n",
    "            f.write(sentence[i])\n",
    "            f.write('\\n')\n",
    "        f.close()\n",
    "        \n",
    "    else:\n",
    "        #use the document to summarize answer and get rouge score    \n",
    "        parser = PlaintextParser.from_file(\"document.txt\", Tokenizer(LANGUAGE))\n",
    "        stemmer = Stemmer(LANGUAGE)\n",
    "\n",
    "        summarizer = Summarizer(stemmer)\n",
    "        summarizer.stop_words = get_stop_words(LANGUAGE)   \n",
    "      \n",
    "        evaluated_sentences = (summarizer(parser.document, SENTENCES_COUNT))\n",
    "        if len(evaluated_sentences)>0:\n",
    "            for i in range(0,3):\n",
    "                #evaluated_sentences[i].words\n",
    "                str_111 = ' '\n",
    "                for i in range(0,len(evaluated_sentences[0].words)):\n",
    "                    str_111 = str_111 + evaluated_sentences[0].words[i]\n",
    "                    str_111 = str_111 + ' '\n",
    "                hypothesis = hypothesis + str_111\n",
    "\n",
    "            reference = bestAnswers[query_id_flag]\n",
    "            rouge = Rouge()\n",
    "            scores = rouge.get_scores(reference, hypothesis)\n",
    "            rouge_1_f = scores[0]['rouge-1']['f']\n",
    "            rouge_1_p = scores[0]['rouge-1']['p']\n",
    "            rouge_1_r = scores[0]['rouge-1']['r']\n",
    "            rouge_2_f = scores[0]['rouge-2']['f']\n",
    "            rouge_2_p = scores[0]['rouge-2']['p']\n",
    "            rouge_2_r = scores[0]['rouge-2']['r']\n",
    "            rouge_l_f = scores[0]['rouge-l']['f']\n",
    "            rouge_l_p = scores[0]['rouge-l']['p']\n",
    "            rouge_l_r = scores[0]['rouge-l']['r']\n",
    "            \n",
    "            rouge_1_F.append(rouge_1_f)\n",
    "            rouge_1_P.append(rouge_1_p)\n",
    "            rouge_1_R.append(rouge_1_r)\n",
    "            rouge_2_F.append(rouge_2_f)\n",
    "            rouge_2_P.append(rouge_2_p)\n",
    "            rouge_2_R.append(rouge_2_r)\n",
    "            rouge_l_F.append(rouge_l_f)\n",
    "            rouge_l_P.append(rouge_l_p)\n",
    "            rouge_l_R.append(rouge_l_r)\n",
    "\n",
    "        #update query_id_flag\n",
    "        query_id_flag = query_id_flag + 1\n",
    "    \n",
    "        #update document.txt\n",
    "        with open('document.txt','w') as f:\n",
    "            f.write(' ')\n",
    "        f.close()\n",
    "        \n",
    "        #update hypothesis\n",
    "        hypothesis = ' '\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge_1: f: \n",
      "0.5026268618931697\n",
      "rouge_1: p: \n",
      "0.4443284853503644\n",
      "rouge_1: Rr: \n",
      "0.6496826927942\n",
      "rouge_2: f: \n",
      "0.38077093884293656\n",
      "rouge_2: p: \n",
      "0.34336433758611445\n",
      "rouge_2: Rr: \n",
      "0.4736010587935103\n",
      "rouge_L: f: \n",
      "0.2471571755187573\n",
      "rouge_L: p: \n",
      "0.4722198284253882\n",
      "rouge_L: Rr: \n",
      "0.237431990835491\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "for i in range(0,len(rouge_1_F)):\n",
    "    sum = sum + rouge_1_F[i]\n",
    "_rouge_1_f = sum/len(rouge_1_F)\n",
    "print(\"rouge_1: f: \")\n",
    "print(_rouge_1_f)\n",
    "sum = 0\n",
    "for i in range(0,len(rouge_1_P)):\n",
    "    sum = sum + rouge_1_P[i]\n",
    "_rouge_1_p = sum/len(rouge_1_P)\n",
    "print(\"rouge_1: p: \")\n",
    "print(_rouge_1_p)\n",
    "sum = 0\n",
    "for i in range(0,len(rouge_1_R)):\n",
    "    sum = sum + rouge_1_R[i]\n",
    "_rouge_1_r = sum/len(rouge_1_R)\n",
    "print(\"rouge_1: Rr: \")\n",
    "print(_rouge_1_r)\n",
    "\n",
    "sum = 0\n",
    "for i in range(0,len(rouge_2_F)):\n",
    "    sum = sum + rouge_2_F[i]\n",
    "_rouge_2_f = sum/len(rouge_2_F)\n",
    "print(\"rouge_2: f: \")\n",
    "print(_rouge_2_f)\n",
    "sum = 0\n",
    "for i in range(0,len(rouge_2_P)):\n",
    "    sum = sum + rouge_2_P[i]\n",
    "_rouge_2_p = sum/len(rouge_2_P)\n",
    "print(\"rouge_2: p: \")\n",
    "print(_rouge_2_p)\n",
    "sum = 0\n",
    "for i in range(0,len(rouge_2_R)):\n",
    "    sum = sum + rouge_2_R[i]\n",
    "_rouge_2_r = sum/len(rouge_2_R)\n",
    "print(\"rouge_2: Rr: \")\n",
    "print(_rouge_2_r)\n",
    "\n",
    "sum = 0\n",
    "for i in range(0,len(rouge_l_F)):\n",
    "    sum = sum + rouge_l_F[i]\n",
    "_rouge_l_f = sum/len(rouge_l_F)\n",
    "print(\"rouge_L: f: \")\n",
    "print(_rouge_l_f)\n",
    "sum = 0\n",
    "for i in range(0,len(rouge_l_P)):\n",
    "    sum = sum + rouge_l_P[i]\n",
    "_rouge_l_p = sum/len(rouge_l_P)\n",
    "print(\"rouge_L: p: \")\n",
    "print(_rouge_l_p)\n",
    "sum = 0\n",
    "for i in range(0,len(rouge_l_R)):\n",
    "    sum = sum + rouge_l_R[i]\n",
    "_rouge_l_r = sum/len(rouge_l_R)\n",
    "print(\"rouge_L: Rr: \")\n",
    "print(_rouge_l_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Luhn - heurestic method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "mydata = json.load(open('/Users/zhangyiman/Desktop/nfL6.json','r'))\n",
    "questions = []\n",
    "bestAnswers = []\n",
    "NotBestAnswers = []\n",
    "main_categorys = []\n",
    "q_id = []\n",
    "for q_a in mydata:\n",
    "    questions.append(q_a['question'])\n",
    "    bestAnswers.append(q_a['answer'])\n",
    "    NotBestAnswers.append(q_a['nbestanswers'])\n",
    "    main_categorys.append(q_a['main_category'])\n",
    "    q_id.append(q_a['id'])\n",
    "    \n",
    "# sentences candidate\n",
    "\n",
    "query_id = []\n",
    "query = []\n",
    "category = []\n",
    "sentence_id = []\n",
    "sentence = []\n",
    "whether_groundTruth = []\n",
    "for j in range (0,len(NotBestAnswers)):\n",
    "    # for every question, \n",
    "    sentence_candidate = []\n",
    "    Ground_Truth_sentence = []\n",
    "    \n",
    "    # there are len_sen answers for this question\n",
    "    len_sen = len(NotBestAnswers[j])\n",
    "    for a in range(0,len_sen):\n",
    "        str = NotBestAnswers[j][a]\n",
    "        str_split = str.split('. ')\n",
    "        # reduce if sentence is null\n",
    "        for i in range(0,len(str_split)):\n",
    "            if len(str_split[i])>=2:\n",
    "                sentence_candidate.append(str_split[i])\n",
    "                \n",
    "    #Ground Truth sentences\n",
    "    str = bestAnswers[j]\n",
    "    str_split = str.split('. ')\n",
    "    for i in range(0,len(str_split)):\n",
    "        Ground_Truth_sentence.append(str_split[i])\n",
    "        \n",
    "    #Whether ground truth\n",
    "    for i in range(0,len(sentence_candidate)):\n",
    "        if sentence_candidate[i] not in Ground_Truth_sentence:\n",
    "            Ground_Truth = 0\n",
    "        else:\n",
    "            Ground_Truth = 1\n",
    "        query_id.append(j)\n",
    "        query.append(questions[j])\n",
    "        category.append(main_categorys[j])\n",
    "        sentence_id.append(i)\n",
    "        sentence.append(sentence_candidate[i])\n",
    "        whether_groundTruth.append(Ground_Truth)\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "from sumy.parsers.html import HtmlParser\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.luhn import LuhnSummarizer as Summarizer\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.utils import get_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use part of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query_id = query_id[0:100026]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LANGUAGE = \"english\"\n",
    "SENTENCES_COUNT = 3\n",
    "\n",
    "rouge_1_F = []\n",
    "rouge_1_P = []\n",
    "rouge_1_R = []\n",
    "rouge_2_F = []\n",
    "rouge_2_P = []\n",
    "rouge_2_R = []\n",
    "rouge_l_F = []\n",
    "rouge_l_P = []\n",
    "rouge_l_R = []\n",
    "\n",
    "query_id_flag = 0\n",
    "\n",
    "hypothesis = ' '\n",
    "\n",
    "with open('document.txt','w') as f:\n",
    "    f.write(' ')\n",
    "f.close()\n",
    "\n",
    "for i in range(0,len(query_id)):\n",
    "    #for one query\n",
    "    if query_id[i] == query_id_flag:\n",
    "        with open('document.txt','a') as f:\n",
    "            f.write(sentence[i])\n",
    "            f.write('\\n')\n",
    "        f.close()\n",
    "        \n",
    "    else:\n",
    "        #use the document to summarize answer and get rouge score    \n",
    "        parser = PlaintextParser.from_file(\"document.txt\", Tokenizer(LANGUAGE))\n",
    "        stemmer = Stemmer(LANGUAGE)\n",
    "\n",
    "        summarizer = Summarizer(stemmer)\n",
    "        summarizer.stop_words = get_stop_words(LANGUAGE)   \n",
    "      \n",
    "        evaluated_sentences = (summarizer(parser.document, SENTENCES_COUNT))\n",
    "        if len(evaluated_sentences)>0:\n",
    "            for i in range(0,3):\n",
    "                #evaluated_sentences[i].words\n",
    "                str_111 = ' '\n",
    "                for i in range(0,len(evaluated_sentences[0].words)):\n",
    "                    str_111 = str_111 + evaluated_sentences[0].words[i]\n",
    "                    str_111 = str_111 + ' '\n",
    "                hypothesis = hypothesis + str_111\n",
    "\n",
    "            reference = bestAnswers[query_id_flag]\n",
    "            rouge = Rouge()\n",
    "            scores = rouge.get_scores(reference, hypothesis)\n",
    "            rouge_1_f = scores[0]['rouge-1']['f']\n",
    "            rouge_1_p = scores[0]['rouge-1']['p']\n",
    "            rouge_1_r = scores[0]['rouge-1']['r']\n",
    "            rouge_2_f = scores[0]['rouge-2']['f']\n",
    "            rouge_2_p = scores[0]['rouge-2']['p']\n",
    "            rouge_2_r = scores[0]['rouge-2']['r']\n",
    "            rouge_l_f = scores[0]['rouge-l']['f']\n",
    "            rouge_l_p = scores[0]['rouge-l']['p']\n",
    "            rouge_l_r = scores[0]['rouge-l']['r']\n",
    "            \n",
    "            rouge_1_F.append(rouge_1_f)\n",
    "            rouge_1_P.append(rouge_1_p)\n",
    "            rouge_1_R.append(rouge_1_r)\n",
    "            rouge_2_F.append(rouge_2_f)\n",
    "            rouge_2_P.append(rouge_2_p)\n",
    "            rouge_2_R.append(rouge_2_r)\n",
    "            rouge_l_F.append(rouge_l_f)\n",
    "            rouge_l_P.append(rouge_l_p)\n",
    "            rouge_l_R.append(rouge_l_r)\n",
    "\n",
    "        #update query_id_flag\n",
    "        query_id_flag = query_id_flag + 1\n",
    "    \n",
    "        #update document.txt\n",
    "        with open('document.txt','w') as f:\n",
    "            f.write(' ')\n",
    "        f.close()\n",
    "        \n",
    "        #update hypothesis\n",
    "        hypothesis = ' '\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge_1: f: \n",
      "0.48250960513661045\n",
      "rouge_1: p: \n",
      "0.43236977950887484\n",
      "rouge_1: Rr: \n",
      "0.6159745300853949\n",
      "rouge_2: f: \n",
      "0.35864907716308947\n",
      "rouge_2: p: \n",
      "0.325525595778387\n",
      "rouge_2: Rr: \n",
      "0.44290731827035873\n",
      "rouge_L: f: \n",
      "0.23445843433946761\n",
      "rouge_L: p: \n",
      "0.45938242815423447\n",
      "rouge_L: Rr: \n",
      "0.22457448651521883\n"
     ]
    }
   ],
   "source": [
    "sum = 0\n",
    "for i in range(0,len(rouge_1_F)):\n",
    "    sum = sum + rouge_1_F[i]\n",
    "_rouge_1_f = sum/len(rouge_1_F)\n",
    "print(\"rouge_1: f: \")\n",
    "print(_rouge_1_f)\n",
    "sum = 0\n",
    "for i in range(0,len(rouge_1_P)):\n",
    "    sum = sum + rouge_1_P[i]\n",
    "_rouge_1_p = sum/len(rouge_1_P)\n",
    "print(\"rouge_1: p: \")\n",
    "print(_rouge_1_p)\n",
    "sum = 0\n",
    "for i in range(0,len(rouge_1_R)):\n",
    "    sum = sum + rouge_1_R[i]\n",
    "_rouge_1_r = sum/len(rouge_1_R)\n",
    "print(\"rouge_1: Rr: \")\n",
    "print(_rouge_1_r)\n",
    "\n",
    "sum = 0\n",
    "for i in range(0,len(rouge_2_F)):\n",
    "    sum = sum + rouge_2_F[i]\n",
    "_rouge_2_f = sum/len(rouge_2_F)\n",
    "print(\"rouge_2: f: \")\n",
    "print(_rouge_2_f)\n",
    "sum = 0\n",
    "for i in range(0,len(rouge_2_P)):\n",
    "    sum = sum + rouge_2_P[i]\n",
    "_rouge_2_p = sum/len(rouge_2_P)\n",
    "print(\"rouge_2: p: \")\n",
    "print(_rouge_2_p)\n",
    "sum = 0\n",
    "for i in range(0,len(rouge_2_R)):\n",
    "    sum = sum + rouge_2_R[i]\n",
    "_rouge_2_r = sum/len(rouge_2_R)\n",
    "print(\"rouge_2: Rr: \")\n",
    "print(_rouge_2_r)\n",
    "\n",
    "sum = 0\n",
    "for i in range(0,len(rouge_l_F)):\n",
    "    sum = sum + rouge_l_F[i]\n",
    "_rouge_l_f = sum/len(rouge_l_F)\n",
    "print(\"rouge_L: f: \")\n",
    "print(_rouge_l_f)\n",
    "sum = 0\n",
    "for i in range(0,len(rouge_l_P)):\n",
    "    sum = sum + rouge_l_P[i]\n",
    "_rouge_l_p = sum/len(rouge_l_P)\n",
    "print(\"rouge_L: p: \")\n",
    "print(_rouge_l_p)\n",
    "sum = 0\n",
    "for i in range(0,len(rouge_l_R)):\n",
    "    sum = sum + rouge_l_R[i]\n",
    "_rouge_l_r = sum/len(rouge_l_R)\n",
    "print(\"rouge_L: Rr: \")\n",
    "print(_rouge_l_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## todo: CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## todo: another dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# dataset_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import fileinput\n",
    "lines = []\n",
    "for line in fileinput.input(\"QA_dataset.txt\"):\n",
    "    lines.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "db_query_id = []\n",
    "db_query = []\n",
    "db_answer = []\n",
    "db_score = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# score for answer: 4 : excellent 3: good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "question_id = -1\n",
    "\n",
    "for i in range(0,len(lines)):\n",
    "    x = lines[i].split('\\t',1)\n",
    "    \n",
    "    if x[0] == 'QUESTION':\n",
    "        y = x[1].split('\\t',2)\n",
    "        question_id = question_id + 1\n",
    "        question = y[2]\n",
    "    else: \n",
    "        y = x[1].split('\\t',2)\n",
    "        score = int(y[1])\n",
    "        answer = y[2]\n",
    "        \n",
    "        db_query_id.append(question_id)\n",
    "        db_query.append(question)\n",
    "        db_answer.append(answer)\n",
    "        db_score.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sentences candidate\n",
    "\n",
    "query_id = []\n",
    "query = []\n",
    "sentence_id = []\n",
    "sentence = []\n",
    "score = []\n",
    "# or whether_groundTruth\n",
    "\n",
    "query_flag = 0\n",
    "sen_ID = 0\n",
    "\n",
    "for i in range(0,len(db_answer)):\n",
    "    if db_query_id[i] == query_flag: \n",
    "        str = db_answer[i]\n",
    "        str_split = str.split('. ')\n",
    "        for j in range(0,len(str_split)):\n",
    "            sentence.append(str_split[j])\n",
    "            query_id.append(db_query_id[i])\n",
    "            query.append(db_query[i])\n",
    "            score.append(db_score[i])\n",
    "            sentence_id.append(sen_ID)\n",
    "            sen_ID = sen_ID + 1\n",
    "    else: \n",
    "        query_flag = query_flag + 1\n",
    "        sen_ID = 0\n",
    "        str = db_answer[i]\n",
    "        str_split = str.split('. ')\n",
    "        for j in range(0,len(str_split)):\n",
    "            sentence.append(str_split[j])\n",
    "            query_id.append(db_query_id[i])\n",
    "            query.append(db_query[i])\n",
    "            score.append(db_score[i])\n",
    "            sentence_id.append(sen_ID)\n",
    "            sen_ID = sen_ID + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query_flag = 0\n",
    "num_3 = 0\n",
    "num_4 = 0\n",
    "num_num_3 = []\n",
    "num_num_4 = []\n",
    "\n",
    "for i in range(0,len(sentence)):\n",
    "    if query_id[i] == query_flag:\n",
    "        if score[i] == 4: \n",
    "            num_4 = num_4 + 1\n",
    "        if score[i] == 3: \n",
    "            num_3 = num_3 + 1\n",
    "    else: \n",
    "        num_num_3.append(num_3)\n",
    "        num_num_4.append(num_4)\n",
    "        \n",
    "        query_flag = query_flag + 1\n",
    "        \n",
    "        num_3 = 0\n",
    "        num_4 = 0\n",
    "        if score[i] == 4: \n",
    "            num_4 = num_4 + 1\n",
    "        if score[i] == 3: \n",
    "            num_3 = num_3 + 1\n",
    "\n",
    "# need to delete the query_id = 1014 in the dataframe because here we simply set num_num_3 and num_num_4 as 0\n",
    "num_num_4.append(0)\n",
    "num_num_3.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "delete_question_id = []\n",
    "for i in range(0,len(num_num_3)):\n",
    "    if num_num_3[i] == 0 and num_num_4[i] == 0:\n",
    "        delete_question_id.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[17, 56, 64, 67, 91]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delete_question_id[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "delete_question_id.append(1014)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set whether_select_this_sentence\n",
    "# whether_select --> 0: not select 1:select\n",
    "# for questions which have score 4 sentences, select these score 4 sentences\n",
    "# for question which don't have score 4 sentences, select score 3 sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "whether_select = []\n",
    "for i in range(0,len(query_id)):\n",
    "    if num_num_4[query_id[i]] != 0 :\n",
    "        #set score 4 sentences as selected\n",
    "        if score[i] == 4:\n",
    "            whether_select.append(1)\n",
    "        else:\n",
    "            whether_select.append(0)\n",
    "    else: \n",
    "        # set score 3 sentences as selected\n",
    "        if score[i] ==3:\n",
    "            whether_select.append(1)\n",
    "        else:\n",
    "            whether_select.append(0)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153335"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#len(query_id)\n",
    "#len(whether_select)\n",
    "len(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set a list: Best_Answer[] --> use to evaluate the baselines "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_answer = []\n",
    "str = ''\n",
    "query_flag = 0\n",
    "\n",
    "for i in range(0,len(whether_select)):\n",
    "    if query_id[i] == query_flag:\n",
    "        if whether_select[i] == 1:\n",
    "            str = str + sentence[i]\n",
    "    else: \n",
    "        best_answer.append(str)\n",
    "        query_flag = query_flag + 1\n",
    "        str = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1014"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(best_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Here is what I do...I soap it up real good when showering and rinse thoroughly, when I am drying off, I use qtips to dry out the belly button and remove lint, etcafterwards I spray a small amount of body spray in there to keep it smelling freshDo this daily and you're gooI know exactly how you feel, I live near the beach and every time I go, I always find sand hidden deep inside my bellyholeIf your at the beach and you get sand all over your belly, you may have to just brush it off unless you want to discretely clean your navel with your fingerAfter you get home, you can do a more thorough cleaning by using a qtipGood luck, beach sand is awfully stickMy.boyfriend absolutely hates touching his belly buttonHe keeps it clean but it grosses him out everytimeAnd It's not even a deep belly button you wipe the cloth across his stomach and It's clean, hes a stick so to sayI don't have issues with my own belly button th.\\nSeriously, and I am not being sarcastic hereHe needs to wash his belly button every time he showersMost people don't and you need to soap up your hands and use your fingers to wash inside your belly button.\\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_answer[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### write into csv and read as dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "with open('sentence_QA_2.csv', 'w') as datacsv:\n",
    "    writer = csv.writer(datacsv,dialect=(\"excel\"))\n",
    "    writer.writerow(['q_id','query_id','query','sentence_id','sentence','score','whether_select'])\n",
    "    for i in range(0,len(query_id)):\n",
    "        writer.writerow([query_id[i],query_id[i],query[i],sentence_id[i],sentence[i],score[i],whether_select[i]])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "sentence_df = pd.read_csv('sentence_QA_2.csv')\n",
    "#sentence_df[400:450]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>q_id</th>\n",
       "      <th>query_id</th>\n",
       "      <th>query</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>sentence</th>\n",
       "      <th>score</th>\n",
       "      <th>whether_select</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>One side of my body tenses/freezes? Help?\\tI'm...</td>\n",
       "      <td>0</td>\n",
       "      <td>Don't seek help here...Go and see a doctor imm...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>One side of my body tenses/freezes? Help?\\tI'm...</td>\n",
       "      <td>1</td>\n",
       "      <td>I just did a body scan and my right side, ..</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>One side of my body tenses/freezes? Help?\\tI'm...</td>\n",
       "      <td>2</td>\n",
       "      <td>My entire body is one giant tense muscle</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>One side of my body tenses/freezes? Help?\\tI'm...</td>\n",
       "      <td>3</td>\n",
       "      <td>If I tense one eye ..</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>One side of my body tenses/freezes? Help?\\tI'm...</td>\n",
       "      <td>4</td>\n",
       "      <td>Tense neck muscle when i yawn? The throbbing i...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   q_id  query_id                                              query  \\\n",
       "0     0         0  One side of my body tenses/freezes? Help?\\tI'm...   \n",
       "1     0         0  One side of my body tenses/freezes? Help?\\tI'm...   \n",
       "2     0         0  One side of my body tenses/freezes? Help?\\tI'm...   \n",
       "3     0         0  One side of my body tenses/freezes? Help?\\tI'm...   \n",
       "4     0         0  One side of my body tenses/freezes? Help?\\tI'm...   \n",
       "\n",
       "   sentence_id                                           sentence  score  \\\n",
       "0            0  Don't seek help here...Go and see a doctor imm...      4   \n",
       "1            1       I just did a body scan and my right side, ..      2   \n",
       "2            2           My entire body is one giant tense muscle      2   \n",
       "3            3                              If I tense one eye ..      2   \n",
       "4            4  Tense neck muscle when i yawn? The throbbing i...      2   \n",
       "\n",
       "   whether_select  \n",
       "0               1  \n",
       "1               0  \n",
       "2               0  \n",
       "3               0  \n",
       "4               0  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### remove query which don't have ground good answer sentences and use less in experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence_df = sentence_df.set_index('q_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(0,len(delete_question_id)):\n",
    "    sentence_df = sentence_df.drop(delete_question_id[i],axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#sentence_df[156:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentence_df = sentence_df.dropna(axis=0,how='any') #drop all rows that have any NaN values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random select sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### average best_answer_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "sum_s = 0\n",
    "num_s = 0\n",
    "for i in range(0,len(best_answer)):\n",
    "    str = best_answer[i]\n",
    "    str_split = str.split('.')\n",
    "    j = 0\n",
    "    while j < len(str_split):\n",
    "        if str_split[j] == '':\n",
    "            del str_split[j]\n",
    "            j = j -1\n",
    "        j = j + 1\n",
    "    sum_s = sum_s + len(str_split)\n",
    "    num_s = num_s + 1\n",
    "average_length = int(sum_s/num_s)\n",
    "print(average_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import defaultdict\n",
    "from rouge import Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rouge_1_f_sum = 0\n",
    "rouge_1_p_sum = 0\n",
    "rouge_1_r_sum = 0\n",
    "rouge_2_f_sum = 0\n",
    "rouge_2_p_sum = 0\n",
    "rouge_2_r_sum = 0\n",
    "rouge_l_f_sum = 0\n",
    "rouge_l_p_sum = 0\n",
    "rouge_l_r_sum = 0\n",
    "num = 0\n",
    "\n",
    "for i in range(0,1014):\n",
    "    if len(sentence_df[sentence_df['query_id']== i]) > 8: \n",
    "        sen_num = random.sample(range(len(sentence_df[sentence_df['query_id']== i])),8)\n",
    "        hypothesis = ''\n",
    "        for j in range(0,len(sen_num)):\n",
    "            sub = sentence_df[sentence_df['query_id']==i]\n",
    "            sub = sub.set_index('sentence_id')\n",
    "            \n",
    "            sentence_sel = sub['sentence'].values[j]\n",
    "            hypothesis = hypothesis + sentence_sel\n",
    "            \n",
    "        reference = best_answer[i]\n",
    "        rouge = Rouge()\n",
    "        scores = rouge.get_scores(reference, hypothesis)\n",
    "        rouge_1_f = scores[0]['rouge-1']['f']\n",
    "        rouge_1_p = scores[0]['rouge-1']['p']\n",
    "        rouge_1_r = scores[0]['rouge-1']['r']\n",
    "        rouge_2_f = scores[0]['rouge-2']['f']\n",
    "        rouge_2_p = scores[0]['rouge-2']['p']\n",
    "        rouge_2_r = scores[0]['rouge-2']['r']\n",
    "        rouge_l_f = scores[0]['rouge-l']['f']\n",
    "        rouge_l_p = scores[0]['rouge-l']['p']\n",
    "        rouge_l_r = scores[0]['rouge-l']['r']\n",
    "    \n",
    "        rouge_1_f_sum = rouge_1_f_sum + rouge_1_f\n",
    "        rouge_1_p_sum = rouge_1_p_sum + rouge_1_p\n",
    "        rouge_1_r_sum = rouge_1_r_sum + rouge_1_r\n",
    "        rouge_2_f_sum = rouge_2_f_sum + rouge_2_f\n",
    "        rouge_2_p_sum = rouge_2_p_sum + rouge_2_p\n",
    "        rouge_2_r_sum = rouge_2_r_sum + rouge_2_r\n",
    "        rouge_l_f_sum = rouge_l_f_sum + rouge_l_f\n",
    "        rouge_l_p_sum = rouge_l_p_sum + rouge_l_p\n",
    "        rouge_l_r_sum = rouge_l_r_sum + rouge_l_r\n",
    "    \n",
    "        num = num + 1\n",
    "        \n",
    "rouge_1_F = rouge_1_f_sum/num\n",
    "rouge_1_P = rouge_1_p_sum/num\n",
    "rouge_1_R = rouge_1_r_sum/num\n",
    "rouge_2_F = rouge_2_f_sum/num\n",
    "rouge_2_P = rouge_2_p_sum/num\n",
    "rouge_2_R = rouge_2_r_sum/num\n",
    "rouge_l_F = rouge_l_f_sum/num\n",
    "rouge_l_P = rouge_l_p_sum/num\n",
    "rouge_l_R = rouge_l_r_sum/num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rouge_1: f: \n",
      "0.22430505028259645\n",
      "rouge_1: p: \n",
      "0.21549287395592748\n",
      "rouge_1: r: \n",
      "0.3627529454799493\n",
      "rouge_2: f: \n",
      "0.10173984826122628\n",
      "rouge_2: p: \n",
      "0.08984945021209327\n",
      "rouge_2: r: \n",
      "0.2001187128239444\n",
      "rouge_L: f: \n",
      "0.11317252993971307\n",
      "rouge_L: p: \n",
      "0.14225121541177846\n",
      "rouge_L: r: \n",
      "0.28954199841489353\n"
     ]
    }
   ],
   "source": [
    "print(\"rouge_1: f: \")\n",
    "print(rouge_1_F)\n",
    "print(\"rouge_1: p: \")\n",
    "print(rouge_1_P)\n",
    "print(\"rouge_1: r: \")\n",
    "print(rouge_1_R)\n",
    "\n",
    "print(\"rouge_2: f: \")\n",
    "print(rouge_2_F)\n",
    "print(\"rouge_2: p: \")\n",
    "print(rouge_2_P)\n",
    "print(\"rouge_2: r: \")\n",
    "print(rouge_2_R)\n",
    "      \n",
    "print(\"rouge_L: f: \")\n",
    "print(rouge_l_F)\n",
    "print(\"rouge_L: p: \")\n",
    "print(rouge_l_P)\n",
    "print(\"rouge_L: r: \")\n",
    "print(rouge_l_R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM for second dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "#import datetime\n",
    "from sklearn.svm import SVC\n",
    "#import os\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "#cwd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('/Users/zhangyiman/Desktop/word2vec_pre/glove.6B.50d.txt')\n",
    "embeddings_index = {}\n",
    "count = 0\n",
    "\n",
    "for line in f:\n",
    "        values_int = line.split('\\t')\n",
    "        #print(values_int[0])\n",
    "        #values = values_int[0].split(' ',1)\n",
    "        values = values_int[0].split(' ')\n",
    "        id = values[0]\n",
    "        #print(id)\n",
    "        #coefs = np.asarray(values[1:])\n",
    "        coefs = values[1:]\n",
    "        #print(coefs)\n",
    "        embeddings_index[id] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def sentence_vector(sentence_str):\n",
    "    str = sentence_str\n",
    "    #str = str.replace(',',' ,')\n",
    "    str = str.replace(\"'s\",'')\n",
    "    str = str.replace(\"determent\",'deter')\n",
    "    str = str.replace(\"200B\",'200 b')\n",
    "    token = re.compile(\"[\\w]+(?=n't)|n't|\\'m|\\'ll|[\\w]+|[.?!;,\\-\\(\\)—\\:']\")\n",
    "    tokens = token.findall(str)\n",
    "    \n",
    "    #str_convert = ''.join(tokens[0])\n",
    "    a = [0]*50\n",
    "    for i in range(0,len(tokens)):\n",
    "            for j in range(0,50):\n",
    "                str_convert = ''.join(tokens[i])\n",
    "                str_lower = str_convert.lower()\n",
    "                try: \n",
    "                    m = float(embeddings_index[str_lower][j])\n",
    "                except:\n",
    "                    m = 0\n",
    "                a[j] = a[j] + m\n",
    "    for i in range(0,50):\n",
    "        if len(tokens) ==0:\n",
    "            a[i] = 0\n",
    "        else:\n",
    "            a[i] = a[i]/len(tokens)\n",
    "    return(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(0,len(query)):\n",
    "    str = query[i]\n",
    "    query[i] = sentence_vector(str)\n",
    "    str = sentence[i]\n",
    "    sentence[i] = sentence_vector(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.2567643953608248,\n",
       " 0.12853462371134036,\n",
       " -0.0035550721649484575,\n",
       " -0.2723664639175257,\n",
       " 0.4016127061855672,\n",
       " 0.11410566185567009,\n",
       " -0.4018567907216495,\n",
       " 0.029143582474226826,\n",
       " -0.2166815389175259,\n",
       " -0.04793512340206185,\n",
       " -0.012361213402061854,\n",
       " 0.13971055927835063,\n",
       " -0.44455645721649467,\n",
       " -0.1285194952061856,\n",
       " 0.5957187108247424,\n",
       " 0.30770805000000007,\n",
       " 0.041629103608247425,\n",
       " -0.0048980278350515055,\n",
       " -0.3024270948453608,\n",
       " -0.4270346170103095,\n",
       " -0.10960781443298959,\n",
       " 0.2769147041237113,\n",
       " 0.35894074226804135,\n",
       " 0.0443243298969072,\n",
       " 0.3005610345360826,\n",
       " -1.6955116340206187,\n",
       " -0.357974675257732,\n",
       " 0.1939805773195878,\n",
       " 0.4037584721649484,\n",
       " -0.4255267628865982,\n",
       " 3.2488940618556694,\n",
       " 0.3189583257731959,\n",
       " -0.2566510463917528,\n",
       " -0.18459720783505154,\n",
       " 0.03316788324742269,\n",
       " -0.07661612525773198,\n",
       " 0.15693918711340207,\n",
       " 0.22201750824742286,\n",
       " 0.2559271011340207,\n",
       " -0.18962159793814432,\n",
       " -0.23580749312886604,\n",
       " 0.07441659015463911,\n",
       " 0.004442796237113394,\n",
       " 0.23278254783505162,\n",
       " -0.011192520618556705,\n",
       " 0.03726610567010306,\n",
       " -0.04595910221649484,\n",
       " -0.2648436515463918,\n",
       " -0.051619772164948445,\n",
       " 0.0948479809278351]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2567643953608248, 0.12853462371134036, -0.0035550721649484575, -0.2723664639175257, 0.4016127061855672, 0.11410566185567009, -0.4018567907216495, 0.029143582474226826, -0.2166815389175259, -0.04793512340206185, -0.012361213402061854, 0.13971055927835063, -0.44455645721649467, -0.1285194952061856, 0.5957187108247424, 0.30770805000000007, 0.041629103608247425, -0.0048980278350515055, -0.3024270948453608, -0.4270346170103095, -0.10960781443298959, 0.2769147041237113, 0.35894074226804135, 0.0443243298969072, 0.3005610345360826, -1.6955116340206187, -0.357974675257732, 0.1939805773195878, 0.4037584721649484, -0.4255267628865982, 3.2488940618556694, 0.3189583257731959, -0.2566510463917528, -0.18459720783505154, 0.03316788324742269, -0.07661612525773198, 0.15693918711340207, 0.22201750824742286, 0.2559271011340207, -0.18962159793814432, -0.23580749312886604, 0.07441659015463911, 0.004442796237113394, 0.23278254783505162, -0.011192520618556705, 0.03726610567010306, -0.04595910221649484, -0.2648436515463918, -0.051619772164948445, 0.0948479809278351]\n",
      "0.2567643953608248\n"
     ]
    }
   ],
   "source": [
    "print(query[1])\n",
    "print(query[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('sentence_QA_2_vector.csv', 'w') as datacsv:\n",
    "    writer = csv.writer(datacsv,dialect=(\"excel\"))\n",
    "    writer.writerow(['query_id','query_dim0','query_dim1','query_dim2','query_dim3','query_dim4','query_dim5','query_dim6','query_dim7','query_dim8','query_dim9','query_dim10','query_dim11','query_dim12','query_dim13','query_dim14','query_dim15','query_dim16','query_dim17','query_dim18','query_dim19','query_dim20','query_dim21','query_dim22','query_dim23','query_dim24','query_dim25','query_dim26','query_dim27','query_dim28','query_dim29','query_dim30','query_dim31','query_dim32','query_dim33','query_dim34','query_dim35','query_dim36','query_dim37','query_dim38','query_dim39','query_dim40','query_dim41','query_dim42','query_dim43','query_dim44','query_dim45','query_dim46','query_dim47','query_dim48','query_dim49','sentence_id','sentence_dim0','sentence_dim1','sentence_dim2','sentence_dim3','sentence_dim4','sentence_dim5','sentence_dim6','sentence_dim7','sentence_dim8','sentence_dim9','sentence_dim10','sentence_dim11','sentence_dim12','sentence_dim13','sentence_dim14','sentence_dim15','sentence_dim16','sentence_dim17','sentence_dim18','sentence_dim19','sentence_dim20','sentence_dim21','sentence_dim22','sentence_dim23','sentence_dim24','sentence_dim25','sentence_dim26','sentence_dim27','sentence_dim28','sentence_dim29','sentence_dim30','sentence_dim31','sentence_dim32','sentence_dim33','sentence_dim34','sentence_dim35','sentence_dim36','sentence_dim37','sentence_dim38','sentence_dim39','sentence_dim40','sentence_dim41','sentence_dim42','sentence_dim43','sentence_dim44','sentence_dim45','sentence_dim46','sentence_dim47','sentence_dim48','sentence_dim49','whether_select'])\n",
    "    for i in range(0,len(query_id)):\n",
    "        writer.writerow([query_id[i],query[i][0],query[i][1],query[i][2],query[i][3],query[i][4],query[i][5],query[i][6],query[i][7],query[i][8],query[i][9],query[i][10],query[i][11],query[i][12],query[i][13],query[i][14],query[i][15],query[i][16],query[i][17],query[i][18],query[i][19],query[i][20],query[i][21],query[i][22],query[i][23],query[i][24],query[i][25],query[i][26],query[i][27],query[i][28],query[i][29],query[i][30],query[i][31],query[i][32],query[i][33],query[i][34],query[i][35],query[i][36],query[i][37],query[i][38],query[i][39],query[i][40],query[i][41],query[i][42],query[i][43],query[i][44],query[i][45],query[i][46],query[i][47],query[i][48],query[i][49],sentence_id[i],sentence[i][0],sentence[i][1],sentence[i][2],sentence[i][3],sentence[i][4],sentence[i][5],sentence[i][6],sentence[i][7],sentence[i][8],sentence[i][9],sentence[i][10],sentence[i][11],sentence[i][12],sentence[i][13],sentence[i][14],sentence[i][15],sentence[i][16],sentence[i][17],sentence[i][18],sentence[i][19],sentence[i][20],sentence[i][21],sentence[i][22],sentence[i][23],sentence[i][24],sentence[i][25],sentence[i][26],sentence[i][27],sentence[i][28],sentence[i][29],sentence[i][30],sentence[i][31],sentence[i][32],sentence[i][33],sentence[i][34],sentence[i][35],sentence[i][36],sentence[i][37],sentence[i][38],sentence[i][39],sentence[i][40],sentence[i][41],sentence[i][42],sentence[i][43],sentence[i][44],sentence[i][45],sentence[i][46],sentence[i][47],sentence[i][48],sentence[i][49],whether_select[i]])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query_id</th>\n",
       "      <th>query_dim0</th>\n",
       "      <th>query_dim1</th>\n",
       "      <th>query_dim2</th>\n",
       "      <th>query_dim3</th>\n",
       "      <th>query_dim4</th>\n",
       "      <th>query_dim5</th>\n",
       "      <th>query_dim6</th>\n",
       "      <th>query_dim7</th>\n",
       "      <th>query_dim8</th>\n",
       "      <th>...</th>\n",
       "      <th>sentence_dim41</th>\n",
       "      <th>sentence_dim42</th>\n",
       "      <th>sentence_dim43</th>\n",
       "      <th>sentence_dim44</th>\n",
       "      <th>sentence_dim45</th>\n",
       "      <th>sentence_dim46</th>\n",
       "      <th>sentence_dim47</th>\n",
       "      <th>sentence_dim48</th>\n",
       "      <th>sentence_dim49</th>\n",
       "      <th>whether_select</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.256764</td>\n",
       "      <td>0.128535</td>\n",
       "      <td>-0.003555</td>\n",
       "      <td>-0.272366</td>\n",
       "      <td>0.401613</td>\n",
       "      <td>0.114106</td>\n",
       "      <td>-0.401857</td>\n",
       "      <td>0.029144</td>\n",
       "      <td>-0.216682</td>\n",
       "      <td>...</td>\n",
       "      <td>0.131674</td>\n",
       "      <td>0.064359</td>\n",
       "      <td>0.050794</td>\n",
       "      <td>0.176399</td>\n",
       "      <td>0.102187</td>\n",
       "      <td>-0.079441</td>\n",
       "      <td>-0.163512</td>\n",
       "      <td>0.083254</td>\n",
       "      <td>0.242382</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.256764</td>\n",
       "      <td>0.128535</td>\n",
       "      <td>-0.003555</td>\n",
       "      <td>-0.272366</td>\n",
       "      <td>0.401613</td>\n",
       "      <td>0.114106</td>\n",
       "      <td>-0.401857</td>\n",
       "      <td>0.029144</td>\n",
       "      <td>-0.216682</td>\n",
       "      <td>...</td>\n",
       "      <td>0.140784</td>\n",
       "      <td>-0.141418</td>\n",
       "      <td>0.186607</td>\n",
       "      <td>-0.058426</td>\n",
       "      <td>0.068691</td>\n",
       "      <td>0.048071</td>\n",
       "      <td>-0.325942</td>\n",
       "      <td>0.096276</td>\n",
       "      <td>0.038044</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.256764</td>\n",
       "      <td>0.128535</td>\n",
       "      <td>-0.003555</td>\n",
       "      <td>-0.272366</td>\n",
       "      <td>0.401613</td>\n",
       "      <td>0.114106</td>\n",
       "      <td>-0.401857</td>\n",
       "      <td>0.029144</td>\n",
       "      <td>-0.216682</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.107614</td>\n",
       "      <td>-0.268862</td>\n",
       "      <td>0.170626</td>\n",
       "      <td>0.058393</td>\n",
       "      <td>0.088156</td>\n",
       "      <td>0.143237</td>\n",
       "      <td>-0.276487</td>\n",
       "      <td>-0.201549</td>\n",
       "      <td>-0.184999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.256764</td>\n",
       "      <td>0.128535</td>\n",
       "      <td>-0.003555</td>\n",
       "      <td>-0.272366</td>\n",
       "      <td>0.401613</td>\n",
       "      <td>0.114106</td>\n",
       "      <td>-0.401857</td>\n",
       "      <td>0.029144</td>\n",
       "      <td>-0.216682</td>\n",
       "      <td>...</td>\n",
       "      <td>0.254807</td>\n",
       "      <td>-0.047262</td>\n",
       "      <td>0.227747</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.056698</td>\n",
       "      <td>0.081251</td>\n",
       "      <td>-0.170346</td>\n",
       "      <td>-0.051025</td>\n",
       "      <td>0.187470</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.256764</td>\n",
       "      <td>0.128535</td>\n",
       "      <td>-0.003555</td>\n",
       "      <td>-0.272366</td>\n",
       "      <td>0.401613</td>\n",
       "      <td>0.114106</td>\n",
       "      <td>-0.401857</td>\n",
       "      <td>0.029144</td>\n",
       "      <td>-0.216682</td>\n",
       "      <td>...</td>\n",
       "      <td>0.064894</td>\n",
       "      <td>-0.170583</td>\n",
       "      <td>0.172435</td>\n",
       "      <td>0.047038</td>\n",
       "      <td>0.097803</td>\n",
       "      <td>0.034851</td>\n",
       "      <td>-0.259232</td>\n",
       "      <td>-0.182091</td>\n",
       "      <td>-0.057514</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 103 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   query_id  query_dim0  query_dim1  query_dim2  query_dim3  query_dim4  \\\n",
       "0         0    0.256764    0.128535   -0.003555   -0.272366    0.401613   \n",
       "1         0    0.256764    0.128535   -0.003555   -0.272366    0.401613   \n",
       "2         0    0.256764    0.128535   -0.003555   -0.272366    0.401613   \n",
       "3         0    0.256764    0.128535   -0.003555   -0.272366    0.401613   \n",
       "4         0    0.256764    0.128535   -0.003555   -0.272366    0.401613   \n",
       "\n",
       "   query_dim5  query_dim6  query_dim7  query_dim8       ...        \\\n",
       "0    0.114106   -0.401857    0.029144   -0.216682       ...         \n",
       "1    0.114106   -0.401857    0.029144   -0.216682       ...         \n",
       "2    0.114106   -0.401857    0.029144   -0.216682       ...         \n",
       "3    0.114106   -0.401857    0.029144   -0.216682       ...         \n",
       "4    0.114106   -0.401857    0.029144   -0.216682       ...         \n",
       "\n",
       "   sentence_dim41  sentence_dim42  sentence_dim43  sentence_dim44  \\\n",
       "0        0.131674        0.064359        0.050794        0.176399   \n",
       "1        0.140784       -0.141418        0.186607       -0.058426   \n",
       "2       -0.107614       -0.268862        0.170626        0.058393   \n",
       "3        0.254807       -0.047262        0.227747        0.000003   \n",
       "4        0.064894       -0.170583        0.172435        0.047038   \n",
       "\n",
       "   sentence_dim45  sentence_dim46  sentence_dim47  sentence_dim48  \\\n",
       "0        0.102187       -0.079441       -0.163512        0.083254   \n",
       "1        0.068691        0.048071       -0.325942        0.096276   \n",
       "2        0.088156        0.143237       -0.276487       -0.201549   \n",
       "3        0.056698        0.081251       -0.170346       -0.051025   \n",
       "4        0.097803        0.034851       -0.259232       -0.182091   \n",
       "\n",
       "   sentence_dim49  whether_select  \n",
       "0        0.242382               1  \n",
       "1        0.038044               0  \n",
       "2       -0.184999               0  \n",
       "3        0.187470               0  \n",
       "4       -0.057514               0  \n",
       "\n",
       "[5 rows x 103 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "sentence_vector_data = pd.read_csv('sentence_QA_2_vector.csv')\n",
    "sentence_vector_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122668.0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.8*len(sentence_vector_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## use small part of dataset to train\n",
    "num_train = 122668\n",
    "#num_test = 60000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence_train = sentence_vector_data.iloc[:num_train]\n",
    "#sentence_test = sentence_vector_data.iloc[num_train:num_test]\n",
    "sentence_test = sentence_vector_data.iloc[num_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_train = sentence_train['whether_select'].values\n",
    "X_train = sentence_train.drop('whether_select',axis = 1).values\n",
    "X_test = sentence_test.drop('whether_select',axis = 1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********************training************************\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm = SVC()\n",
    "print('***********************training************************')\n",
    "svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***********************predict*************************\n"
     ]
    }
   ],
   "source": [
    "print('***********************predict*************************')\n",
    "prediction = svm.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rouge evaluation for dataset 2 use baseline SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "       1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction[170:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''rouge_1_F = []\n",
    "rouge_1_P = []\n",
    "rouge_1_R = []\n",
    "rouge_2_F = []\n",
    "rouge_2_P = []\n",
    "rouge_2_R = []\n",
    "rouge_l_F = []\n",
    "rouge_l_P = []\n",
    "rouge_l_R = []\n",
    "\n",
    "train_num = 50000\n",
    "test_num = 60000\n",
    "\n",
    "question_flag = sentence_data['query_id'][train_num]\n",
    "sentence_predicted = ''\n",
    "\n",
    "for i in range(0,len(prediction)):\n",
    "    if prediction[i] == 1:\n",
    "        \n",
    "        sentence_predicted_in_answer = sentence_data['sentence'][train_num+i]\n",
    "        question_id = sentence_data['query_id'][train_num+i]\n",
    "        \n",
    "        if question_id == question_flag:\n",
    "            sentence_predicted = sentence_predicted + sentence_predicted_in_answer\n",
    "        else:\n",
    "            #for this question_flag: \n",
    "            hypothesis = sentence_predicted\n",
    "            reference = bestAnswers[question_flag]\n",
    "            rouge = Rouge()\n",
    "            scores = rouge.get_scores(reference, hypothesis)\n",
    "            rouge_1_f = scores[0]['rouge-1']['f']\n",
    "            rouge_1_p = scores[0]['rouge-1']['p']\n",
    "            rouge_1_r = scores[0]['rouge-1']['r']\n",
    "            rouge_2_f = scores[0]['rouge-2']['f']\n",
    "            rouge_2_p = scores[0]['rouge-2']['p']\n",
    "            rouge_2_r = scores[0]['rouge-2']['r']\n",
    "            rouge_l_f = scores[0]['rouge-l']['f']\n",
    "            rouge_l_p = scores[0]['rouge-l']['p']\n",
    "            rouge_l_r = scores[0]['rouge-l']['r']\n",
    "            \n",
    "            rouge_1_F.append(rouge_1_f)\n",
    "            rouge_1_P.append(rouge_1_p)\n",
    "            rouge_1_R.append(rouge_1_r)\n",
    "            rouge_2_F.append(rouge_2_f)\n",
    "            rouge_2_P.append(rouge_2_p)\n",
    "            rouge_2_R.append(rouge_2_r)\n",
    "            rouge_l_F.append(rouge_l_f)\n",
    "            rouge_l_P.append(rouge_l_p)\n",
    "            rouge_l_R.append(rouge_l_r)\n",
    "            \n",
    "            #update question_flag:\n",
    "            question_flag = sentence_data['query_id'][train_num+i]\n",
    "            #update sentence_predicted:\n",
    "            sentence_predicted = ''\n",
    "            sentence_predicted = sentence_predicted + sentence_predicted_in_answer'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''sum = 0\n",
    "for i in range(0,len(rouge_1_F)):\n",
    "    sum = sum + rouge_1_F[i]\n",
    "_rouge_1_f = sum/len(rouge_1_F)\n",
    "print(\"rouge_1: f: \")\n",
    "print(_rouge_1_f)\n",
    "sum = 0\n",
    "for i in range(0,len(rouge_1_P)):\n",
    "    sum = sum + rouge_1_P[i]\n",
    "_rouge_1_p = sum/len(rouge_1_P)\n",
    "print(\"rouge_1: p: \")\n",
    "print(_rouge_1_p)\n",
    "sum = 0\n",
    "for i in range(0,len(rouge_1_R)):\n",
    "    sum = sum + rouge_1_R[i]\n",
    "_rouge_1_r = sum/len(rouge_1_R)\n",
    "print(\"rouge_1: Rr: \")\n",
    "print(_rouge_1_r)\n",
    "\n",
    "sum = 0\n",
    "for i in range(0,len(rouge_2_F)):\n",
    "    sum = sum + rouge_2_F[i]\n",
    "_rouge_2_f = sum/len(rouge_2_F)\n",
    "print(\"rouge_2: f: \")\n",
    "print(_rouge_2_f)\n",
    "sum = 0\n",
    "for i in range(0,len(rouge_2_P)):\n",
    "    sum = sum + rouge_2_P[i]\n",
    "_rouge_2_p = sum/len(rouge_2_P)\n",
    "print(\"rouge_2: p: \")\n",
    "print(_rouge_2_p)\n",
    "sum = 0\n",
    "for i in range(0,len(rouge_2_R)):\n",
    "    sum = sum + rouge_2_R[i]\n",
    "_rouge_2_r = sum/len(rouge_2_R)\n",
    "print(\"rouge_2: Rr: \")\n",
    "print(_rouge_2_r)\n",
    "\n",
    "sum = 0\n",
    "for i in range(0,len(rouge_l_F)):\n",
    "    sum = sum + rouge_l_F[i]\n",
    "_rouge_l_f = sum/len(rouge_l_F)\n",
    "print(\"rouge_L: f: \")\n",
    "print(_rouge_l_f)\n",
    "sum = 0\n",
    "for i in range(0,len(rouge_l_P)):\n",
    "    sum = sum + rouge_l_P[i]\n",
    "_rouge_l_p = sum/len(rouge_l_P)\n",
    "print(\"rouge_L: p: \")\n",
    "print(_rouge_l_p)\n",
    "sum = 0\n",
    "for i in range(0,len(rouge_l_R)):\n",
    "    sum = sum + rouge_l_R[i]\n",
    "_rouge_l_r = sum/len(rouge_l_R)\n",
    "print(\"rouge_L: Rr: \")\n",
    "print(_rouge_l_r)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
